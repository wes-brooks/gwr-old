\documentclass[authoryear, review, 11pt]{elsarticle}

\setlength{\textwidth}{6.5in}
%\setlength{\textheight}{9in}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{bm}
\usepackage{multirow}

%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{verbatim}
\usepackage{longtable}
\usepackage{rotating}
\usepackage[nolists,nomarkers]{endfloat}
\DeclareDelayedFloatFlavour{sidewaystable}{table}

\usepackage{relsize}
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{booktabs}


\usepackage{setspace}
\setstretch{2}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\bw}{\mbox{bw}}
\DeclareMathOperator*{\df}{\mbox{df}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\E}{\mathop{\mathbb E}}


\title{Local Variable Selection and Parameter Estimation of Spatially Varying Coefficient Regression Models}
\author{Wesley Brooks}
\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}

%Pastebin:
%Spatial association - meaning that nearby locations are more alike than distant locations - is a key concept in spatial statistics.\
%for univariate output $y$, $p$-vector of covariates $\bm{x}$, and random noise $\varepsilon$ all indexed by location $\bm{s}
%where $\phi$ is a bandwidth parameter and $\delta(\bm{s}, \bm{t})$ is the Euclidean distance between locations $\bm{s}$ and $\bm{t}$.

%\begin{spacing}{2}
\section{Introduction}
	%Varying coefficient regression
	Whereas the coefficients in traditional linear regression are scalar constants, the coefficients in a varying coefficient regression (VCR) model are functions - often \emph{smooth} functions - of some effect modifying variable \citep{Hastie:1993a}. When the effect modifying variable represents location in a spatial domain, a VCR model implies a spatially local regression model such that the regression coefficients vary over space and will be referred to as a spatially varying coefficient regression model (SVCR). Statistical inference for the coefficients as functions of location in an SVCR model is more complicated than estimating the coefficients in a global linear regression model where the coefficients are constant across the spatial domain. This document concerns the development of new methodologies for the analysis of spatial data using SVCR.
	
	%Spatial data / spatial regression
	The methodology described herein is applicable to geostatistical data and areal data. Let $\mathcal{D}$ be a spatial domain on which data is collected. For geostatistical data, let $\bm{s}$ denote a location in $\mathcal{D}$. Let univariate $\left\{Y(\bm{s}) : \bm{s} \in \mathcal{D}\right\}$ and possibly multivariate $\left\{\bm{X}(\bm{s}) : \bm{s} \in \mathcal{D}\right\}$ denote random fields of the response and the covariates, respectively. For $i = 1, \dots, n$, let $\bm{s}_i$ denote the location in $\mathcal{D}$ of the $i$th observation of the response and the covariates. Then the data are a realization of the random variables $\left\{Y(\bm{s}_i), \bm{X}(\bm{s}_i)\right\}$ for $i=1, \dots, n$. Let the observed data be denoted $\left\{y(\bm{s}_i), \bm{x}(\bm{s}_i)\right\}$, $i=1, \dots, n$.
	
	For areal data, the spatial domain $\mathcal{D}$ is partitioned into $n$ regions $\{D_1, \dots, D_n\}$ such that $\mathcal{D} = \bigcup \limits_{i=1}^nD_i$. In the case of areal data, the random variables $\left\{Y(D_i), \bm{X}(D_i)\right\}$ are defined for regions instead of for point locations; population and spatial mean temperature are examples of areal data. The analytical method described herein can be applied to areal data if they are recast as geostatistical data by assuming that the data are point-referenced to a representative location of each region, such as the centroid. That is, $\left\{\bm{X}(\bm{s}_i), Y(\bm{s}_i)\right\} $ where $\bm{s}_i$ is the centroid of $D_i$ for $i=i, \dots, n$.
	
	 Common practice in the analysis of geostatistical and areal data is to model the response variable with a spatial linear regression model consisting of the sum of a fixed mean function, a spatial random effect, and random error all on domain $\mathcal{D}$, as in: 
\begin{align}\label{eq:spatial-regression}
    Y(\bm{s}) = \bm{X}(\bm{s})'\bm{\beta} + W(\bm{s}) + \varepsilon(\bm{s})
\end{align}
where $\bm{X}(\bm{s})'\bm{\beta}$ is the mean function consisting of a vector of covariates $\bm{X}(\bm{s})$, and a vector of regression coefficients $\bm{\beta}$. The random error $\varepsilon(\bm{s})$ denotes white noise such that the errors are independent and identically distributed with mean zero and variance $\sigma^2$, while the random component $W(\bm{s})$ denotes a mean-zero, second-order stationary random field that is independent of the random error. The mean function captures the large-scale systematic trend of the response, the spatial random field $W(\bm{s})$ can be thought of as a small-scale spatial random effect, and the error term $\varepsilon(\bm{s})$ captures micro-scale variation \citep{Cressie:1993}. It is common to pre-specify the form of a covariance function for the spatial random effect $W(\bm{s})$ \citep{Diggle:2007}. For example, the exponential covariance function (a special case of the Mat\'{e}rn class of covariance functions) has the form
\begin{align}\label{eq:exponential-covariance}
    \text{Cov}(W(\bm{s}), W(\bm{t})) = \exp\left\{-\phi^{-1} \delta(\bm{s}, \bm{t}) \right\}
\end{align}
where $\phi$ denotes a range parameter and $\delta(\bm{s}, \bm{t})$ denotes the Euclidean distance between locations $\bm{s}$ and $\bm{t}$. The general form of a covariance function in the Mat\'{e}rn class is
\begin{align}\label{eq:matern-covarinace}
    \text{Cov}(W(\bm{s}), W(\bm{t})) = \left\{\Gamma(\nu) 2^{\nu-1} \right\}^{-1} \left\{\delta(\bm{s}, \bm{t}) \phi^{-1}\sqrt{2\nu}\right\}^\nu K_{\nu} \left(\delta(\bm{s}, \bm{t}) \phi^{-1}\sqrt{2\nu}\right)
    \end{align}
where $\nu$ denotes the degree of smoothness, $K_{\nu}$ denotes the modified Bessel equation of the second kind, and as before $\phi$ denotes a range parameter and $\delta(\bm{s}, \bm{t})$ the Euclidean distance between locations $\bm{s}$ and $\bm{t}$. The exponential covariance function corresponds to a Mat\'{e}rn class covariance function with $\nu = 1/2$.

	%SVCR - justification
	%Stationarity in spatial linear regression
	A random field is said to be stationary if the joint distribution of a the response at a finite set of locations does not change when the set of locations are all shifted in space by a fixed spatial lag. That is, letting $\left\{T(\bm{s}) : \bm{s} \in \mathcal{D}\right\}$ be a random field on spatial domain $\mathcal{D}$ that takes value $T(\bm{s}_i)$ at location $\bm{s}_i \in \mathcal{D}$ for $i = 1, \dots, n$, the random field $T(\bm{s})$ is stationary if $F_n\left(T(\bm{s}_1), \dots, T(\bm{s}_n)\right) = F_n\left(T(\bm{s}_1+\bm{h}), \dots, T(\bm{s}_n+\bm{h})\right)$ where $F_n(\cdot)$ is the joint distribution of a length $n$ sample from $T(\bm{s})$ and $\bm{h}$ is a fixed spatial lag. A random field is second-order stationary if the joint distribution at any two locations in the domain does not change when the locations are shifted by a fixed spatial lag.
	
	The coefficient vector $\bm{\beta}$ in (\ref{eq:spatial-regression}) is a fixed constant. The model can be made more flexible if the coefficients are described by a stationary random field. Such a model is written
\begin{align}\label{eq:SVCR-process}
    Y(\bm{s}) = \bm{X}(\bm{s})'\bm{\beta}(\bm{s}) + \varepsilon(\bm{s})
\end{align}
	where $\bm{\beta}(\bm{s})$ is a random coefficient field with a Mat\'{e}rn-class covariance function and the spatial random effect $W(\bm{s})$ included in the intercept $\beta_0(\bm{s})$. The random coefficient field $\bm{\beta}(\bm{s})$ can be estimated by Markov Chain Monte Carlo (MCMC) methods under the assumption that $\bm{\beta}(\bm{s})$ is stationary \citep{Gelfand:2003}.

	%The spatial random effect describes the spatial pattern in the deviations from the systematic part of the model. When fitting the spatial regression model (\ref{eq:spatial-regression}), it is usually required that the the fitted values of the spatial random effect and of the residuals sum to zero, i.e. $\sum\limits_{i=1}^n\hat{W}(\bm{s}_i) = 0$ and $\sum\limits_{i=1}^n\hat{\varepsilon}(\bm{s}_i) = 0$. This mode of analysis is appropriate when the systematic part of the regression model does not vary between locations. On the other hand, a VCR model is appropriate for the case where the systematic part of the regression model does vary across locations.

	%Spatial VCR
	Alternatively, kernel-based and spline-based methods can be considered for fitting varying coefficient models without assuming the coefficients are described by a stationary random field. For example, it is straightforward to modify a thin plate regression spline model into a traditional, non-spatial VCR model \citep{Wood:2006}. A local likelihood can also be used to fit generalized linear models with varying coefficients using kernel smoothing \citep{Loader:1999}. \cite{Fan:1999} demonstrated that the optimal kernel bandwidth estimate for a VCR model can be found via a two-step technique.
	
	Model selection in VCR models may be local or global. Global selection means including or excluding variables everywhere in the spatial domain, while local selection means including or excluding variables at individual locations within the spatial domain. Two methods have been proposed for global model selection in spline-based VCR models. \cite{Wang:2008a} applied a SCAD penalty \citep{Fan:2001} for variable selection in spline-based VCR models with a univariate effect-modifying variable. \cite{Antoniadis:2012a} used the nonnegative Garrote penalty \citep{Breiman:1995} in P-spline-based VCR models having a univariate effect-modifying variable.
	
	Wavelet methods for fitting SVCR models were explored by \cite{Shang-2011} and \cite{Zhang-2011}. Sparsity in the wavelet coefficients is achieved either by $\ell_1$-penalization (also known as the Lasso \citep{Tibshirani:1996}) \citep{Shang-2011} or by Bayesian variable selection \citep{Zhang-2011}. Sparsity in the wavelet domain does not imply sparsity in the covariates, though, so neither method can be used for local variable selection.

	%GWR
	Geographically weighted regression (GWR) is a kernel-based method of estimating the coefficients of an SVCR model where the kernel weights are based on the distance between sampling locations \cite{Brundson:1998a, Fotheringham:2002}. At each sampling location, traditional GWR estimates the local regression coefficients by the local likelihood \citep{Loader:1999}. As a kernel-based smoother for regression coefficients, traditional GWR tends to exhibit bias near the boundary of the region being modeled \citep{Hastie:1993b}. One way to reduce the boundary-effect bias is to model the coefficient surface as locally linear rather than locally constant by including coefficient-by-location interactions \cite{Wang:2008b}.
	
	%Local variable selection
	Traditional GWR relies on \emph{a priori} global model selection to decide which variables should be included in the model. In the context of ordinary least squares regression, Lasso regularization for variable selection \citep{Tibshirani:1996}, while popular, does not generally produce consistent estimates of the relevant predictor variables \citep{Leng-2006}. Regularization methods such as the adaptive Lasso (AL) \citep{Zou:2006} were developed and shown to have appealing properties for automating variable selection, sometimes including the ``oracle" property of asymptotically selecting exactly the correct set of covariates for inclusion in a regression model.
	
	The idea of using Lasso regularization for local variable selection in a GWR model appeared in the literature as the geographically weighted Lasso (GWL) \citep{Wheeler:2009}. The GWL uses the Lasso with a jackknife criterion for selection of the tuning parameters. Because the jackknife criterion can only be computed at sampling locations where the response variable is observed, the GWL cannot be used for imputation of missing data nor for interpolation between sampling locations.
	
	This paper introduces a new regularization method for local variable selection in GWR models that overcomes this limitation of the GWL by using a penalized-likelihood criterion to select the Lasso tuning parameters. In particular, a type of BIC is developed and used herein, but in principle another information criterion like the AIC is also possible. The local BIC presented here is based on the local likelihood \citep{Loader:1999} and a total BIC is based on an \emph{ad hoc} calculation of the sample size and degrees of freedom for estimating the spatially-varying coefficient surfaces.
	
	Three regularization methods were used in this work. The AL was implemented in two ways - once via the least angle regression (LARS) algorithm \citep{Efron:2004b} which uses least squares, and once via the coordinate descent algorithm using the {\tt R} package {\tt glmnet} \citep{Friedman:2010}. The third regularization method implemented here uses an adaptive elastic net (AEN) penalty \citep{Zou:2009}, also via the coordinate descent algorithm using the {\tt glmnet} package.
	

\section{Geographically Weighted Regression \label{section:GWR}}
	\subsection{Model}	
	Consider $n$ data observations, taken at sampling locations $\bm{s}_1, \dots, \bm{s}_n$ in a spatial domain $D \subset \mathbb{R}^2$. For $i = 1, \dots, n$, let $y(\bm{s}_i)$ and $\bm{x}(\bm{s}_i)$ denote the univariate response variable, and a $(p+1)$-variate vector of covariates measured at location $\bm{s}_i$, respectively. At each location $\bm{s}_i$, assume that the outcome is related to the covariates by a linear model where the coefficients $\bm{\beta}(\bm{s}_i)$ may be spatially-varying and $\varepsilon(\bm{s}_i)$ is random error at location $\bm{s}_i$. That is,
	\begin{align}\label{eq:lm(s)}
		y(\bm{s}_i) = \bm{x}(\bm{s}_i)' \bm{\beta}(\bm{s}_i) + \varepsilon(\bm{s}_i).
	\end{align}
	
	Further assume that the error term $\varepsilon(\bm{s}_i)$ is normally distributed with zero mean and variance $\sigma^2$, and that $\varepsilon(\bm{s}_i)$, $i=1, \dots, n$ are independent. That is,
	\begin{align} \label{eq:err}
		\varepsilon(\bm{s}_i) \overset{iid}{\sim} \mathcal{N} \left( 0,\sigma^2 \right).
	\end{align}
	
	In order to simplify the notation, let $\bm{x}(\bm{s}_i) \equiv \bm{x}_i \equiv \left( 1, x_{i1}, \dots, x_{ip} \right)'$, $\bm{\beta}(\bm{s}_i) \equiv \bm{\beta}_i \equiv \left(\beta_{i0}, \beta_{i1}, \dots, \beta_{ip} \right)'$, and $y(\bm{s}_i) \equiv y_i$.  Equations (\ref{eq:lm(s)}) and (\ref{eq:err}) can now be rewritten as
	\begin{align}
		y_i = \bm{x}'_i \bm{\beta}_i + \varepsilon_i \text{ and } \varepsilon_i \overset{iid}{\sim} \mathcal{N} \left( 0,\sigma^2 \right).
	\end{align}
	
	Further, let $\bm{X} = \left( \bm{x}_1, \dots, \bm{x}_n \right)'$ and $\bm{y} = \left( y_1, \dots, y_n \right)'$. Thus, conditional on the design matrix $\bm{X}$, observations of the response variable at different locations are independent of each other. Then, a total log-likelihood of the observed data is the sum of the log-likelihood of each individual observation:
	 \begin{align}
	 	\ell\left( \bm{\beta} \right) = - \left(1/2\right) \left\{ n \log \left( 2 \pi \sigma^2\right) +  \left(\sigma^{2}\right)^{-1}  \sum_{i=1}^n \left(y_i - \bm{x}'_i\bm{\beta}_i \right)^2  \right\}.
	\end{align}
	
	Since there are a total of $n \times (p+1)$ free parameters for $n$ observations, the model is not identifiable and it is not possible to directly maximize the total likelihood. One way to effectively reduce the number of parameters is to assume that the coefficients $\bm{\beta}(\bm{s})$ are smoothly varying over space, and use a kernel smoother to make pointwise estimates of the coefficients by maximizing a local likelihood. In the setting of spatial data and with the kernel smoother based on the physical distance between sampling locations, this is the traditional GWR.
		
	\subsection{Estimation}		
	In the traditional GWR, the coefficient surface $\bm{\beta}(\bm{s})$ is estimated at each sampling location $\bm{s}_i$. First calculate the Euclidean distance $\delta_{ii'} \equiv \delta\left(\bm{s}_i, \bm{s}_{i'}\right) \equiv \|\bm{s}_i  -\bm{s}_{i'}\|_2$ between locations $\bm{s}_i$ and $\bm{s}_{i'}$ for all $i, i' = 1, \dots, n$. The bi-square kernel can be used to generate spatial weights based on the Euclidean distances and a bandwidth $\phi$:
	
	\begin{align}\label{eq:bisquare}
		w_{ii'} = \begin{cases} \left[1-\left(\phi^{-1}\delta_{ii'}\right)^2\right]^2 &\mbox{ if } \delta_{ii'} < \phi, \\ 0 &\mbox{ if } \delta_{ii'} \geq \phi. \end{cases}
	\end{align}
	
	The bisquare kernel in (\ref{eq:bisquare}) assigns the maximum weight of one where $\bm{s}_i = \bm{s}_{i'}$ (i.e. $\delta_{ii'}=0$), is continuously differentiable, and assigns zero weight to observations at distances greater than one bandwidth from $\bm{s}_i$. For the purpose of estimation, define a local likelihood at each location:
	\begin{align}\label{eq:local-likelihood}
		\mathcal{L}_i \left(\bm{\beta}_i \right) &= \prod_{i'=1}^n \left[ \left(2 \pi \sigma^2_i  \right)^{-1/2}  \exp\left\{-\left(2 \sigma^2_i\right)^{-1}  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right\} \right] ^ {w_{ii'}},
	\end{align}			
	where $\sigma_i^2$ is a local approximation to the error variance $\sigma^2$ \citep{Fotheringham:2002}. Thus, the local log-likelihood function is, up to an additive constant:
	\begin{align}\label{eq:local-log-likelihood}
		\ell_i\left(\bm{\beta}_i\right) &= -(1/2) \sum_{i'=1}^n w_{ii'} \left\{ \log{\sigma^2_i}  + \left(\sigma^2_i\right)^{-1}  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right\}.
	\end{align}
	
	The GWR coefficient estimates $\hat{\bm{\beta}}_{i,\text{GWR}}$ maximize the local likelihood at location $\bm{s}_i$. From (\ref{eq:local-likelihood}) and (\ref{eq:local-log-likelihood}), it is apparent that $\hat{\bm{\beta}}_{i,\text{GWR}}$ can be obtained using weighted least squares. Let $\bm{W}_i$ denote a diagonal weight matrix with diagonal entries $w_{ii'}$ for $i'=1, \dots, n$. That is,
	\begin{align}
		\bm{W}_i =  {\rm diag}\left\{w_{ii'}\right\}_{i' = 1}^n.
	\end{align}
	
	It follows that, by weighted least squares,
	\begin{align}
		\hat{\bm{\beta}}_{i, \text{GWR}} = \left( \bm{X}'\bm{W}_i\bm{X} \right)^{-1} \bm{X}'\bm{W}_i\bm{y}.
	\end{align}
	
	The estimate of $\sigma_i^2$ is obtained by maximizing (\ref{eq:local-log-likelihood}), and is:
	\begin{align}
		\hat{\sigma}^2_i &= \left(\bm{1}_n'\bm{w}_i \right)^{-1} \left(\bm{y}-\bm{X}\left(\bm{X}'\bm{W}_i\bm{X}\right)^{-1}\bm{X}'\bm{W}_i\bm{y}\right)' \bm{W}_i \left(\bm{y}-\bm{X}\left(\bm{X}'\bm{W}_i\bm{X}\right)^{-1}\bm{X}'\bm{W}_i\bm{y}\right) \notag \\
		&= \left(\bm{1}_n'\bm{w}_i \right)^{-1}  \left(\bm{y}-\hat{\bm{y}}\right)' \bm{W}_i \left(\bm{y}-\hat{\bm{y}}\right),
	\end{align}	
	where $\bm{1}_n$ is an $n$-variate vector of ones.
	
	\begin{comment}
	Estimation of $\hat{\bm{\beta}}_i$ and $\hat{\sigma}_i$ is by maximum local likelihood, which is implemented by setting the derivatives of (\ref{eq:local-log-likelihood}) to zero:
	\begin{eqnarray}
		\left\{\frac{\partial \ell_i}{\partial \bm{\beta}_i} \right\}_j =   \sum_{i'=1}^n \left\{ x_{i'j} w_{ii'} \sigma^{-2}_i \left( y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right) \right\} \\
		\frac{\partial \ell_i}{\partial \sigma_i^2} \bigg|_{\hat{\beta}_i} &=& -\frac{1}{2} \sum_{i'=1}^n w_{ii'} \left\{ \left(\sigma_i^{2}\right)^{-1} - \left(\sigma_i^{2}\right)^{-2} \left( y_i - \bm{x}_i'\bm{\hat{\beta}}_i \right)^2 \right\} \\
		\hat{\sigma}_i^2 &=& \left(\sum_{i'=1}^n w_{ii'}\right)^{-1} \sum_{i'=1}^n w_{ii'} \left(y_i - \bm{x}_i'\hat{\bm{\beta}}_i\right)
	\end{eqnarray}
		
		\left\{\frac{\partial^2 \ell_i}{\partial \bm{\beta}_i \partial \bm{\beta}'_i} \right\}_{j,k} = -\sum_{i'=1}^n \left\{ x_{i'j} x_{i'k} w_{ii'} \sigma^{-2}_i \right\}
	\end{eqnarray}
	
	So the observed Fisher information in the locally weighted sample is
	\begin{align}
		\bm{\mathcal{J}}_i &=& \sigma^{-2}_i \left( \begin{array}{ccc} \sum_{i'=1}^n  w_{ii'} x^2_{i'1}   & \dots & \sum_{i'=1}^n w_{ii'} x_{i'1} x_{i'p}   \\ \vdots & \ddots & \vdots \\ \sum_{i'=1}^n  w_{ii'} x_{i'p} x_{i'1}    & \dots & \sum_{i'=1}^n  w_{ii'} x^2_{i'p}  \end{array} \right) \notag\\
		&=& \sigma^{-2}_i \sum_{i'=1}^n w_{ii'}\left( \begin{array}{ccc}  x^2_{i'1} & \dots & x_{i'1} x_{i'p} \\ \vdots & \ddots & \vdots \\ x_{i'p} x_{i'1} & \dots &  x^2_{i'p} \end{array} \right)\notag \\
		&=& \sigma^{-2}_i \sum_{i'=1}^n w_{ii'} \bm{x}_{i'} \bm{x}'_{i'}
	\end{align}	
	
	The form of the observed Fisher information suggests that the information in the data $\bm{x}_{i'}$ about the coefficients at location $s_i$ is proportional to the weight $w_{ii'}$.
	\end{comment}

	
	 
	
\section{Model Selection \label{section:model-selection}}
	\subsection{Local Variable Selection}
	Both the adaptive Lasso (AL) and the adaptive elastic net (AEN) are explored as penalty functions for local variable selection in GWR models. 	
	The proposed local variable selection with AL penalty is an $\ell_1$ regularization method for variable selection in regression models \citep{Zou:2006}. Unlike the traditional Lasso penalty, which applies an equal penalty to each covariate in the local model at $\bm{s}_i$, the AL adjusts the penalty of each covariate based on the covariate's unpenalized local coefficient.	
	
	The proposed local variable selection with AEN penalty generalizes the AL penalty to include an additional ridge penalty \citep{Zou:2009}. Ridge regression is an $\ell_2$ regularization technique that differs from the Lasso in that the ridge penalty is applied to the sum of the squared local regression coefficients \citep{Hoerl:1970}. The ridge penalty is used to estimate coefficients in regression models with correlated covariates because it stabilizes the inversion of the covariance matrix, which improves the robustness of the coefficient estimates \citep{Hastie:2009}.
	
	In fact, since the AL is an $\ell_1$ regularization method while the AEN is a combined $\ell_1$ and $\ell_2$ regularization method, the AL can be viewed as a special case of the AEN where the $\ell_2$ penalty is set to zero.
	
	\subsubsection{Local variable selection with the adaptive Lasso}
	The objective function for the local geographically weighted adaptive Lasso (GWAL) method at $\bm{s}_i$ is defined to be
	\begin{align}\label{eq:adaptive-lasso-WLS}
		\mathcal{S}(\bm{\beta}_i) = \sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 +  \lambda_i \sum_{j=1}^p |\beta_{ij}| / \gamma_{ij},
	\end{align}
	where $\sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2$ is the weighted sum of squares minimized by traditional GWR, and $\lambda_i \sum_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ is the AL penalty. With the vector of unpenalized local coefficients $\bm{\gamma}_i$, the AL penalty for the $j$th coefficient $\beta_{ij}$ at location $\bm{s}_i$ is $\lambda_i / \gamma_{ij}$, where $\lambda_i > 0$ is a the local penalty that applies to all coefficients at location $\bm{s}_i$ and $\bm{\gamma}_i = \left(\gamma_{i1}, \dots, \gamma_{ip}\right)'$ is the vector of adaptive weights at location $\bm{s}_i$.
	
	\begin{comment}\paragraph{Computation} To compute coefficient estimates at location $\bm{s}_i$ by the GWAL, the design matrix $\bm{X}$ is first multiplied by $\bm{W}_i^{1/2}$, the diagonal matrix of geographic weights at $\bm{s}_i$. Since some of the weights $w_{ii'}$ may be zero, the matrix $\bm{W}_i^{1/2}\bm{X}$ is not of full rank. The matrices $\bm{Y}_i^*$, $\bm{X}_i^*$, and $\bm{W}_i^*$ are formed by dropping the rows of $\bm{X}$  and $\bm{W}_i$ that correspond to observations with zero weight in the regression model at location $\bm{s}_i$. Now, letting $\bm{U}_i^* = \bm{W}_i^{*1/2} \bm{X}_i^*$ and $\bm{V}_i^* = \bm{W}_i^{*1/2} \bm{Y}_i^*$, we seek to estimate the coefficients $\bm{\beta}_i$ of the regression model:	
	\begin{align}
		\bm{V}_i^* = \bm{U}_i^* \bm{\beta}_i + \bm{\varepsilon}
	\end{align}
	
	Each column of $\bm{U}_i^*$ is centered around zero and rescaled to have an $\ell_2$-norm of one. Let $\widetilde{\bm{U}}_i^*$ denote the centered-and-scaled version of $\bm{U}_i^*$. Now the adaptive weights $\bm{\gamma}_i^*$ are calculated via least squares:	
	\begin{align}\label{eq:adaptive-weights-regression}
		\bm{\gamma}_i = \left( \widetilde{\bm{U}}_i^{*'} \widetilde{\bm{U}}_i^* \right)^{-1} \widetilde{\bm{U}}_i^{*'} \bm{V}_i^*
	\end{align}
	
	For $j=1, \dots, p$, the $j$th column of $\tilde{\bm{U}}_i^*$ is multiplied by $\gamma_{ij}$, the $j^\text{th}$ element of $\bm{\gamma}_i$. Call this rescaled matrix $\widecheck{\bm{U}}_i^*$.
	
	Finally, the AL coefficient estimates minimizing (\ref{eq:adaptive-lasso-WLS}) at location $\bm{s}_i$ are found, either by using the {\tt lars} algorithm \citep{Efron:2004b} to model $\bm{V}_i^*$ as a function of $\widecheck{\bm{U}}_i^*$ or by using the \verb!glmnet! package to implement coordinate descent.
	\end{comment}

	\subsubsection{Local variable selection with the adaptive elastic net}
	The objective function for the local geographically weighted adaptive elastic net (GWAEN) method at $\bm{s}_i$ is defined to be
	\begin{align}
		\mathcal{S}\left(\bm{\beta}_i\right) &= \sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 + \alpha_i \lambda_i \sum_{j=1}^p |\beta_{ij}| / \gamma_{ij} + (1-\alpha_i) \lambda_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2\notag\\
		&= \sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 + \lambda_i \left\{\alpha_i \sum_{j=1}^p |\beta_{ij}| / \gamma_{ij} + (1-\alpha_i) \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2 \right\}
	\end{align}
	
	where the adaptive weights $\bm{\gamma}_i = \left(\gamma_{i1}, \dots, \gamma_{ip}\right)'$ are calculated as for the AL, and the elastic net parameter $\alpha_i$ controls the balance between the $\ell_1$ and $\ell_2$ penalties.
	
	Fitting a SVCR model by the GWAEN requires selecting the vector of elastic net parameters $\bm{\alpha} = \left( \alpha_1, \dots, \alpha_n \right)$. In the simulation study (Section \ref{sec:simulation}), the elastic net parameter is chosen globally ($\alpha_i \equiv \alpha$ for $i=1, \dots, n$). The global elastic net parameter is calculated as $\alpha = 1-\rho_{\text{max}}$ where $\rho_{\text{max}}$ is the maximum global (i.e. for all data without weighting) Pearson correlation between any two covariates.

	\subsection{Tuning Parameter Selection}	
	A local tuning parameter $\lambda_i$ is required for the variable selection step of fitting each local model by the GWAL or GWAEN method. To select $\lambda_i$, we propose a locally-weighted version of the Bayesian Information Criterion (BIC) \citep{Schwarz-1978} which we call the local BIC ($\text{BIC}_{\text{loc}}$):
	
	\begin{align}
		\mbox{BIC}_{\text{loc}, i} &= -2 \sum_{i'=1}^n \ell_{ii'}  + \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i\notag\\
		&= -2 \times \sum_{i'=1}^n \log \left\{ \left(2 \pi \hat{\sigma}_i^2\right)^{-1/2} \exp \left[-\frac{1}{2} \hat{\sigma}_i^{-2} \left(y_{i'} - \bm{x}_{i'}' \hat{\bm{\beta}}_{i'} \right)^2\right] \right\}^{w_{ii'}} + \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i\notag\\
		&= \sum_{i'=1}^n w_{ii'} \left\{ \log \left(2 \pi \right) + \log \hat{\sigma}_i^2 + \hat{\sigma}_i^{-2} \left(y_{i'} - \bm{x}_{i'}' \hat{\bm{\beta}}_{i'} \right)^2 \right\} + \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i \notag\\
		&= \hat{\sigma}_i^{-2} \sum_{i'=1}^n w_{ii'} \left( y_{i'} - \bm{x}'_{i'} \hat{\bm{\beta}}_i \right)^2 + \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i + C_i
	\end{align}
	where $C_i = \sum \limits_{i'=1}^n w_{ii'} \left\{\log{2\pi} + \log{\hat{\sigma}_i^2} \right\}$.
	
	 The local BIC is calculated by adding a penalty to the local likelihood, with the sum of the weights around $\bm{s}_i$, $\sum_{i'=1}^n w_{ii'}$, playing the role of the sample size and the ``degrees of freedom" $\left( \df_i \right)$ at $\bm{s}_i$ given by the number of nonzero coefficients in $\bm{\beta}_i$ \citep{Zou:2007}. Since the estimated variance $\hat{\sigma}_i^2$ is the variance estimate from the unpenalized local model, $C_i$ does not depend on the choice of tuning parameter and can be ignored \citep{Zou:2007}.
	
	\cite{Wheeler:2009} proposed selecting the tuning parameter for the Lasso at location $\bm{s}_i$ to minimize the jackknife prediction error $|y_i - \hat{y}_i^{(i)}|$. Because the jackknife prediction error is undefined everywhere except for at observation locations, this choice restricts coefficient estimation to occur at the locations where data has been observed. By contrast, the local BIC can be calculated at any location where the local log-likelihood can be obtained. As a practical matter this allows for variable selection and coefficient surface estimation to be done at locations where no data are observed and for imputation of missing values of the response variable.
	
	\subsection{Coefficient estimation}
	After the variables are selected for inclusion in the local model, either by the GWAL or the GWAEN, the coefficient estimates are computed via weighted least squares on the selected variables without regularization. That is, letting $\bm{\Omega}_i = \text{diag}\left\{\bm{\omega}_i\right\}$ where $\bm{\omega}_i = \left(\omega_{i1}, \dots \omega_{ip}\right)'$ and $\omega_{ij}= I \left(\beta_{ij} \neq 0\right)$, the local coefficient estimates are:
	\begin{align} \label{eq:coefficients}
		\hat{\bm{\beta}}_i &= \argmin \limits_{\beta} \sum_{i'=1}^n w_{ii'} \left( y_{i'} - \bm{x}_{i'}' \bm{\Omega}_i \bm{\beta}_i' \right)^2\\
		&= \left( \tilde{\bm{X}}'\bm{W}_i \tilde{\bm{X}} \right)^{-1} \tilde{\bm{X}} '\bm{W}_i\bm{y}.
	\end{align}
	and $\tilde{\bm{X}}_i$ is the design matrix $\bm{X}$ with columns corresponding to zeroes in $\bm{\omega}_i$ removed.
	 
	\subsection{Bandwidth selection}
	Let $H_i$ denote the $i$th row of the matrix $\bm{W}_i^{1/2} \tilde{\bm{X}} \left( \tilde{\bm{X}}'\bm{W}_i \tilde{\bm{X}} \right)^{-1} \tilde{\bm{X}}'\bm{W}_i^{1/2}$, and let	
	\begin{align}
		\bm{H} = \left(H_1 \cdots H_n \right)'.
	\end{align}	
	The fitted values from the model are	
	\begin{align}
		\hat{\bm{y}} = \bm{H} \bm{y}
	\end{align}	
	The global bandwidth parameter $\phi$ in (\ref{eq:bisquare}) is selected by minimizing an approximation to the global AIC:	
	\begin{align}
		\text{AIC} = 2 n \log{\sigma} + n \left\{\frac{n + \nu}{n - 2 - \nu}\right\}
	\end{align}	
	where $\nu$ is the trace of the smoothing matrix $\bm{H}$, and approximates the total degrees of freedom of the SVCR \citep{Hurvich:1998}.
	
	%The bandwidth parameter $\phi$ in (\ref{eq:bisquare}) is global and so a global statistic is needed, by which prospective bandwidths can be compared. We propose the following statistic, called the total BIC ($\mbox{BIC}_{\text{tot}}$):
	%\begin{align}\label{eq:total-BIC}
	%	\mbox{BIC}_{\mbox{tot}} &= \sum_{i=1}^n \left\{ \log \hat{\sigma}_i^2 + \hat{\sigma}_i^{-2} \left(y_i - \bm{x}'_i \hat{\bm{\beta}}_i \right)^2 + \left( \sum_{i'=1}^n w_{ii'}\right) \left(\sum_{i'=1}^n w_{ii'} \right)^{-1} \mbox{df}_i \right\}
	%\end{align}
	%which is different than the BIC for traditional GWR \citep{Fotheringham:2002}. The BIC for traditional GWR is based on the trace of the projection matrix of the GWR model. But using an $\ell_1$ penalty (as in the adaptive Lasso or the adaptive elastic net) results in a non-linear smoother, as can be seen by equating the derivatives of (\ref{eq:adaptive-lasso-WLS}) with respect to $\bm{\beta}$ to zero to obtain the maximum likelihood estimates $\hat{\bm{\beta}}_i$ \citep{Zou:2007}.
	%\begin{align}
	%	\hat{\bm{\beta}}_i &= \left( \bm{X}'\bm{W}_i\bm{X} \right)^{-1}  \bm{X}'\bm{W}_i\bm{y}  - (1/2) \left(\bm{X}'\bm{W}_i\bm{X} \right)^{-1} \bm{\lambda}_i \notag \\
	%	\hat{y}_i = \bm{x}_i' \hat{\bm{\beta}}_i &=  \bm{x}_i' \left( \bm{X}'\bm{W}_i\bm{X} \right)^{-1}  \bm{X}'\bm{W}_i\bm{y}  - (1/2) \bm{x}_i' \left(\bm{X}'\bm{W}_i\bm{X} \right)^{-1} \bm{\lambda}_i \notag
	%\end{align}
	%where $\bm{\lambda}_i$ is a vector of adaptive Lasso penalties, the $j^{\text{th}}$ component of which is $|\lambda_i / \gamma_{ij}|$.
	
	%Because of the kernel weights and the application of the adaptive Lasso, the sample size and the degrees of freedom are different for each local model. The total BIC is found by summing the local likelihood and local penalty over all of the locations $\bm{s}_i$. The local penalty requires that we calculate $\text{df}_{\text{tot}}$, the total degrees of freedom used by the local models.
	
	%The expression for $\text{df}_{\text{tot}}$ is derived by analogy. It is certainly true that $\text{df}_{\text{tot}} = \sum\limits_{i=1}^n \left( n^{-1} \text{df}_{\text{tot}} \right)$, which suggests that the total degrees of freedom is the sum of the local degrees of freedom. Rather than using the mean local degrees of freedom, $n^{-1} \text{df}_{\text{tot}}$ at each location, substitute $\text{df}_i$ for $\text{df}_{\text{tot}}$ and $\sum\limits_{i'=1}^n w_{ii'}$ for $n$ to get 
	%\begin{align}
	%	\text{df}_{\text{tot}} &= \sum\limits_{i=1}^n \text{df}_i  \left( \sum\limits_{i'=1}^n w_{ii'} \right)^{-1}
	%\end{align}

\section{Simulation \label{sec:simulation}}
	\subsection{Simulation Setup}
	A simulation study was conducted to assess the performance of the method described in Sections \ref{section:GWR}--\ref{section:model-selection}. 
	
	Data were simulated on the spatial domain $[0,1]^2$, which was divided into a $30 \times 30$ grid. Each of $p=5$ covariates $X_1, \dots, X_5$ was simulated by a Gaussian random field (GRF) with mean zero and exponential spatial covariance $\text{Cov} \left(X_{ji}, X_{ji'} \right) = \sigma_x^2 \exp{\left( -\tau_x^{-1} \delta_{ii'} \right)}$ where $\sigma_x^2=1$ is the variance, $\tau_x = 0$ is the range parameter, and $\delta_{ii'}$ is the Euclidean distance $\|\bm{s}_i - \bm{s}_{i'}\|_2$. Correlation was induced between the covariates by multiplying the $\bm{X}$ matrix by $\bm{R}$, where $\bm{R}$ is the Cholesky decomposition of the covariance matrix $\bm{\Sigma} = \bm{R}'\bm{R}$. The covariance matrix $\bm{\Sigma}$ is a $5 \times 5$ matrix that has ones on the diagonal and $\rho$ for all off-diagonal entries, where $\rho$ is the between-covariate correlation.
		
	The simulated response was $y_i = \bm{x}'_i \bm{\beta}_i + \varepsilon_i$ for $i=1, \dots, n$ where $n=900$ and for simplicity the $\varepsilon_i$'s were iid Gaussian with mean zero and variance $\sigma_\varepsilon^2$. The simulated data included the response $y$ and five covariates $x_1, \dots, x_5$. The true data-generating model uses only $x_1$, so $x_2, \dots, x_5$ are included to assess performance in variable-selection.
	
	There were twelve simulation settings, each of which was simulated 100 times. For each of the twelve settings, $\beta_1(\bm{s})$, the true coefficient surface for $x_1$, was nonzero in at least part of the spatial domain $[0,1]^2$. There were four other simulated covariates, but their true coefficient surfaces were zero across the area under simulation. The twelve simulation settings are described in Table \ref{table:simulation_settings}. Three parameters were varied to produce the twelve settings: there were three functional forms for the coefficient surface $\beta_1(\bm{s})$, data was simulated both with ($\rho = 0.5$) and without ($\rho = 0$) correlation between the covariates, and simulations were made with low ($\sigma_\varepsilon^2 = 0.25$) and high ($\sigma_\varepsilon^2 = 1$) variance for the random error term.
	
	The three coefficient surfaces used to produce the response variable in the simulations are pictured in Figure \ref{fig:sim-actual}. The first is a ``step" function, which is equal to zero in 40\% of the spatial domain, equal to one in a different 40\% of the spatial domain, and increases linearly in the middle 20\% of the domain. The second is a gradient function, which increases linearly from zero at one end of the domain to one at the other. The final coefficient function is a parabola taking its maximum value of 0.535 at the center of the domain and falling to zero at each corner of the domain. The parabola is computed by finding the squared distance of each sampling location from the domain's center, multiplying by -1 and then adding an offset so that the corner points are equal to zero.
	
	The performance of the penalized GWR methods (AL via {\tt lars} and via {\tt glmnet}, and the AEN  via {\tt enet}) was compared to that of oracular GWR (O-GWR), which is ordinary GWR with ``oracular" variable selection, meaning that exactly the correct set of covariates was used to fit the GWR model at each location in the simulation. Also included in the comparison was the GWR algorithm of \cite{Fotheringham:2002} without variable selection ({\tt gwr}). Finally, there is a category of simulation results using the three penalized GWR methods for local variable selection and then ordinary GWR for coefficient estimation.
	
% latex table generated in R 2.15.1 by xtable 1.7-0 package
% Fri Jan 18 10:19:47 2013
\begin{table}[h!]
	\begin{center}
	\begin{tabular}{ccc}
		\hline
		Setting & function & $\rho$ \\ 
		\hline
		1 & step & 0 \\ 
		4 & step & 0.5 \\ 
		\hline
		5 & gradient & 0 \\ 
		8 & gradient & 0.5 \\ 
		\hline
		10 & parabola & 0 \\ 
		11 & parabola & 0.5 \\ 
	\end{tabular}
	\end{center}
	\caption{Simulation parameters for each setting.\label{table:simulation_settings}}
\end{table}

	\begin{figure}
		\begin{center}
			\includegraphics[width=0.32\textwidth]{../../figures/simulation/step.pdf}
			\includegraphics[width=0.32\textwidth]{../../figures/simulation/gradient.pdf}
			\includegraphics[width=0.32\textwidth]{../../figures/simulation/parabola.pdf}
			\caption{The actual $\beta_1$ coefficient surface used in the simulation.\label{fig:sim-actual}}
		\end{center}
	\end{figure}
	
	\begin{figure}
		\begin{center}
			\includegraphics[width=0.5\textwidth]{../../figures/simulation/illustrations/summary-locations.pdf}
			\caption{Locations where the variable selection and coefficient estimation of GWL were summarized.\label{fig:summary-locations}}
		\end{center}
	\end{figure}
	
	
	Results from the simulation were summarized at five locations on the simulated grid (see Figure \ref{fig:summary-locations}). The five key locations were chosen because they represent interesting regions of the $\beta_1$ coefficient surfaces. The results of variable selection and coefficient estimation are presented in the tables below.
	
	%Results of the simulation experiment were summarized to asses the consistency in selection and estimation, as well as the coverage properties of the confidence intervals. The confidence intervals based on the bootstrap (without shrinkage) were used for the GWL because they seemed to uniformly outperform the other options.\\ table:loc1-X1-BiasX
	
		
	\subsection{Simulation results}
	%At locations where $\beta_1$ is nonzero, $X_1$ usually selected for inclusion in all or nearly all of the model runs. An exception is at location four for the step function, where $X_1$ was included in about half of the model runs. This is probably because location four is at the very point where $\beta_1$ transitions from zero to nonzero. Selection performance was relatively poor for the step function at location one, especially for data with $\sigma^2 = 1$. For those simulations, $X_1$ was correctly included in around 85\% of the simulations. The bias, variance, and MSE of $\hat{\beta}_1$ under the same settings were also much larger than the baseline established by the standard \verb!gwr! algorithm. The reason(s) for the poor performance under those particular conditions is currently unknown.\\
	
	\paragraph{Selection} Table \ref{table:selection} lists the results of variable selection. The correct covariate was usually included in the local models, and the unimportant covariates were usually excluded. Arguably the least-accurate selection was at locations one and five for the step function using the {\tt lars} algorithm, where variables that do not appear in the true model were selected for inclusion at rates between 11\% and 22\%. The {\tt enet} and {\tt glmnet} algorithms, using the same data, had false-positive errors at rates between 0\% and 8\%, which are typical of the error rates for all other location/function/algorithm combinations.
	
	Selection performance was more affected by an increase in the noise variance from $\sigma_{\varepsilon}=0.5$ to $\sigma_{\varepsilon}=1$ than by an increase in colinearity from $\rho = 0$ to $\rho = 0.5$. For instance, for the step function at location three, $\beta_1(\bm{s}_3) = 0.5$. Where $\sigma_{\varepsilon}=0.5$, the {\tt glmnet} algorithm selected $\beta_1(\bm{s}_3) $ for inclusion at a rate of 100\% (when $\rho = 0$) and 99\% (when $\rho = 0.5$). But when $\sigma_{\varepsilon}=1$, the same algorithm selected $\beta_1(\bm{s}_3)$ for inclusion at a rate of 75\% (when $\rho = 0$) and 68\% (when $\rho = 0.5$).
	
	The \verb!enet! algorithm outperforms the others in selection but the difference is small - a roughly one percentage point improvement in the rate of true positives and true negatives when $\rho = 0.5$.There is no apparent difference between {\tt glmnet} and {\tt enet} when $\rho = 0$.  
	
	%Otherwise, selection performance was good, with the rate of false positive selections for $X_2$--$X_5$ (and for $X_1$ where its true coefficient was zero) usually below 0.10. Selection (also bias, variance, and MSE of $\hat{\beta}_1(\bm{s})$) tended to suffer worse by the change from low to high error variance than by the change from low to high collinearity amongst the predictors.\\
	
	\paragraph{Coefficient Estimation} The MSE, bias, and variance of $\hat{\beta}_1$ are listed in Tables \ref{table:X1-mse}, \ref{table:X1-bias}, and \ref{table:X1-var}, respectively. The method of oracular selection led to the best MSE in 28 of the 60 cases, which is more than any other single method. In general, the methods that do local variable selection had lower MSE than traditional GWR. As was the case for selection, estimation accuracy (in terms of MSE) suffered more by an increase in $\sigma_{\varepsilon}$ from 0.5 to 1 than from an increase in $\rho$ from 0 to 0.5. Oracular selection was decisively superior to traditional GWR and to local variable selection for estimating the gradient $\beta_1$, turning in the best MSE for all combinations of location and simulation parameters.
	
	In general, oracular selection and traditional GWR were quite similar in terms of $\text{var}\left(\hat{\beta_1}\right)$, with notably greater variance for the local selection methods. However, the local selection methods had less bias than traditional GWR, even exhibiting less bias than oracular selection in many settings. There was no simulation setting for which traditional GWR had the smallest or second-smallest bias.
	
	It seems, therefore, that the local selection methods reduce bias and increase variance of the coefficient estimates, as compared to traditional GWR. Whether traditional GWR or local selection is better in terms of MSE of the coefficient estimates is not clear in all cases, but when the actual coefficient is equal to zero (or nearly so), local selection does seem to reduce the MSE over traditional GWR.
	
	%There was not a clear and consistent difference in performance between the three selection methods. It might be expected that the adaptive elastic net would outperform the adaptive Lasso under greater covariate collinearity, but if such effect is real it is not apparent from this simulation. The unshrunk coefficient-estimation methods tended to exhibit more bias than the selection-plus-shrinkage methods when the true coefficient value was near zero, and vice versa when the true coefficient was not near zero. The unshrunk methods were perhaps more consistent in their performance and for that reason they are probably preferable in practice.\\
	
	%Bias in coefficient estimation was greater and variance less for the standard \verb!gwr! algorithm than for the methods described here. This is probably due to the fact that the methods described here show a preference for smaller bandwidths than those select by \verb!gwr!. Accuracy (as measured by MSE) in fitting the true Y variables was comparable for all the methods. \\	
	
	\paragraph{Fitted Values} The MSE of the $\hat{Y}$, $\text{MSE}\left(\hat{Y}\right)$, is listed in Table \ref{table:Y-mse}. Nominally, $\text{MSE}\left(\hat{Y}\right)$ should be equal to the noise variance, $\sigma_{\varepsilon}^2$, which is 1 for odd-numbered rows and 0.25 for even numbered rows. There is not much difference in $\text{MSE}\left(\hat{Y}\right)$ between the various estimation methods, except that it is larger for the oracular and \verb!gwr! methods where $\beta_1(\bm{s})$ is near or equal to zero.
	
	\subsection{Tables}
		\subsubsection{Selection}
		% latex table generated in R 2.15.2 by xtable 1.7-1 package
% Thu Aug 29 16:21:16 2013
		\begin{table}
		\thispagestyle{empty}
		\begin{center}
		\setlength\tabcolsep{4pt}	
\begin{tabular}{ccc|cc|cc|cc|cc|cc}
& \multicolumn{4}{c}{step} & \multicolumn{4}{|c|}{gradient} & \multicolumn{4}{c}{parabola} \\
& \multicolumn{2}{c}{{\tt enet}} & \multicolumn{2}{c|}{{\tt glmnet}} & \multicolumn{2}{c}{{\tt enet}} & \multicolumn{2}{c|}{{\tt glmnet}} & \multicolumn{2}{c}{{\tt enet}} & \multicolumn{2}{c}{{\tt glmnet}}\\
 location & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ \\ 
  \hline
   \multirow{2}{*}{1} & 1.00 & 0.04 & 1.00 & 0.05 & 0.99 & 0.10 & 0.96 & 0.08 & 1.00 & 0.03 & 1.00 & 0.02 \\ 
   & 0.86 & 0.08 & 0.82 & 0.07 & 0.84 & 0.07 & 0.88 & 0.05 & 0.97 & 0.06 & 0.97 & 0.07 \\ 
      \cline{2-13}
   \multirow{2}{*}{2} & 1.00 & 0.07 & 1.00 & 0.06 & 1.00 & 0.06 & 1.00 & 0.05 & 1.00 & 0.08 & 1.00 & 0.07 \\ 
   & 1.00 & 0.06 & 1.00 & 0.06 & 1.00 & 0.07 & 0.99 & 0.04 & 0.98 & 0.08 & 0.99 & 0.07 \\ 
      \cline{2-13}
   \multirow{2}{*}{3} & 0.99 & 0.06 & 0.99 & 0.06 & 0.97 & 0.08 & 0.92 & 0.04 & 1.00 & 0.08 & 1.00 & 0.07 \\ 
   & 0.84 & 0.08 & 0.82 & 0.07 & 0.81 & 0.11 & 0.80 & 0.08 & 0.95 & 0.08 & 0.96 & 0.08 \\ 
      \cline{2-13}
   \multirow{2}{*}{4} & 0.64 & 0.06 & 0.59 & 0.06 & 0.51 & 0.12 & 0.40 & 0.07 & 1.00 & 0.06 & 1.00 & 0.06 \\ 
   & 0.48 & 0.07 & 0.49 & 0.07 & 0.52 & 0.07 & 0.51 & 0.07 & 0.95 & 0.07 & 0.93 & 0.06 \\ 
      \cline{2-13}
   \multirow{2}{*}{5} & 0.03 & 0.03 & 0.03 & 0.03 & 0.02 & 0.03 & 0.03 & 0.05 & 0.93 & 0.05 & 0.94 & 0.04 \\ 
   & 0.06 & 0.04 & 0.04 & 0.05 & 0.04 & 0.03 & 0.06 & 0.06 & 0.70 & 0.07 & 0.70 & 0.07 \\ 
  \end{tabular}
		\caption{Selection frequency for the simulation experiment\label{table:selection}}
		\end{center}
		\end{table}
		


		\subsubsection{Estimation}
% latex table generated in R 2.15.2 by xtable 1.7-1 package
% Thu Aug 29 16:21:15 2013
		\begin{table}
		\thispagestyle{empty}
		\begin{center}
\begin{tabular}{cccccccc}
 function & location & enet & glmnet & u.enet & u.glmnet & oracular & gwr \\ 
  \hline
   \multirow{10}{*}{step} & \multirow{2}{*}{1} & 0.025 & \emph{0.023} & 0.127 & 0.124 & 0.082 & \textbf{0.005} \\ 
   &  & 0.186 & 0.216 & 0.376 & 0.375 & \emph{0.134} & \textbf{0.009} \\ 
   \cline{3-8}
   & \multirow{2}{*}{2} & 0.024 & 0.024 & \emph{0.021} & \textbf{0.021} & 0.021 & 0.042 \\ 
   &  & 0.063 & 0.068 & \emph{0.054} & 0.056 & \textbf{0.042} & 0.070 \\ 
   \cline{3-8}
   & \multirow{2}{*}{3} & 0.011 & 0.010 & 0.007 & 0.007 & \textbf{0.004} & \emph{0.005} \\ 
   &  & 0.043 & 0.047 & 0.049 & 0.054 & \emph{0.009} & \textbf{0.008} \\ 
   \cline{3-8}
   & \multirow{2}{*}{4} & \emph{0.014} & \textbf{0.014} & 0.019 & 0.018 & 0.021 & 0.042 \\ 
   &  & \textbf{0.036} & \emph{0.039} & 0.042 & 0.046 & 0.047 & 0.074 \\ 
   \cline{3-8}
   & \multirow{2}{*}{5} & \emph{0.001} & 0.002 & 0.004 & 0.004 & \textbf{0.000} & 0.007 \\ 
   &  & 0.006 & \emph{0.002} & 0.024 & 0.009 & \textbf{0.000} & 0.011 \\ 
   \hline
   \multirow{10}{*}{gradient} & \multirow{2}{*}{1} & \emph{0.045} & 0.073 & 0.134 & 0.205 & 0.101 & \textbf{0.011} \\ 
   &  & 0.218 & 0.179 & 0.425 & 0.369 & \emph{0.154} & \textbf{0.022} \\ 
   \cline{3-8}
   & \multirow{2}{*}{2} & 0.027 & 0.021 & 0.021 & \textbf{0.017} & \emph{0.018} & 0.044 \\ 
   &  & 0.071 & 0.071 & \emph{0.056} & 0.061 & \textbf{0.043} & 0.075 \\ 
   \cline{3-8}
   & \multirow{2}{*}{3} &  0.014 & 0.022 & 0.011 & 0.021 & \textbf{0.005} & \emph{0.005} \\ 
   &  & 0.047 & 0.045 & 0.045 & 0.044 & \emph{0.008} & \textbf{0.008} \\ 
   \cline{3-8}
   & \multirow{2}{*}{4} & \emph{0.012} & \textbf{0.011} & 0.016 & 0.014 & 0.020 & 0.044 \\ 
   &  & \textbf{0.028} & \emph{0.038} & 0.047 & 0.048 & 0.043 & 0.082 \\ 
   \cline{3-8}
   & \multirow{2}{*}{5} & \emph{0.002} & 0.003 & 0.009 & 0.009 & \textbf{0.000} & 0.010 \\ 
   &  & \emph{0.004} & 0.022 & 0.038 & 0.043 & \textbf{0.000} & 0.015 \\ 
   \hline
   \multirow{10}{*}{parabola} & \multirow{2}{*}{1} & 0.069 & 0.070 & \textbf{0.007} & \emph{0.007} & 0.010 & 0.016 \\ 
   &  & 0.094 & 0.096 & 0.078 & 0.085 & \emph{0.045} & \textbf{0.042} \\ 
   \cline{3-8}
   & \multirow{2}{*}{2} & 0.003 & 0.003 & \emph{0.001} & 0.001 & \textbf{0.001} & 0.001 \\ 
   &  & 0.013 & 0.008 & 0.013 & 0.009 & \emph{0.002} & \textbf{0.002} \\ 
   \cline{3-8}
   & \multirow{2}{*}{3} & 0.001 & 0.001 & 0.001 & \emph{0.001} & \textbf{0.001} & 0.001 \\ 
   &  & 0.017 & 0.015 & 0.019 & 0.017 & \emph{0.002} & \textbf{0.002} \\ 
   \cline{3-8}
   & \multirow{2}{*}{4} & 0.003 & 0.003 & 0.001 & \emph{0.001} & \textbf{0.001} & 0.001 \\ 
   &  & 0.014 & 0.016 & 0.012 & 0.015 & \textbf{0.002} & \emph{0.003} \\ 
   \cline{3-8}
   & \multirow{2}{*}{5} & 0.068 & 0.069 & 0.004 & \emph{0.004} & \textbf{0.000} & 0.016 \\ 
   &  & 0.051 & 0.052 & 0.019 & \emph{0.019} & \textbf{0.000} & 0.044 \\ 
  \end{tabular}
\caption{Mean squared error of $\hat{\beta_1}$ (\textbf{minimum}, \emph{next best}).\label{table:X1-mse}}
\end{center}
\end{table}



% latex table generated in R 2.15.2 by xtable 1.7-1 package
% Thu Aug 29 16:21:15 2013
		\begin{table}
		\thispagestyle{empty}
		\begin{center}
		\begin{tabular}{cccccccc}
 function & location & enet & glmnet & u.enet & u.glmnet & oracular & gwr \\ 
  \hline
  \multirow{10}{*}{step} & \multirow{2}{*}{1} & -0.029 & \emph{-0.020} & 0.038 & 0.033 & 0.034 & \textbf{-0.004} \\ 
   &  & -0.195 & -0.211 & -0.082 & -0.075 & \emph{0.053} & \textbf{-0.017} \\ 
   \cline{3-8}
   & \multirow{2}{*}{2} & -0.119 & -0.119 & \emph{-0.110} & \textbf{-0.110} & -0.124 & -0.196 \\ 
   &  & -0.178 & -0.186 & \textbf{-0.145} & \emph{-0.150} & -0.175 & -0.253 \\ 
   \cline{3-8}
   & \multirow{2}{*}{3} & \emph{-0.014} & \textbf{-0.010} & 0.017 & 0.015 & 0.021 & 0.040 \\ 
   &  & -0.027 & -0.031 & \emph{0.009} & \textbf{0.004} & 0.050 & 0.059 \\ 
   \cline{3-8}
   & \multirow{2}{*}{4} & \emph{0.059} & \textbf{0.049} & 0.074 & 0.065 & 0.129 & 0.196 \\ 
   &  & \textbf{0.075} & \emph{0.076} & 0.088 & 0.090 & 0.193 & 0.263 \\ 
   \cline{3-8}
   & \multirow{2}{*}{5} & \emph{-0.006} & -0.006 & -0.009 & -0.010 & \textbf{0.000} & -0.006 \\ 
   &  & -0.009 & \emph{-0.000} & -0.025 & -0.008 & \textbf{0.000} & -0.011 \\ 
   \hline
   \multirow{10}{*}{gradient} & \multirow{2}{*}{1} & -0.077 & -0.073 & 0.028 & \textbf{-0.014} & 0.050 & \emph{-0.017} \\ 
   &  & -0.214 & -0.167 & -0.067 & -0.068 & \emph{0.035} & \textbf{0.006} \\ 
   \cline{3-8}
   & \multirow{2}{*}{2} & -0.130 & \emph{-0.099} & -0.103 & \textbf{-0.083} & -0.110 & -0.199 \\ 
   &  & -0.221 & -0.216 & \textbf{-0.167} & -0.184 & \emph{-0.182} & -0.263 \\ 
   \cline{3-8}
   & \multirow{2}{*}{3} & -0.056 & -0.056 & \textbf{-0.009} & -0.030 & \emph{0.017} & 0.034 \\ 
   &  & -0.094 & -0.077 & -0.056 & -0.056 & \textbf{0.017} & \emph{0.055} \\ 
   \cline{3-8}
   & \multirow{2}{*}{4} & 0.027 & \textbf{0.010} & 0.043 & \emph{0.020} & 0.129 & 0.199 \\ 
   &  & \textbf{0.073} & \emph{0.089} & 0.105 & 0.105 & 0.189 & 0.275 \\ 
   \cline{3-8}
   & \multirow{2}{*}{5} & \emph{-0.005} & -0.009 & -0.009 & -0.012 & \textbf{0.000} & -0.009 \\ 
   &  & -0.011 & -0.011 & -0.036 & -0.021 & \textbf{0.000} & \emph{-0.007} \\ 
   \hline
   \multirow{10}{*}{parabola} & \multirow{2}{*}{1} & -0.248 & -0.253 & \textbf{0.010} & 0.011 & \emph{0.011} & -0.111 \\ 
   &  & -0.242 & -0.248 & \emph{-0.014} & -0.022 & \textbf{-0.007} & -0.182 \\ 
   \cline{3-8}
   & \multirow{2}{*}{2} & -0.047 & -0.048 & \emph{0.002} & \textbf{0.001} & 0.004 & 0.002 \\ 
   &  & -0.044 & -0.035 & \textbf{0.003} & 0.011 & \emph{0.008} & -0.011 \\ 
   \cline{3-8}
   & \multirow{2}{*}{3} & 0.005 & 0.005 & \emph{0.002} & \textbf{0.001} & 0.003 & 0.002 \\ 
   &  & -0.017 & -0.012 & -0.013 & -0.007 & \textbf{0.003} & \emph{0.006} \\ 
   \cline{3-8}
   & \multirow{2}{*}{4} & 0.043 & 0.045 & \emph{0.006} & 0.007 & \textbf{0.004} & 0.008 \\ 
   &  & 0.006 & \textbf{0.002} & -0.014 & -0.023 & \emph{0.004} & 0.020 \\ 
   \cline{3-8}
   & \multirow{2}{*}{5} & 0.249 & 0.253 & \emph{0.002} & 0.003 & \textbf{0.000} & 0.113 \\ 
   &  & 0.182 & 0.186 & \emph{-0.001} & 0.004 & \textbf{0.000} & 0.187 \\ 
  \end{tabular}
\caption{Bias of $\hat{\beta_1}$ (\textbf{minimum}, \emph{next best}).\label{table:X1-bias}} 
\end{center}
\end{table}


% latex table generated in R 2.15.2 by xtable 1.7-1 package
% Thu Aug 29 16:21:15 2013
		\begin{table}
		\thispagestyle{empty}
		\begin{center}
		\begin{tabular}{cccccccc}
 function & location & enet & glmnet & u.enet & u.glmnet & oracular & gwr \\ 
  \hline
   \multirow{10}{*}{step} & \multirow{2}{*}{1} & 0.024 & \emph{0.023} & 0.127 & 0.124 & 0.081 & \textbf{0.005} \\ 
   &  & 0.149 & 0.173 & 0.373 & 0.373 & \emph{0.133} & \textbf{0.009} \\ 
   \cline{3-8}
   & \multirow{2}{*}{2} & 0.010 & 0.010 & 0.009 & 0.009 & \emph{0.006} & \textbf{0.003} \\ 
   &  & 0.032 & 0.034 & 0.033 & 0.034 & \emph{0.012} & \textbf{0.006} \\ 
   \cline{3-8}
   & \multirow{2}{*}{3} & 0.011 & 0.010 & 0.007 & 0.007 & \emph{0.004} & \textbf{0.003} \\ 
   &  & 0.043 & 0.047 & 0.050 & 0.055 & \emph{0.007} & \textbf{0.004} \\ 
   \cline{3-8}
   & \multirow{2}{*}{4} & 0.011 & 0.012 & 0.014 & 0.014 & \emph{0.004} & \textbf{0.003} \\ 
   &  & 0.030 & 0.033 & 0.035 & 0.038 & \emph{0.009} & \textbf{0.005} \\ 
   \cline{3-8}
   & \multirow{2}{*}{5} & \emph{0.001} & 0.002 & 0.004 & 0.004 & \textbf{0.000} & 0.007 \\ 
   &  & 0.006 & \emph{0.002} & 0.024 & 0.009 & \textbf{0.000} & 0.011 \\ 
   \hline
   \multirow{10}{*}{gradient} & \multirow{2}{*}{1} & \emph{0.040} & 0.068 & 0.134 & 0.207 & 0.099 & \textbf{0.011} \\ 
   &  & 0.174 & \emph{0.153} & 0.424 & 0.368 & 0.154 & \textbf{0.022} \\ 
   \cline{3-8}
   & \multirow{2}{*}{2} & 0.011 & 0.012 & 0.010 & 0.010 & \emph{0.006} & \textbf{0.005} \\ 
   &  & 0.022 & 0.025 & 0.028 & 0.028 & \emph{0.010} & \textbf{0.006} \\ 
   \cline{3-8}
   & \multirow{2}{*}{3} & 0.011 & 0.019 & 0.011 & 0.020 & \emph{0.004} & \textbf{0.004} \\ 
   &  & 0.039 & 0.039 & 0.043 & 0.042 & \emph{0.008} & \textbf{0.005} \\ 
   \cline{3-8}
   & \multirow{2}{*}{4} & 0.011 & 0.011 & 0.014 & 0.013 & \textbf{0.003} & \emph{0.004} \\ 
   &  & 0.023 & 0.031 & 0.037 & 0.037 & \emph{0.007} & \textbf{0.006} \\ 
   \cline{3-8}
   & \multirow{2}{*}{5} & \emph{0.002} & 0.003 & 0.009 & 0.009 & \textbf{0.000} & 0.010 \\ 
   &  & \emph{0.004} & 0.022 & 0.037 & 0.043 & \textbf{0.000} & 0.015 \\ 
   \hline
   \multirow{10}{*}{parabola} & \multirow{2}{*}{1} & 0.007 & \emph{0.006} & 0.007 & 0.007 & 0.010 & \textbf{0.004} \\ 
   &  & \emph{0.035} & 0.035 & 0.079 & 0.085 & 0.046 & \textbf{0.009} \\ 
   \cline{3-8}
   & \multirow{2}{*}{2} & 0.001 & \emph{0.001} & 0.001 & 0.001 & \textbf{0.001} & 0.001 \\ 
   &  & 0.011 & 0.007 & 0.013 & 0.009 & \emph{0.002} & \textbf{0.002} \\ 
   \cline{3-8}
   & \multirow{2}{*}{3} & 0.001 & 0.001 & 0.001 & \emph{0.001} & \textbf{0.001} & 0.001 \\ 
   &  & 0.017 & 0.015 & 0.019 & 0.017 & \emph{0.002} & \textbf{0.002} \\ 
   \cline{3-8}
   & \multirow{2}{*}{4} & 0.001 & 0.001 & 0.001 & \emph{0.001} & \textbf{0.001} & 0.001 \\ 
   &  & 0.014 & 0.017 & 0.012 & 0.014 & \textbf{0.002} & \emph{0.002} \\ 
   \cline{3-8}
   & \multirow{2}{*}{5} & 0.006 & 0.005 & 0.004 & 0.004 & \textbf{0.000} & \emph{0.003} \\ 
   &  & 0.018 & 0.018 & 0.020 & 0.020 & \textbf{0.000} & \emph{0.009} \\ 
  \end{tabular}
\caption{Variance of $\hat{\beta_1}$ (\textbf{minimum}, \emph{next best}).\label{VarX}} 
\end{center}
\end{table}


% latex table generated in R 2.15.2 by xtable 1.7-1 package
% Thu Aug 29 16:21:15 2013
		\begin{table}
		\thispagestyle{empty}
		\begin{center}
		\begin{tabular}{cccccccc}
 function & location & enet & glmnet & u.enet & u.glmnet & oracular & gwr \\ 
  \hline
  \multirow{10}{*}{step} & \multirow{2}{*}{1} & \textbf{0.100} & 0.101 & \emph{0.100} & 0.101 & 0.111 & 0.118 \\ 
   &  & 0.594 & \textbf{0.564} & 0.594 & \emph{0.564} & 0.694 & 0.850 \\ 
   \cline{3-8}
   & \multirow{2}{*}{2} & 0.196 & \textbf{0.194} & 0.196 & \emph{0.194} & 0.225 & 0.244 \\ 
   &  & 1.019 & \textbf{1.001} & 1.019 & \textbf{1.001} & 1.171 & 1.123 \\ 
   \cline{3-8}
   & \multirow{2}{*}{3} & \emph{0.232} & 0.233 & \textbf{0.232} & 0.233 & 0.255 & 0.262 \\ 
   &  & 0.850 & \textbf{0.833} & 0.850 & \textbf{0.833} & 1.025 & 1.020 \\ 
   \cline{3-8}
   & \multirow{2}{*}{4} & \textbf{0.241} & 0.250 & \emph{0.241} & 0.250 & 0.269 & 0.288 \\ 
   &  & 0.950 & \textbf{0.950} & 0.950 & \textbf{0.950} & 1.045 & 1.053 \\ 
   \cline{3-8}
   & \multirow{2}{*}{5} & 0.231 & \textbf{0.224} & 0.231 & \emph{0.224} & 0.293 & 0.234 \\ 
   &  & \textbf{0.675} & 0.697 & \textbf{0.675} & 0.697 & 0.782 & 0.716 \\ 
   \hline
  \multirow{10}{*}{gradient} & \multirow{2}{*}{1} & \emph{0.151} & 0.169 & \textbf{0.151} & 0.169 & 0.213 & 0.247 \\ 
   &  & 0.559 & \textbf{0.552} & 0.559 & \emph{0.552} & 0.757 & 0.895 \\ 
   \cline{3-8}
   & \multirow{2}{*}{2} & 0.275 & \textbf{0.273} & 0.275 & \emph{0.273} & 0.311 & 0.332 \\ 
   &  & \textbf{0.897} & 0.953 & \textbf{0.897} & 0.953 & 1.000 & 1.048 \\ 
   \cline{3-8}
   & \multirow{2}{*}{3} & 0.257 & \textbf{0.246} & 0.257 & \emph{0.246} & 0.275 & 0.265 \\ 
   &  & \textbf{0.620} & 0.652 & \emph{0.620} & 0.652 & 0.673 & 0.664 \\ 
   \cline{3-8}
   & \multirow{2}{*}{4} & 0.293 & \textbf{0.259} & 0.293 & \textbf{0.259} & 0.304 & 0.333 \\ 
   &  & 0.748 & \emph{0.743} & 0.748 & \textbf{0.743} & 0.815 & 0.802 \\ 
   \cline{3-8}
   & \multirow{2}{*}{5} & 0.259 & \textbf{0.203} & 0.259 & \textbf{0.203} & 0.278 & 0.238 \\ 
   &  & 0.961 & \emph{0.915} & 0.961 & \textbf{0.915} & 1.127 & 0.972 \\ 
   \hline
  \multirow{10}{*}{parabola} & \multirow{2}{*}{1} & 0.224 & 0.232 & 0.224 & 0.232 & \emph{0.223} & \textbf{0.222} \\ 
   &  & \textbf{0.669} & 0.671 & \emph{0.669} & 0.671 & 0.723 & 0.757 \\ 
   \cline{3-8}
   & \multirow{2}{*}{2} & 0.216 & 0.218 & \emph{0.216} & 0.218 & 0.221 & \textbf{0.210} \\ 
   &  & \textbf{0.814} & 0.836 & \emph{0.814} & 0.836 & 0.863 & 0.832 \\ 
   \cline{3-8}
   & \multirow{2}{*}{3} & 0.241 & \emph{0.241} & 0.241 & 0.241 & 0.249 & \textbf{0.229} \\ 
   &  & \textbf{1.094} & 1.096 & \textbf{1.094} & 1.096 & 1.135 & 1.117 \\ 
   \cline{3-8}
   & \multirow{2}{*}{4} & 0.276 & 0.277 & \emph{0.276} & 0.277 & 0.281 & \textbf{0.262} \\ 
   &  & 0.882 & \emph{0.875} & 0.882 & 0.875 & 0.885 & \textbf{0.870} \\ 
   \cline{3-8}
   & \multirow{2}{*}{5} & \textbf{0.197} & 0.202 & \textbf{0.197} & 0.202 & 0.222 & 0.202 \\ 
   &  & 1.257 & \emph{1.256} & 1.257 & \textbf{1.256} & 1.289 & 1.275 \\ 
  \end{tabular}
\caption{Mean squared error of $\hat{Y}$ (\textbf{minimum}, \emph{next best}).\label{table:Y-mse}} 
\end{center}
\end{table}
		
	%\subsection{Figures}
	%The plots of bias demonstrate that GWL tended to ``fill the valleys" and ``trim the peaks" of the coefficient surface for $\beta_1$, which is not unexpected for a smoother like GWR. 
	
	%Figures \ref{fig:coveragemap1} - \ref{fig:coveragemap18} show the frequency with which the true value of the parameter $\beta_1$ was covered by the 95\% confidence intervals at each location under each simulation setting. The left column shows the coverage frequency of the 95\% CI of the GWL using the unshrunk-bootstrap method of CI construction. The middle column is the coverage frequency of the 95\% CI the O-GWR using the bootstrap to generate the CI. The right column is the relative efficiency of the GWL to O-GWR. In the first two columns, the color white is used to indicate areas where the nominal coverage frequency of 95\% is achieved, while blue codes areas that exceeded 95\% coverage and orange codes areas that fell short of 95\% coverage. In the third column, the color white indicates areas where the relative efficiency is unity, while orange indicates areas where the relative efficiency was less than unity and blue indicates areas where the relative efficiency exceeded unity.\\	

	


	

\section{References}
\bibliographystyle{chicago}
\bibliography{../../references/gwr}

\end{document}  