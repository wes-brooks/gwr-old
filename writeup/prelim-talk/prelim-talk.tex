\documentclass[12pt,t]{beamer}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{multirow}
\setbeameroption{hide notes}
\setbeamertemplate{note page}[plain]

% get rid of junk
\usetheme{default}
\beamertemplatenavigationsymbolsempty
\hypersetup{pdfpagemode=UseNone} % don't show bookmarks on initial view

% tables
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

% font
\usefonttheme{professionalfonts}
\usefonttheme{serif}
\usepackage{fontspec}
\setmainfont{Helvetica Neue}
\setbeamerfont{note page}{family*=pplx,size=\footnotesize} % Palatino for notes
\setbeamerfont{frametitle}{size=\large}

% named colors
\definecolor{offwhite}{RGB}{249,242,215}
\definecolor{foreground}{RGB}{25,25,25}
\definecolor{background}{RGB}{250,250,245}
\definecolor{title}{RGB}{20,20,20}
\definecolor{gray}{RGB}{80,80,80}
\definecolor{subtitle}{RGB}{20,20,20}
\definecolor{hilight}{RGB}{255,127,0}
\definecolor{vhilight}{RGB}{255,111,207}
\definecolor{lolight}{RGB}{155,155,155}
%\definecolor{green}{RGB}{125,250,125}

% use those colors
\setbeamercolor{titlelike}{fg=title}
\setbeamercolor{subtitle}{fg=subtitle}
\setbeamercolor{institute}{fg=gray}
\setbeamercolor{normal text}{fg=foreground,bg=background}
\setbeamercolor{item}{fg=foreground} % color of bullets
\setbeamercolor{subitem}{fg=gray}
\setbeamercolor{itemize/enumerate subbody}{fg=gray}
\setbeamertemplate{itemize subitem}{{\textendash}}
\setbeamerfont{itemize/enumerate subbody}{size=\footnotesize}
\setbeamerfont{itemize/enumerate subitem}{size=\footnotesize}

% page number
\setbeamertemplate{footline}{%
    \raisebox{5pt}{\makebox[\paperwidth]{\hfill\makebox[20pt]{\color{gray}
          \scriptsize\insertframenumber}}}\hspace*{5pt}}

% add a bit of space at the top of the notes page
\addtobeamertemplate{note page}{\setlength{\parskip}{12pt}}

% a few macros
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ig}{\includegraphics}
\newcommand{\subt}[1]{{\footnotesize \color{subtitle} {#1}}}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\bw}{\mbox{bw}}
\DeclareMathOperator*{\df}{\mbox{df}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\E}{\mathop{\mathbb E}}

% title info
\title{Local variable selection and parameter estimation for spatially varying coefficient models}
%\subtitle{A researcher's perspective}
\author{\href{http://www.somesquares.org}{Wesley Brooks}}
\institute{\href{http://www.stat.wisc.edu}{Department of Statistics} \\[2pt] \href{http://www.wisc.edu}{University of Wisconsin{\textendash}Madison}}


%bibliography stuff
\usepackage[autostyle]{csquotes}
\usepackage[backend=biber,style=authoryear,natbib=true]{biblatex}
\addbibresource{../../references/gwr.bib}


\setbeamerfont{section title}{parent=title,size=\large}
\setbeamercolor{section title}{parent=titlelike}
\defbeamertemplate*{mod section page}{default}[1][]
{
  \centering
    \vspace{3cm}
    \begin{beamercolorbox}[sep=8pt,center,#1]{section title}
      \usebeamerfont{section title}\insertsection\par
    \end{beamercolorbox}
    \note{New Section}
}
\newcommand*{\modsectionpage}{\usebeamertemplate*{mod section page}}

\AtBeginSection{\frame{\modsectionpage}}



\begin{document}

% title slide
{
  \setbeamertemplate{footline}{} % no page number here
  \frame{
    \titlepage
    \note{These slides were prepared for a practice version of my preliminary exam to advance to Ph.D candidacy in statistics at the University of Wisconsin{\textendash}Madison.}
  }
}



\section{Motivation}



\begin{frame}{Motivation}
\subt{Response variable}

\begin{center}
  \ig[width=0.6\textwidth]{../../figures/practice-talk/poverty-response}
\end{center}

\note{This is the county-level poverty rate from 1970}
\end{frame}




\begin{frame}{Motivation}
\subt{Covariates}

\bigskip
\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/poverty-covariates}
\end{center}

\note{Here we have the proportion of people in each county who worked in manufacturing, agriculture, and services in 1970.

How is this data to be analyzed?}
\end{frame}





\begin{frame}{Motivation}
\subt{Scientific questions}

\bigskip
\begin{itemize}
    \item Which of the economic-structure variables is associated with poverty rate?
    \item What are the sign and magnitude of that association?
    \item Is poverty rate associated with the same economic-structure variables across the entire region?
    \item How do the sign and magnitude of the associations vary across the region?
\end{itemize}

\note{These are some sensible questions to ask about the county-level poverty rate. The work I'm presenting today attempts to answer these questions.

There are several other methods to answer at least some of these questions, which we'll cover next.}
\end{frame}



\section{Introduction}



\begin{frame}{Introduction}
\subt{An overview}

\bigskip
\begin{itemize}
    \item Spatial regression
    \item Varying coefficient regression
    \begin{itemize}
        \item Splines
        \item Kernels
        \item Wavelets
    \end{itemize}
    \item Model selection via regularization
\end{itemize}

\note{The existing methods to address the questions draw from these areas. Behind the methodology that I'm discussing is a wide range of literature.}
\end{frame}






\begin{frame}{Introduction}
\subt{Definitions}

\bigskip
\begin{itemize}
    \item Univariate spatial response process $\left\{ Y(\bm{s}) : \bm{s} \in \mathcal{D} \right\}$
    \item Multivariate spatial covariate process $\left\{ \bm{X}(\bm{s}) : \bm{s} \in \mathcal{D} \right\}$
    \item $n = $ number of observations
    \item $p = $ number of covariates
    \item Location (2-dimensional) $\bm{s}$
    \item Spatial domain $\mathcal{D}$
\end{itemize}


\note{We'll use these variables throughout.}
\end{frame}





\begin{frame}{Introduction}
\subt{Types of spatial data}

\bigskip
\begin{itemize}
    \item Geostatistical data:
    \begin{itemize}
        \item Observations are made at sampling locations $\bm{s}_i$ for $i = 1, \dots, n$
        \item E.g. elevation, temperature
    \end{itemize}
    \item Areal data:
    \begin{itemize}
        \item Domain is partitioned into $n$ regions $\left\{ D_1, \dots, D_n \right\}$
        \item The regions do not overlap, and they divide the domain completely: $\mathcal{D} = \bigcup_{i=1}^n D_i$
        \item Sampling locations $\bm{s}_i$ for $i = 1, \dots, n$ are the centroids of the regions
        \item E.g. poverty rate, population, spatial mean temperature
    \end{itemize}
\end{itemize}


\note{The method I'm describing applies to geostatistical data, or to areal data when the observations are assumed to be located at the centroid.

The poverty data example is areal data; the simulation study I'll present later is based on simulated geostatistical data.}
\end{frame}








\begin{frame}{Introduction}
\subt{Spatial linear regression (Cressie, 1993)}

\bigskip
\begin{itemize}
    \item A typical spatial linear regression model
\end{itemize}
\begin{align*}
    Y(\bm{s}) &= \bm{X}(\bm{s})'\bm{\beta} + W(\bm{s}) + \varepsilon(\bm{s}) \\    
\end{align*}

\begin{itemize}
    \item $W(\bm{s})$ is a spatial random effect that accounts for autocorrelation in the response variable
    \item cov$(W(\bm{s}), W(\bm{t}))$: Mat\`{e}rn class
    \item The coefficients $\bm{\beta} = \left(1, \beta_1, \dots, \beta_p \right)$ are constant
    \item Relies on \emph{a priori} global variable selection
\end{itemize}

\note{Here we have the usual spatial regression as described by Noel Cressie in his 1993 book.

This model assumes that the model coefficients are constant across the spatial domain and that the residuals can be separated into:

 - The spatial random effect W that captures autocorrelation of the response, and
 - epsilon, which is iid white noise

The autocorrelation of the W's is from a Mat\'{e}rn class covariance function, like the exponential covariance function.

This model relies on a priori model selection.

Typically Bayesian methods are used to estimate the coefficients.}
\end{frame}






\begin{frame}{Introduction}
\subt{Spatially varying coefficient model (Gelfand \emph{et al.}, 2003)}

\bigskip
\begin{itemize}
    \item A more flexible model: coefficients in a spatial regression model can vary
\end{itemize}

\[
    Y(\bm{s}) = \bm{X}(\bm{s})' \bm{\beta}(\bm{s}) + \varepsilon(\bm{s})
\]
\begin{itemize}
  \item $\{ \beta_0(\bm{s}) : \bm{s} \in \mathcal{D}\}, \dots, \{\beta_p(\bm{s}) : \bm{s} \in \mathcal{D}\}$ are stationary spatial processes with Mat\`{e}rn covariance functions
  \item Still relies on \emph{a priori} global variable selection
\end{itemize}

\note{The spatial regression model can be made more flexible by representing the coefficients as stationary spatial processes, rather than constants. The method was introduced by Gelfand in 2003.

The coefficient processes have mat\`{e}rn class covariance functions, just like the autocorrelated errors W in the traditional spatial regression.

The autocorrelated errors W are now incorporated in the spatially varying intercept process.

This model also relies on a priori model selection and uses Bayesian methods to estimate the coefficients.}
\end{frame}





\begin{frame}{Introduction}
\subt{Varying coefficients regression (VCR) (Hastie and Tibshirani, 1993)}

\[
    Y(\bm{s}) = \bm{X}(\bm{s})' \bm{\beta}(\bm{s}) + \varepsilon(\bm{s})
\]

\begin{itemize}
    \item Assume an effect modifying variable $\bm{s}$
    \item Coefficients are functions of $\bm{s}$
\end{itemize}

\note{The varying coefficient regression model was described by Hastie and Tibshirani in 1993. The form of this model looks like the spatially varying coefficient process, but this model is more general.

The coefficients are not necessarily spatial processes in this model. In fact, the effect-modifying variable s does not necessarily need to represent spatial location.

There are non-Bayesian methods to fit the model. We'll look at three.}
\end{frame}




\begin{frame}{Introduction}
\subt{Spline-based VCR models (Wood, 2006)}

\bigskip
\begin{itemize}
    \item Splines are a way to parameterize smooth functions
    \item Estimate the varying coefficients via splines:
    \begin{align*}
        E\{Y(t)\} = \beta_1(t) X_1(t) + \cdots + \beta_p(t) X_p(t)
    \end{align*}
\end{itemize}

\note{There is a good overview in Simon Wood's 2006 book of how to use regression splines to fit a varying coefficients regression model.

Regression splines are a way to parameterize a smooth function. In this case, the coefficient is a smooth function of the spatial location

fitting a spline-based VCR requires a priori model selection(?).}
\end{frame}




\begin{frame}{Introduction}
\subt{Global selection in spline-based VCR models}

\bigskip
Regularization methods for global variable selection in VCR models:
\begin{itemize}
  \item The integral of a function squared (e.g. $\int \{f(t)\}^2 dt$) is zero if and only if the function is zero everywhere.
  \item Use regularization to encourage coefficient functions to be zero
  \begin{itemize}
    \item SCAD penalty (Wang \emph{et al.}, 2008a)
    \item Non-negative garrote penalty (Antoniadis \emph{et al.}, 2012b)
  \end{itemize}
\end{itemize}

\note{There are at least two references that describe how to select the covariates for a spline-based VCR model. Both rely on regularization.

The regularization penalizes the smooth coefficient function for being non-zero. Antoniadis et al. used a non-negative garrote penalty and Wang et al. used a SCAD penalty.

These selection methods are global - that is, they select variables for the entire domain simultaneously.}
\end{frame}




\begin{frame}{Introduction}
\subt{Wavelet methods for VCR models}

\bigskip
\begin{itemize}
  \item Wavelet methods: decompose coefficient function into local frequency components
  \item Selection of nonzero local frequency components with nonzero coefficients:
  \begin{itemize}
    \item Bayesian variable selection (Shang, 2011)
    \item Lasso (Zhang and Clayton, 2011)
  \end{itemize} 
  \item Sparsity in the local frequency components; not in the local covariates
\end{itemize}

\note{Another way to fit a VCR model is to use a wavelet decomposition, which decomposes the coefficient function into its local frequency components. Model selection is then used to identify which local frequency components to use in the model.

Murray Clayton's students Zuofeng Shang and Jun Zhang used Bayesian variable selection and the Lasso, respectively, to select the local frequency components.

However, these methods achieve sparsity in the wavelet coefficients, which does not imply sparsity in the covariates. So these methods don't achieve local model selection.

Now let's take a look at geographically weighted regression.}
\end{frame}




\section{Geographically weighted regression}



\begin{frame}{Geographically weighted regression}
\subt{Brundson \emph{et al.} (1998), Fotheringham \emph{et al.} (2002)}

\bigskip
\begin{itemize}
  \item Consider observations at sampling locations $\bm{s}_1, \dots, \bm{s}_n$
  \item $y(\bm{s}_i) = y_i$ the univariate response at location $\bm{s}_i$
  \item $\bm{x}(\bm{s}_i) = \bm{x}_i$ the $(p+1)$-variate vector of covariates at location $\bm{s}_i$
  \item Assume $y_i = \bm{x}'_i \bm{\beta}_i + \varepsilon_i$ where $\varepsilon_i \overset{iid}{\sim} \mathcal{N} \left( 0,\sigma^2 \right)$
\end{itemize} 

\note{Geographically weighted regression is the method of using local regression to estimate the coefficients in a spatially varying coefficient regression model.

Our sampling locations are called s, the response is y and the covariates (which number p) are called x.

Assume that the errors are iid normal.

The notation $\beta_i$ is used to indicate that he coefficients are specific to location $i$.}
\end{frame}





\begin{frame}{Geographically weighted regression}
\subt{Brundson \emph{et al.} (1998), Fotheringham \emph{et al.} (2002)}

\bigskip
\begin{itemize}
  \item The total log likelihood is
\begin{align*}
\ell\left( \bm{\beta} \right) = - \left(1/2\right) \left\{ n \log \left( 2 \pi \sigma^2\right) +  \sigma^{-2}  \sum_{i=1}^n \left(y_i - \bm{x}'_i\bm{\beta}_i \right)^2  \right\}
\end{align*}
  \item With $n$ observations and $n(p+1)$ parameters, the model is not identifiable.
  \item Idea: to estimate parameters by borrowing strength from nearby observations
\end{itemize} 

\note{We have here the total log likelihood of the observed data.

Because each $\beta_i$ is a p-vector of local coefficients, this model has n observations and n(p+1) free parameters, so the model is not identifiable.

We will estimate the parameters by borrowing strength from nearby observations}
\end{frame}





\begin{frame}{Geographically weighted regression}
\subt{Local regression (Loader, 1999)}

\bigskip
Local regression uses a kernel function at each sampling location to weight observations based on their distance from the sampling location.
\begin{align}
    \mathcal{L}_i &= \prod_{i'=1}^n \left(\mathcal{L}_{i'}\right)^{w_{ii'}} \notag \\
    \ell_i &= \sum_{i'=1}^n w_{ii'} \left\{\log \left(\sigma^2\right) + \sigma^{-2}\left(y_{i'} - \bm{x}_{i'}'\bm{\beta}_i\right)^2 \right\} \notag
\end{align}

Given the weights, a local model is fit at each sampling location using the local likelihood

\note{Local regression uses a kernel function at each sampling location to weight the observations. For a GWR model, the kernel weights are based on an observation's distance from the sampling location.

Here we have the likelihood at one sampling location. Note that each observation is given a weight $w_{ii'}$

Given the weights, a local model is fit at each sampling location using the local likelihood

Maximizing the local likelihood for a model of Gaussian data with iid errors can be done by weighted least squares.}
\end{frame}







\begin{frame}{Geographically weighted regression}
\subt{Local likelihood (Loader, 1999)}

\bigskip
Weights are calculated via a kernel, e.g. the bisquare kernel:
\begin{align}
	w_{ii'} = \begin{cases} \left\{1-\left(\phi^{-1}\delta_{ii'}\right)^2\right\}^2 &\mbox{ if } \delta_{ii'} < \phi, \\ 0 &\mbox{ if } \delta_{ii'} \geq \phi \end{cases}
\end{align}

where
\begin{itemize}
  \item $\phi$ is a bandwidth parameter
  \item $\delta_{ii'} = \delta(\bm{s}_i,\bm{s}_{i'}) = \|\bm{s}_i - \bm{s}_{i'}\|_2$ is the Euclidean distance between sampling locations $\bm{s}_i$ and $\bm{s}_{i'}$.
\end{itemize}

\note{The local weights $w_{ii'}$ from the previous slide are calculated from a kernel.

This is the form of the bisquare kernel, which is what I've used in this work.

$\phi$ is a bandwidth parameter and $\delta_{ii'}$ is the distance between points i and i'.}
\end{frame}





\begin{frame}{Geographically weighted regression}
\subt{Bandwidth estimation via the $\text{AIC}_{\tt c}$ (Hurvich \emph{et al.}, 1998)}

\bigskip
\begin{itemize}
    \item Smaller bandwidth: less bias, more flexible coefficient surface
    \item Large bandwidth: less variance, less flexible coefficient surface
    \item Choose the bandwidth parameter to optimize the bias-variance tradeoff
\end{itemize}

\note{To estimate a GWR model, it is necessary to estimate the bandwidth parameter, which involves a bias-variance tradeoff.

When the bandwidth is small, the coefficient surface is flexible and should have less bias but greater variance.

When the bandwidth is large, the coefficient surface is less flexible so it has less variance but potentially more bias.
}
\end{frame}





\begin{frame}{Geographically weighted regression}
\subt{Bandwidth estimation via the $\text{AIC}_{\tt c}$ (Hurvich \emph{et al.}, 1998)}

\bigskip
\begin{itemize}
    \item The corrected AIC for bandwidth selection is:
    \begin{align*}
      \text{AIC}_{\tt c} = 2 n \log{\sigma} + n \left\{\frac{n + \nu}{n - 2 - \nu}\right\}
    \end{align*}
    \begin{itemize}
        \item $\hat{\bm{y}} = \bm{H}\bm{y}$
        \item $\nu = $ tr($\bm{H}$)
        \item $\bm{H}_i = \left\{\bm{W}\bm{X}(\bm{X}'\bm{W}\bm{X})^{-1}\bm{X}\right\}_i$
        \item Where subscript $i$ indicates the $i$th row of the matrix
    \end{itemize}
\end{itemize}

\note{One way to estimate the GWR bandwidth is via the corrected AIC of Hurvich \emph{et al.}. }
\end{frame}





\begin{frame}{Geographically weighted regression}
\subt{Bandwidth estimation via GCV (Wahba, 1990)}

\bigskip
\begin{itemize}
    \item The GCV criterion for bandwidth selection is:
    \begin{align*}
      \text{GCV} = \frac{\sum_i=1^n\left(y - \hat{y}\right)^2}{(n-\nu)^2}
    \end{align*}
    \begin{itemize}
        \item $\hat{\bm{y}} = \bm{H}\bm{y}$
        \item $\nu = $ tr($\bm{H}$)
        \item $\bm{H}_i = \left\{\bm{W}\bm{X}(\bm{X}'\bm{W}\bm{X})^{-1}\bm{X}\right\}_i$
        \item Where subscript $i$ indicates the $i$th row of the matrix
    \end{itemize}
\end{itemize}

\note{Another way to estimate the GWR bandwidth is via Generalized Cross Validation as described in Wahba, 1990.}
\end{frame}






\section{Local variable selection and parameter estimation}



\begin{frame}{Geographically weighted Lasso}
\subt{Geographically weighted Lasso (Wheeler, 2009)}

\bigskip
Within a GWR model, using the Lasso for local variable selection is called the geographically weighted Lasso (GWL).
\begin{itemize}
    \item The GWL requires estimating a Lasso tuning parameter for each local model
    \item \Wheeler (2009) estimates the local Lasso tuning parameter at location $\bm{s}_i$ by minimizing a jacknife criterion: $|y_i - \hat{y}_i|$
    \item The jacknife criterion can only be calculated where data are observed, making it impossible to use the GWL to impute missing data or to estimate the value of the coefficient surface at new locations
    \item Also, the Lasso is known to be biased in variable selection and suboptimal for coefficient estimation
\end{itemize}

\note{For local model selection in a GWR model, Wheeler proposed the geographically weighted lasso (GWL) in 2009.

At each model location, the Lasso is used to select the locally-relevant predictors

The GWL uses a jacknife criterion to select the local lasso tuning parameters, which means the GWL cannot be used at model locations other than sample locations.

That means the GWL cannot be used for interpolating the coefficient surface or for imputing missing values of the response variable.}
\end{frame}



\begin{comment}

\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted adaptive Lasso (GWAL)}

\bigskip
\begin{itemize}
    \item Local variable selection in a GWR model using the adaptive Lasso (AL) (Zou, 2006)
    \item Under suitable conditions, the AL has an oracle property for selection
    \item Let
\begin{align}
		\mathcal{S}(\bm{\beta}_i) &= -2 \ell_i(\bm{\beta}_i) + \mathcal{J}_2(\bm{\beta}_i) \notag \\
		&= \sum_{i'=1}^n w_{ii'}  \left\{ \log{\left(\sigma^2_i\right)}  + \sigma^{-2}_i  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right\}   \notag \\
		&+ \lambda_i \sum_{j=1}^p |\beta_{ij}| / \gamma_{ij} \notag
\end{align}
\end{itemize}

\note{The geographically weighted adaptive lasso (GWAL) overcomes these shortcomings of the GWL. 

The GWAL uses the adaptive lasso, which has an oracle property under suitable conditions.

S here is the penalized likelihood for a local GWAL model

The adaptive lasso consists of applying a different penalty to each covariate based on the adaptive weights, which are derived from the weighted least squares coefficients.



The adaptive weights $\bm{\gamma}_i = \left(\gamma_{i1}, \dots, \gamma_{ip}\right)'$ are defined in the same way as for the AL, and the elastic net parameter $\alpha_i \in [0,1]$ controls the balance between $\ell_1$ penalty $\lambda_i\sum \limits_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ and $\ell_2$ penalty $\lambda_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2$.
}
\end{frame}




\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted adaptive elastic net (GWAL)}

\bigskip
Note:
\begin{itemize}
  \item $\sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2$ is the weighted sum of squares minimized by traditional GWR
  \item $\mathcal{J}_1(\bm{\beta}_i) = \lambda_i \sum_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ is the AL penalty.
\end{itemize}

\note{
The adaptive weights $\bm{\gamma}_i = \left(\gamma_{i1}, \dots, \gamma_{ip}\right)'$ are defined in the same way as for the AL, and the elastic net parameter $\alpha_i \in [0,1]$ controls the balance between $\ell_1$ penalty $\lambda^*_i\sum \limits_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ and $\ell_2$ penalty $\lambda^*_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2$.
}
\end{frame}




\begin{frame}{Local variable selection and  parameter estimation}
\subt{Tuning parameter estimation}

\bigskip
To estimate an AL tuning parameter for each local, use a local BIC that allows fitting a local model at any location within the spatial domain
	\begin{align}
		\mbox{BIC}_i &= -2 \sum_{i'=1}^n \ell_{ii'}  + \log \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i\notag\\
		&= \sum_{i'=1}^n w_{ii'} \left\{ \log \left(2 \pi \right) + \log{ \left(\hat{\sigma}_i^2\right)} + \hat{\sigma}_i^{-2} \left(y_{i'} - \bm{x}_{i'}' \hat{\bm{\beta}}_{i'} \right)^2 \right\} \notag \\
		&+ \log \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i \notag
	\end{align}


\note{
The adaptive weights $\bm{\gamma}_i = \left(\gamma_{i1}, \dots, \gamma_{ip}\right)'$ are defined in the same way as for the AL, and the elastic net parameter $\alpha_i \in [0,1]$ controls the balance between $\ell_1$ penalty $\lambda^*_i\sum \limits_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ and $\ell_2$ penalty $\lambda^*_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2$.

We treat the sum of the weights around the sampling location as the number of observations for the local BIC.
}
\end{frame}

\end{comment}






\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted adaptive elastic net (GWEN)}

\bigskip
\begin{itemize}
    \item Local variable selection in a GWR model using the adaptive elastic net (AEN) (Zou and Zhang, 2009)
    \item Under suitable conditions, the AEN has an oracle property for selection
\end{itemize}
\begin{align*}
		\mathcal{S}(\bm{\beta}_i) &= -2 \ell_i(\bm{\beta}_i) + \mathcal{J}_2(\bm{\beta}_i) \notag \\
		&= \sum_{i'=1}^n w_{ii'}  \left\{ \log{\sigma^2_i}  + \left(\sigma^2_i\right)^{-1}  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right\}   \notag \\
		&+ \alpha_i \lambda^*_i \sum_{j=1}^p |\beta_{ij}| / \gamma_{ij} \notag \\
		&+ (1-\alpha_i) \lambda^*_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2 \notag
\end{align*}

\note{The geographically weighted adaptive elastic net (GWEN) is similar to the GWAL but uses the elastic net for local model selection

The adaptive elastic net also has an oracle property under suitable conditions.

The adaptive elastic net consists of adding an L2 penalty to the regularization in addition to the L1 penalty of the adaptive lasso.

S here is the penalized likelihood for a local GWEN model

The adaptive weights $\bm{\gamma}_i = \left(\gamma_{i1}, \dots, \gamma_{ip}\right)'$ are defined in the same way as for the AL, and the elastic net parameter $\alpha_i \in [0,1]$ controls the balance between $\ell_1$ penalty $\lambda^*_i\sum \limits_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ and $\ell_2$ penalty $\lambda^*_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2$.
}
\end{frame}






\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted adaptive elastic net (GWEN)}

\bigskip
\begin{itemize}
  \item The AEN penalty function is
  \begin{align*}
    \mathcal{J}_2(\bm{\beta}_i) = \alpha_i \lambda^*_i \sum_{j=1}^p |\beta_{ij}| / \gamma_{ij} + (1-\alpha_i) \lambda^*_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2
  \end{align*}
\end{itemize}

\note{
The adaptive weights $\bm{\gamma}_i = \left(\gamma_{i1}, \dots, \gamma_{ip}\right)'$ are defined in the same way as for the AL, and the elastic net parameter $\alpha_i \in [0,1]$ controls the balance between $\ell_1$ penalty $\lambda^*_i\sum \limits_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ and $\ell_2$ penalty $\lambda^*_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2$.
}
\end{frame}














\begin{comment}
\begin{frame}{Local variable selection and  parameter estimation}
\subt{Bandwidth parameter estimation}

\bigskip
Letting $H_i = \bm{W}_i \bm{X}\left(\bm{X}'\bm{W}_i\bm{X}\right)^{-1}\bm{X}'$, where $\bm{W}_i$ is the diagonal weight matrix diag$(w_{ii'})$,
 
\begin{align*}
  \hat{\bm{y}} = \bm{H} \bm{y}
\end{align*}

\note{note}
\end{frame}
\end{comment}



\begin{frame}{Local variable selection and  parameter estimation}
\subt{Bandwidth parameter estimation}

\bigskip 
\begin{itemize}
  \item Traditional GWR:
  \begin{itemize}
    \item $\hat{\bm{y}} = \bm{H} \bm{y}$
    \item So traditional GWR is a linear smoother
    \item $\nu = $ tr$(\bm{H})$ is the degrees of freedom for the model
  \end{itemize}
  \item GWAL:
  \begin{itemize}
    \item $\hat{\bm{y}} = \bm{H}^{\dagger}\bm{y} - \bm{T}^{\dagger}\bm{\gamma}$
  \end{itemize}  
  \item GWEN:
  \begin{itemize}
    \item $\hat{\bm{y}} = \bm{H}^{*}\bm{y} + \bm{T}^{*}\bm{\gamma}$
  \end{itemize}  
  \item Neither GWEN nor GWAL is a linear smoother
  \begin{itemize}
    \item df not equal to trace of projection matrix for GWAL, GWEN
  \end{itemize} 
  \item Solution: use GWEN or GWAL for selection then fit local model for the selected variables via traditional GWR
  \begin{itemize}
    \item Now df $= \nu = $ tr$(\bm{H})$
  \end{itemize} 
\end{itemize}

\note{The }
\end{frame}





\begin{frame}{Local variable selection and  parameter estimation}
\subt{Locally linear coefficient estimation}

\bigskip
\begin{itemize}
  \item GWR, GWEN, GWAL: coefficients locally constant
  \begin{itemize}
    \item as in Nadaraya-Watson kernel smoother
    \item Leads to bias where there is a gradient at the boundary 
  \end{itemize}
  \item Solution: local polynomial modeling
  \begin{itemize}
    \item First-order polynomial: locally linear coefficients
  \end{itemize}
  \item Augment with covariate-by-location interactions
  \begin{itemize}
    \item Two-dimensional
    \item Augment with selected covariates only
  \end{itemize}
\end{itemize}

\note{note}
\end{frame}




\section{Simulation study}




\begin{frame}{Simulation study}
\subt{Simulating covariates}

\bigskip
\begin{itemize}
  \item $30 \times 30$ grid on $[0,1] \times [0,1]$
  \item Five covariates $\tilde{X}_1, \dots, \tilde{X}_5$ 
  \item Gaussian random fields:
  \begin{align}
    \tilde{X}_j &\sim N\left(0, \bm{\Sigma}\right) \text{ for } j = 1, \dots, 5 \notag \\
    \left\{\Sigma\right\}_{i,i'} &= \exp \{ -\tau^{-1} \delta_{ii'}\} \text{ for } i,i' = 1, \dots, n \notag
  \end{align}
  \item Colinearity: $\rho$
\end{itemize}

\note{note}
\end{frame}


\begin{frame}{Simulation study}
\subt{Simulating the response}

\bigskip
\begin{itemize}
    \item $Y(\bm{s}) = \bm{X}(\bm{s})'\bm{\beta}(\bm{s}) = \sum_{j=1}^5 \beta_j(\bm{s}) X_j(\bm{s}) + \varepsilon(\bm{s})$
    \item $\varepsilon(\bm{s}) \sim \; iid \;\; N(0,\sigma^2) $ 
    \item $\beta_1(\bm{s})$, the coefficient function for $X_1$, is nonzero in part of the domain.
    \item Coefficients for $X_2, \dots, X_5$ are zero everywhere
\end{itemize}

\note{note}
\end{frame}



\begin{frame}{Simulation study}
\subt{Coefficient functions: step, gradient, and parabola}

\bigskip

\begin{figure}
    \begin{center}
    \ig[width=0.35\textwidth]{../../figures/practice-talk/step.jpg}
    \ig[width=0.35\textwidth]{../../figures/practice-talk/gradient.jpg}
    \ig[width=0.35\textwidth]{../../figures/practice-talk/parabola.jpg}
    \end{center}
\end{figure}

\note{note}
\end{frame}




\begin{frame}{Simulation study}
\subt{Simulation settings}

Each setting simulated 100 times:
    \begin{table}[h!]
        \begin{center}
        \begin{tabular}{cccc}
            \hline
            Setting & function & $\rho$ & $\sigma^2$ \\ 
            \hline
            1 & step & 0 & 0.25 \\ 
            2 & step & 0 & 1 \\ 
            3 & step & 0.5 & 0.25 \\ 
            4 & step & 0.5 & 1 \\ 
            \hline
            5 & gradient & 0 & 0.25 \\ 
            6 & gradient & 0 & 1 \\ 
            7 & gradient & 0.5 & 0.25 \\ 
            8 & gradient & 0.5 & 1 \\ 
            \hline
            9 & parabola & 0 & 0.25 \\ 
            10 & parabola & 0 & 1 \\ 
            11 & parabola & 0.5 & 0.25 \\ 
            12 & parabola & 0.5 & 1 
        \end{tabular}
        \end{center}
    \end{table}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Summary locations}

\begin{figure}
    \begin{center}
    \ig[width=0.65\textwidth]{../../figures/simulation/summary-locations}
    \end{center}
\end{figure}

\note{note}
\end{frame}



\begin{comment}

\begin{frame}{Simulation results}
\subt{Selection}
\input{../../output/prelim-talk/selection}
\note{note}
\end{frame}




\begin{frame}{Simulation results}
\subt{MSE of $\beta_1(\bm{s})$ - step coefficient surface}
\input{../../output/prelim-talk/msex-step}
\note{note}
\end{frame}



\begin{frame}{Simulation results}
\subt{MSE of $\beta_1(\bm{s})$ - gradient coefficient surface}
\input{../../output/prelim-talk/msex-gradient}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{MSE of $\beta_1(\bm{s})$ - parabola coefficient surface}
\input{../../output/prelim-talk/msex-parabola}
\note{note}
\end{frame}





\begin{frame}{Simulation results}
\subt{Variance of $\beta_1(\bm{s})$ - step coefficient surface}
\input{../../output/prelim-talk/varx-step}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Variance of $\beta_1(\bm{s})$ - gradient coefficient surface}
\input{../../output/prelim-talk/varx-gradient}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Variance of $\beta_1(\bm{s})$ - parabola coefficient surface}
\input{../../output/prelim-talk/varx-parabola}
\note{note}
\end{frame}




\begin{frame}{Simulation results}
\subt{Bias of $\beta_1(\bm{s})$ - step coefficient surface}
\input{../../output/prelim-talk/bx-step}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Bias of $\beta_1(\bm{s})$ - gradient coefficient surface}
\input{../../output/prelim-talk/bx-gradient}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Bias of $\beta_1(\bm{s})$ - parabola coefficient surface}
\input{../../output/prelim-talk/bx-parabola}
\note{note}
\end{frame}


\end{comment}




\begin{frame}{Simulation results}
\subt{Selection performance}

\bigskip
\begin{itemize}
  \item Non-ambiguous locations (80):
  \begin{itemize}
    \item 52 saw no false negatives
    \item 72 had no false positives
    \item 26 neither false positives nor false negatives
  \end{itemize}

  \item Incerased noise variance led to worse selection performance
  \item Increased colinearity in the covariates led to worse selection performance
  \item No difference between GWEN and GWAL
\end{itemize}

\note{note}
\end{frame}



	
	
\begin{frame}{Simulation results}
\subt{Estimation performance}


\begin{itemize}
  \item Oracular selection
  \begin{itemize}
    \item best MSE$(\hat{\beta}_1)$ in 41 of the 60 cases
  \end{itemize}

  \item Generally small difference between GWR, oracular, GWEN-LLE, and GWAL-LLE\
  \item Incerased noise variance led to worse estimation accuracy
  \item Increased colinearity in the covariates led to worse estimation accuracy
  \item Fitting $\hat{y}$: best MSE split between GWAL-LLE, oracle, and GWR
\end{itemize}
\note{note}
\end{frame}
	


\section{Data example: poverty rate in the upper midwest}



\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Revisiting the motivating example}

\bigskip
\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/poverty-covariates}
\end{center}

\note{
This is the county-level poverty rate from 1970, as well as the proportion of people who worked in manufacturing, agriculture, and services.

How is this data to be analyzed?
}
\end{frame}







\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Data description}

\bigskip
  \begin{itemize}
    \item Response: logit-transformed poverty rate in the Upper Midwest states of the U.S.
    \begin{itemize}
      \item Minnesota, Iowa, Wisconsin, Illinois, Indiana, Michigan
    \end{itemize}
    \item Covariates: employment structure (raw proportion employed in:)
    \begin{itemize}
      \item agriculture
      \item finance, insurance, and real estate
      \item manufacturing
      \item mining
      \item services
      \item other professions
    \end{itemize}
    \item Data source: U.S. Census Bureau's decennial census of 1970
  \end{itemize}

\note{note}
\end{frame}



\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Data description}

\bigskip
  \begin{itemize}
    \item Data aggregated to the county level
    \begin{itemize}
      \item counties are areal units
    \end{itemize}
    \item county centroid treated as sampling location
  \end{itemize}

\note{note}
\end{frame}


\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Results from traditional GWR}

\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/1970-GWR-coefficients}
\end{center}

\note{note}
\end{frame}


\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Results from GWEN}

\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/1970-GWEN-coefficients}
\end{center}

\note{note}
\end{frame}


\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Results from GWEN-LLE}

\begin{itemize}
  \item Relatively constant compared to GWR
  \item Services, "other professions" do not affect the poverty rate
  \item Manufacturing: negative coefficient everywhere
  \item Finance, insurance, and real estate negative coefficient everywhere
  \begin{itemize}
    \item Largest magnitude (min: -20, next-largest: -3)
    \item GWR comparable to GWEN-LLE
  \end{itemize}

  \item Manufacturing: negative coefficient everywhere
  \begin{itemize}
    \item GWR: coefficient greater than zero near Chicago and in NW Minnesota
  \end{itemize}

  \item Agriculture: nonzero in western Iowa
  \begin{itemize}
    \item North-south gradient to coefficient
    \item ranges positive to negative
  \end{itemize}

  \item Mining: nonzero in parts south 
  \begin{itemize}
    \item Associated with increased poverty rate
    \item Comparable to GWR within far southern range
  \end{itemize}
\end{itemize}

\note{note}
\end{frame}


\section{Future work}


\begin{frame}{Future work}
\begin{itemize}
    \item Apply the GWEN to models for non-Gaussian response variable
    \item Incorporate spatial autocorrelation in the model
    \item PalEON project: modeling and mapping tree biomass in the upper midwest
\end{itemize}
\note{note}
\end{frame}



\begin{frame}{Acknowledgements}

\end{frame}




\end{document}
