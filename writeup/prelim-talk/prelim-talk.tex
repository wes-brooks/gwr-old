\documentclass[12pt,t]{beamer}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{multirow}
\setbeameroption{hide notes}
\setbeamertemplate{note page}[plain]

% get rid of junk
\usetheme{default}
\beamertemplatenavigationsymbolsempty
\hypersetup{pdfpagemode=UseNone} % don't show bookmarks on initial view

% tables
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

% font
\usefonttheme{professionalfonts}
\usefonttheme{serif}
\usepackage{fontspec}
\setmainfont{Helvetica Neue}
\setbeamerfont{note page}{family*=pplx,size=\footnotesize} % Palatino for notes
\setbeamerfont{frametitle}{size=\large}

% named colors
\definecolor{offwhite}{RGB}{249,242,215}
\definecolor{foreground}{RGB}{25,25,25}
\definecolor{background}{RGB}{250,250,245}
\definecolor{title}{RGB}{20,20,20}
\definecolor{gray}{RGB}{80,80,80}
\definecolor{subtitle}{RGB}{20,20,20}
\definecolor{hilight}{RGB}{255,127,0}
\definecolor{vhilight}{RGB}{255,111,207}
\definecolor{lolight}{RGB}{155,155,155}
%\definecolor{green}{RGB}{125,250,125}

% use those colors
\setbeamercolor{titlelike}{fg=title}
\setbeamercolor{subtitle}{fg=subtitle}
\setbeamercolor{institute}{fg=gray}
\setbeamercolor{normal text}{fg=foreground,bg=background}
\setbeamercolor{item}{fg=foreground} % color of bullets
\setbeamercolor{subitem}{fg=gray}
\setbeamercolor{itemize/enumerate subbody}{fg=gray}
\setbeamertemplate{itemize subitem}{{\textendash}}
\setbeamerfont{itemize/enumerate subbody}{size=\footnotesize}
\setbeamerfont{itemize/enumerate subitem}{size=\footnotesize}

% page number
\setbeamertemplate{footline}{%
    \raisebox{5pt}{\makebox[\paperwidth]{\hfill\makebox[20pt]{\color{gray}
          \scriptsize\insertframenumber}}}\hspace*{5pt}}

% add a bit of space at the top of the notes page
\addtobeamertemplate{note page}{\setlength{\parskip}{12pt}}

% a few macros
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ig}{\includegraphics}
\newcommand{\subt}[1]{{\footnotesize \color{subtitle} {#1}}}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\bw}{\mbox{bw}}
\DeclareMathOperator*{\df}{\mbox{df}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\E}{\mathop{\mathbb E}}

% title info
\title{Local variable selection and parameter estimation for spatially varying coefficient models}
%\subtitle{A researcher's perspective}
\author{\href{http://www.somesquares.org}{Wesley Brooks}}
\institute{\href{http://www.stat.wisc.edu}{Department of Statistics} \\[2pt] \href{http://www.wisc.edu}{University of Wisconsin{\textendash}Madison}}


%bibliography stuff
\usepackage[autostyle]{csquotes}
\usepackage[backend=biber,style=authoryear,natbib=true]{biblatex}
\addbibresource{../../references/gwr.bib}


\setbeamerfont{section title}{parent=title,size=\large}
\setbeamercolor{section title}{parent=titlelike}
\defbeamertemplate*{mod section page}{default}[1][]
{
  \centering
    \vspace{3cm}
    \begin{beamercolorbox}[sep=8pt,center,#1]{section title}
      \usebeamerfont{section title}\insertsection\par
    \end{beamercolorbox}
    \note{New Section}
}
\newcommand*{\modsectionpage}{\usebeamertemplate*{mod section page}}

\AtBeginSection{\frame{\modsectionpage}}



\begin{document}

% title slide
{
  \setbeamertemplate{footline}{} % no page number here
  \frame{
    \titlepage
\note{Hello, I'm Wesley Brooks. My undergraduate work was in electrical engineering and math at the University of Alaska, and I completed my M.S. in this department in 2012. Today I'm presenting work toward my Ph.D.

The subject of this research is local variable selection and parameter estimation for spatially varying coefficient regression models.

I'll begin by discussing an example problem that helps to motivate this work.}
  }
}



\section{Motivation}



\begin{frame}{Motivation}
\subt{Response variable}

\begin{center}
  \ig[width=0.6\textwidth]{../../figures/practice-talk/poverty-response}
\end{center}

\note{This is the county-level poverty rate in the upper midwestern states from the US census in 1970

and...}
\end{frame}




\begin{frame}{Motivation}
\subt{Covariates}

\bigskip
\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/poverty-covariates}
\end{center}

\note{Here we have the proportion of people in each county who worked in agriculture, manufacturing, finance, mining, services, and other professions in 1970.

A researcher may approach this data wanting to understand how the poverty rate is related to these variables that describe the economic structure.}
\end{frame}





\begin{frame}{Motivation}
\subt{Scientific questions}

\bigskip
\begin{itemize}
    \item Which of the economic-structure variables is associated with poverty rate?
    \item What are the sign and magnitude of that association?
    \item Is poverty rate associated with the same economic-structure variables across the entire region?
    \item How do the sign and magnitude of the associations vary across the region?
\end{itemize}

\note{These are some sensible scientific questions to ask about the relationship between economic structure and the county-level poverty rate.

Which of the economic-structure variables is associated with poverty rate?

What are the sign and magnitude of that association?

Is poverty rate associated with the same economic-structure variables across the entire region?

How do the sign and magnitude of the associations vary across the region?

The work I'm presenting today attempts to answer these questions and focuses on selecting variables and estimating coefficients locally. I'll begin by discussing some of the methods that can currently be used to approach questions of this kind.}
\end{frame}



\section{Introduction}



\begin{frame}{Introduction}
\subt{An overview}

\bigskip
\begin{itemize}
    \item Definitions
    \item Spatial regression
    \item Varying coefficient regression
    \begin{itemize}
        \item Splines
        \item Wavelets
    \end{itemize}
    \item Model selection via regularization
\end{itemize}

\note{The existing methods to address the questions draw from these areas. Behind the methodology that I'm discussing is a wide range of literature.

Spatial regression

Varying coefficient regression

Splines

Wavelets

Model selection via regularization}
\end{frame}






\begin{frame}{Introduction}
\subt{Definitions}

\bigskip
\begin{itemize}
    \item Univariate spatial response process $\left\{ Y(\bm{s}) : \bm{s} \in \mathcal{D} \right\}$
    \item Multivariate spatial covariate process $\left\{ \bm{X}(\bm{s}) : \bm{s} \in \mathcal{D} \right\}$
    \item $n = $ number of observations
    \item $p = $ number of covariates
    \item Location (2-dimensional) $\bm{s}$
    \item Spatial domain $\mathcal{D}$
\end{itemize}

\note{For the remainder of this talk, I'll be using these terms.

The response Y is a univariate spatial process.

X is a multivariate spatial process of covariates.

n is the number of observations, p is the number of covariates, s indicates the two dimensional spatial location, and indexes that spatial domain D.}
\end{frame}




\begin{comment}
\begin{frame}{Introduction}
\subt{Types of spatial data}

\bigskip
\begin{itemize}
    \item Geostatistical data:
    \begin{itemize}
        \item Observations are made at sampling locations $\bm{s}_i$ for $i = 1, \dots, n$
        \item E.g. elevation, temperature
    \end{itemize}
    \item Areal data:
    \begin{itemize}
        \item Domain is partitioned into $n$ regions $\left\{ D_1, \dots, D_n \right\}$
        \item The regions do not overlap, and they divide the domain completely: $\mathcal{D} = \bigcup_{i=1}^n D_i$
        \item Centroids of the regions are considered to be the sampling locations $\bm{s}_i$ for $i = 1, \dots, n$
        \item E.g. poverty rate, population, spatial mean temperature
    \end{itemize}
\end{itemize}


\note{The method I'm describing applies to geostatistical data, or to areal data when the observations are assumed to be located at the centroid.

The poverty data example is areal data; the simulation study I'll present later is based on simulated geostatistical data.}
\end{frame}
\end{comment}







\begin{frame}{Introduction}
\subt{Spatial linear regression (Cressie, 1993)}

\bigskip
\begin{itemize}
    \item A typical spatial linear regression model
\end{itemize}
\begin{align*}
    Y(\bm{s}) &= \bm{X}(\bm{s})'\bm{\beta} + W(\bm{s}) + \varepsilon(\bm{s}) \\    
\end{align*}

\begin{itemize}
    \item $W(\bm{s})$ is a spatial random effect that accounts for autocorrelation in the response variable
    \item $\varepsilon(\bm{s})$ is iid random noise
    \item The coefficients $\bm{\beta} = \left(1, \beta_1, \dots, \beta_p \right)$ are constant
    \item Requires \emph{a priori} global variable selection
\end{itemize}

\note{Here we have the usual spatial regression as described by Noel Cressie in his 1993 book.

This model extends the traditional linear regression by separating the residuals into:

 - The spatial random effect W that models autocorrelation of the response, and

 - epsilon, which is iid white noise

The spatial regression model estimates the coefficients as constants, and the model relies on a priori global variable selection.}
\end{frame}






\begin{frame}{Introduction}
\subt{Spatially varying coefficient model (Gelfand \emph{et al.}, 2003)}

\bigskip
\begin{itemize}
    \item A more flexible model: coefficients in a spatial regression model can vary
\end{itemize}

\[
    Y(\bm{s}) = \bm{X}(\bm{s})' \bm{\beta}(\bm{s}) + \varepsilon(\bm{s})
\]
\begin{itemize}
  \item $\{ \beta_0(\bm{s}) : \bm{s} \in \mathcal{D}\}, \dots, \{\beta_p(\bm{s}) : \bm{s} \in \mathcal{D}\}$ are stationary spatial processes
  \item Requires \emph{a priori} global variable selection
\end{itemize}

\note{The spatial regression model can be made more flexible by representing the coefficients as stationary spatial processes, rather than constants. The method was introduced by Gelfand in 2003.

The random effect W from the previous slide is now incorporated in the spatially varying intercept process.

This model also relies on a priori global variable selection.}
\end{frame}





\begin{frame}{Introduction}
\subt{Varying coefficients regression (VCR) (Hastie and Tibshirani, 1993)}

\[
    Y(\bm{s}) = \bm{X}(\bm{s})' \bm{\beta}(\bm{s}) + \varepsilon(\bm{s})
\]

\begin{itemize}
    \item Assume an effect modifying variable $\bm{s}$
    \item Coefficients are functions of $\bm{s}$
\end{itemize}

\note{The varying coefficient regression model was described by Hastie and Tibshirani in 1993. The form of this model looks like the spatially varying coefficient process, but this model is more general because the coefficients are not necessarily spatial processes in this model.

In fact, the effect-modifying variable s does not necessarily need to represent spatial location.

There are many ways to fit a varying coefficient regression model. Splines and kernel-based local regression are the most common.}
\end{frame}




\begin{comment}\begin{frame}{Introduction}
\subt{Spline-based VCR models (Wood, 2006)}

\bigskip
\begin{itemize}
    \item Parameterize each coefficient function using splines
    \item Provides some 
    \begin{align*}
        Y(\bm{s}) = \bm{X}(\bm{s})' \bm{\beta}(\bm{s}) + \varepsilon(\bm{s})
    \end{align*}
\end{itemize}

\note{Splines are a method of parameterizing smooth functions as a sum of smooth basis functions

It is possible to use splines to represent the coefficients in a varying coefficient regression model and there is a good overview of this topic in in Simon Wood's 2006 book.

}
\end{frame}
\end{comment}



\begin{frame}{Introduction}
\subt{Global selection in spline-based VCR models}

\bigskip
Global variable selection can be done via regularization in a VCR model where the coefficient functions are modeled as splines
\begin{itemize}
  \item The $L_2$ norm of a function (e.g. $\int \{f(t)\}^2 dt$) is zero if and only if the function is zero everywhere.
  \item Use regularization to penalize nonzero coefficient functions
  \begin{itemize}
    \item SCAD penalty and B-splines (Wang \emph{et al.}, 2008a)
    \item Non-negative garrote penalty and P-splines (Antoniadis \emph{et al.}, 2012b)
  \end{itemize}
\end{itemize}

\note{Splines are a method of parameterizing smooth functions as a sum of smooth basis functions

It is possible to use splines to represent the coefficients in a varying coefficient regression model and there is a good overview of this topic in in Simon Wood's 2006 book.

There are at least two references that describe how to select the covariates for a spline-based VCR model. Both use a similar regularization technique.

The regularization penalizes the smooth coefficient function for being non-zero. Antoniadis et al. used P-splines with a non-negative garrote penalty on the $L_2$ norm and Wang et al. used B-splines with a SCAD penalty on the $L_2$ norm.

These methods select variables globally - that is, the variables are included or excluded on the entire domain. That doesn't suit our goal of selecting locally relevant covariates.}
\end{frame}




\begin{frame}{Introduction}
\subt{Wavelet methods for VCR models}

\bigskip
\begin{itemize}
  \item Wavelet methods: decompose coefficient function into local frequency components
  \item Selection of nonzero local frequency components with nonzero coefficients:
  \begin{itemize}
    \item Bayesian variable selection (Shang, 2011)
    \item Lasso (Zhang and Clayton, 2011)
  \end{itemize} 
  \item Sparsity in the local frequency components; not in the local covariates
\end{itemize}

\note{Another way to fit a VCR model is to use a wavelet decomposition, which decomposes the coefficient function into its local frequency components. Model selection is then used to identify which local frequency components to use in the model.

Murray Clayton's students Zuofeng Shang and Jun Zhang used Bayesian variable selection and the Lasso, respectively, to select the relevant local frequency components.

However, these methods achieve sparsity in the wavelet coefficients, which does not imply sparsity in the covariates. So these methods don't achieve local variable selection.

Now let's take a look at geographically weighted regression.}
\end{frame}




\section{Geographically weighted regression}



\begin{frame}{Geographically weighted regression}
\subt{Brundson \emph{et al.} (1998), Fotheringham \emph{et al.} (2002)}

\bigskip
\begin{itemize}
  \item Consider observations at sampling locations $\bm{s}_1, \dots, \bm{s}_n$
  \item $y(\bm{s}_i) = y_i$ the univariate response at location $\bm{s}_i$
  \item $\bm{x}(\bm{s}_i) = \bm{x}_i$ the $p$-vector of covariates at location $\bm{s}_i$
  \item Assume $y_i = \bm{x}'_i \bm{\beta}_i + \varepsilon_i$ where $\varepsilon_i \overset{iid}{\sim} \mathcal{N} \left( 0,\sigma^2 \right)$
\end{itemize} 

\note{Geographically weighted regression is the method of using local regression to estimate the coefficients in a spatially varying coefficient regression model.

Our sampling locations are called s, the response is y and the covariates (which number p) are called x.

Assume that the errors are iid normal.

The notation $\beta_i$ is used to indicate that he coefficients are specific to location $i$.}
\end{frame}





\begin{frame}{Geographically weighted regression}
\subt{Brundson \emph{et al.} (1998), Fotheringham \emph{et al.} (2002)}

\bigskip
\begin{itemize}
  \item The total log likelihood is
\begin{align*}
\ell\left( \bm{\beta} \right) = - \left(1/2\right) \left\{ n \log \left( 2 \pi \sigma^2\right) +  \sigma^{-2}  \sum_{i=1}^n \left(y_i - \bm{x}'_i\bm{\beta}_i \right)^2  \right\}
\end{align*}
  \item With $n$ observations and $np+1$ parameters, the model is not identifiable.
  \item Idea: to estimate parameters by borrowing strength from nearby observations
\end{itemize} 

\note{We have here the total log likelihood of the observed data.

Because each $\beta_i$ is a p-vector of local coefficients, this model has n observations and np+1 parameters, so the model is not identifiable.

We will estimate the parameters by borrowing strength from nearby observations}
\end{frame}





\begin{frame}{Geographically weighted regression}
\subt{Local regression (Loader, 1999)}

\bigskip
Local regression uses a kernel function at each sampling location to weight observations based on their distance from the sampling location.
\begin{align*}
    \mathcal{L}_i &= \prod_{i'=1}^n \left(\mathcal{L}_{i'}\right)^{w_{ii'}}  \\
    \ell_i &= \sum_{i'=1}^n w_{ii'} \left\{\log \left(\sigma^2\right) + \sigma^{-2}\left(y_{i'} - \bm{x}_{i'}'\bm{\beta}_i\right)^2 \right\}
\end{align*}

Given the weights, a local model is fit at each sampling location using the local likelihood

\note{Local regression uses a kernel function at each sampling location to weight the observations. For a GWR model, the kernel weights are based on an observation's distance from the sampling location.

Here we have the likelihood at one sampling location. Note that each observation is given a weight $w_{ii'}$

Given the weights, a local model is fit at each sampling location using the local likelihood

Maximizing the local likelihood for a model of Gaussian data with iid errors can be done by weighted least squares.}
\end{frame}







\begin{frame}{Geographically weighted regression}
\subt{Local likelihood (Loader, 1999)}

\bigskip
Weights are calculated via a kernel, e.g. the bisquare kernel:
\begin{align}
	w_{ii'} = \begin{cases} \left\{1-\left(\phi^{-1}\delta_{ii'}\right)^2\right\}^2 &\mbox{ if } \delta_{ii'} < \phi, \\ 0 &\mbox{ if } \delta_{ii'} \geq \phi \end{cases}
\end{align}

where
\begin{itemize}
  \item $\phi$ is a bandwidth parameter
  \item $\delta_{ii'} = \delta(\bm{s}_i,\bm{s}_{i'}) = \|\bm{s}_i - \bm{s}_{i'}\|_2$ is the Euclidean distance between sampling locations $\bm{s}_i$ and $\bm{s}_{i'}$.
\end{itemize}

\note{The local weights $w_{ii'}$ from the previous slide are calculated from a kernel.

This is the form of the bisquare kernel, which is what I've used in this work.

$\phi$ is a bandwidth parameter and $\delta_{ii'}$ is the distance between points i and i'.}
\end{frame}





\begin{frame}{Geographically weighted regression}
\subt{Bandwidth estimation via the $\text{AIC}_{\tt c}$ (Hurvich \emph{et al.}, 1998)}

\bigskip
\begin{itemize}
    \item Smaller bandwidth: less bias, more flexible coefficient surface
    \item Large bandwidth: less variance, less flexible coefficient surface
    \item Choose the bandwidth parameter to optimize the bias-variance tradeoff
\end{itemize}

\note{To estimate a GWR model, it is necessary to estimate the bandwidth parameter, which involves a bias-variance tradeoff.

When the bandwidth is small, the coefficient surface is flexible and should have less bias but greater variance.

When the bandwidth is large, the coefficient surface is less flexible so it has less variance but potentially more bias.
}
\end{frame}





\begin{frame}{Geographically weighted regression}
\subt{Bandwidth estimation via the $\text{AIC}_{\tt c}$ (Hurvich \emph{et al.}, 1998)}

\bigskip
\begin{itemize}
    \item The corrected AIC for bandwidth selection is:
    \begin{align*}
      \text{AIC}_{\tt c} = 2 n \log{\sigma} + n \left\{\frac{n + \nu}{n - 2 - \nu}\right\}
    \end{align*}
    \begin{itemize}
        \item $\hat{\bm{y}} = \bm{H}\bm{y}$
        \item $\nu = $ tr($\bm{H}$)
        \item $\bm{H}_j = \left\{\bm{W}\bm{X}(\bm{X}'\bm{W}\bm{X})^{-1}\bm{X}\right\}_j$
        \item Where subscript $j$ indicates the $j$th row of the matrix
    \end{itemize}
\end{itemize}

\note{One way to estimate the GWR bandwidth is via the corrected AIC of Hurvich \emph{et al.}. }
\end{frame}



\begin{comment}
\begin{frame}{Geographically weighted regression}
\subt{Bandwidth estimation via GCV (Wahba, 1990)}

\bigskip
\begin{itemize}
    \item The GCV criterion for bandwidth selection is:
    \begin{align*}
      \text{GCV} = \frac{\sum_{i=1}^n \left(y_i - \hat{y}_i\right)^2}{(n-\nu)^2}
    \end{align*}
    \begin{itemize}
        \item $\hat{\bm{y}} = \bm{H}\bm{y}$
        \item $\nu = \text{tr}(\bm{H})$
        \item $\bm{H}_j = \left\{ \bm{W} \bm{X} (\bm{X}'\bm{W}\bm{X})^{-1} \bm{X} \right\}_j$
        \item Where subscript $j$ indicates the $j$th row of the matrix
    \end{itemize}
\end{itemize}

\note{Another way to estimate the GWR bandwidth is via Generalized Cross Validation as described in Wahba, 1990.}
\end{frame}
\end{comment}






\section{Local variable selection and parameter estimation}




\begin{frame}{Geographically weighted Lasso}
\subt{Geographically weighted Lasso (Wheeler, 2009)}

\bigskip
Within a GWR model, using the Lasso for local variable selection is called the geographically weighted Lasso (GWL).
\begin{itemize}
    \item The GWL requires estimating a Lasso tuning parameter for each local model
    \item Wheeler (2009) estimates the local Lasso tuning parameter at location $\bm{s}_i$ by minimizing a jacknife criterion: $|y_i - \hat{y}_i^{(-i)}|$
    \item The jacknife criterion can only be calculated where data are observed, making it impossible to use the GWL to impute missing data or to estimate the value of the coefficient surface at new locations
    \item Lasso not generally unbiased in variable selection (Fan and Li, 2001; Zou, 2006)
\end{itemize}

\note{For local variable selection in a GWR model, Wheeler proposed the geographically weighted lasso (GWL) in 2009.

At each model location, the Lasso is used to select the locally-relevant predictors

The GWL uses a jacknife criterion to select the local lasso tuning parameters, which means the GWL cannot be used at model locations other than sample locations.

That means the GWL cannot be used for interpolating the coefficient surface or for imputing missing values of the response variable.

Furthermore, the Lasso is generally not unbiased in variable selection, as was suggested in the Fan and Li 2001 paper and proven in Hui Zou's 2006 paper. }
\end{frame}



\begin{comment}

\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted adaptive Lasso (GWAL)}

\bigskip
\begin{itemize}
    \item Local variable selection in a GWR model using the adaptive Lasso (AL) (Zou, 2006)
    \item Under suitable conditions, the AL has an oracle property for selection
    \item Let
\begin{align}
		\mathcal{S}(\bm{\beta}_i) &= -2 \ell_i(\bm{\beta}_i) + \mathcal{J}_2(\bm{\beta}_i) \notag \\
		&= \sum_{i'=1}^n w_{ii'}  \left\{ \log{\left(\sigma^2_i\right)}  + \sigma^{-2}_i  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right\}   \notag \\
		&+ \lambda_i \sum_{j=1}^p |\beta_{ij}| / \gamma_{ij} \notag
\end{align}
\end{itemize}

\note{The geographically weighted adaptive lasso (GWAL) overcomes these shortcomings of the GWL. 

The GWAL uses the adaptive lasso, which has an oracle property under suitable conditions.

S here is the penalized likelihood for a local GWAL model

The adaptive lasso consists of applying a different penalty to each covariate based on the adaptive weights, which are derived from the weighted least squares coefficients.



The adaptive weights $\bm{\gamma}_i = \left(\gamma_{i1}, \dots, \gamma_{ip}\right)'$ are defined in the same way as for the AL, and the elastic net parameter $\alpha_i \in [0,1]$ controls the balance between $\ell_1$ penalty $\lambda_i\sum \limits_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ and $\ell_2$ penalty $\lambda_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2$.
}
\end{frame}





\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted adaptive elastic net (GWAL)}

\bigskip
Note:
\begin{itemize}
  \item $\sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2$ is the weighted sum of squares minimized by traditional GWR
  \item $\mathcal{J}_1(\bm{\beta}_i) = \lambda_i \sum_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ is the AL penalty.
\end{itemize}

\note{
The adaptive weights $\bm{\gamma}_i = \left(\gamma_{i1}, \dots, \gamma_{ip}\right)'$ are defined in the same way as for the AL, and the elastic net parameter $\alpha_i \in [0,1]$ controls the balance between $\ell_1$ penalty $\lambda^*_i\sum \limits_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ and $\ell_2$ penalty $\lambda^*_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2$.
}
\end{frame}



\end{comment}




\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted adaptive elastic net (GWEN)}

\bigskip
To overcome the shortcomings of the GWL, use a variable selection technique with potential for oracle properties and a tuning technique that can be applied anywhere on the domain.

\begin{itemize}
    \item Local variable selection via the adaptive elastic net (AEN) (Zou and Zhang, 2009)
    \item Tuning parameters for local models selected by the BIC (Schwarz, 1978)
\end{itemize}

\note{A modeling method with the potential for an oracle property in variable selection and that can be tuned at model locations other than sampling locations would be superior to the GWL.

I'm proposing in this work the geographically weighted adaptive elastic net, or the GWEN for short. It is a technique for fitting spatially varying coefficient models that uses the adaptive elastic net for local variable selection and the BIC to select the tuning parameters for the local models.}
\end{frame}





\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted adaptive elastic net (GWEN)}

\bigskip
The adaptive elastic net:
\begin{align*}
  \mathcal{S}(\bm{\beta}_i) &= -2 \ell_i(\bm{\beta}_i) + \mathcal{J}_2(\bm{\beta}_i) \\
  &= \sum_{i'=1}^n w_{ii'}  \left\{ \log{\sigma^2_i}  + \left(\sigma^2_i\right)^{-1}  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right\} \\
  &+ \alpha_i \lambda_i \sum_{j=1}^p |\beta_{ij}| / |\gamma_{ij}| \\
  &+ (1-\alpha_i) \lambda_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2
\end{align*}

Where 

\begin{align*}
  \bm{\gamma}_i = \left( \bm{X}' \bm{W}_i \bm{X} \right)^{-1} \bm{X}' \bm{W}_i \bm{Y}
\end{align*}

\note{The adaptive elastic net is a penalized regression technique that applies a different penalty to each covariate based on adaptive weights, which are derived from traditional GWR's weighted least squares coefficients.

S here is the penalized likelihood for a local GWEN model. The elastic net incorporates both an $\ell_1$ and an $\ell_2$ penalty 

The adaptive weights $\bm{\gamma}_i$ are defined in the same way as for the AL, and 

the elastic net parameter $\alpha_i \in [0,1]$ controls the balance between $\ell_1$ penalty and $\ell_2$ penalty}
\end{frame}








\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted adaptive elastic net (GWAL)}

\bigskip
\begin{itemize}
  \item The adaptive Lasso penalty function is (Zou, 2006)
  \begin{align*}
    \mathcal{J}_1(\bm{\beta}_i) = \lambda_i \sum_{j=1}^p |\beta_{ij}| / |\gamma_{ij}|
  \end{align*}
    \item Under suitable conditions, the AL has an oracle property for selection in linear regression
\end{itemize}

\note{The geographically weighted adaptive lasso (GWAL) is a particular case of the GWEN that uses the adaptive lasso for variable selection.

The adaptive Lasso penalty consists of an L1 penalty with adaptive weights.

The adaptive lasso also has an oracle property under suitable conditions in linear regression.}
\end{frame}






\begin{frame}{Local variable selection and  parameter estimation}
\subt{Tuning parameter estimation}

\bigskip
To estimate an AEN tuning parameter for each local model, use a local BIC that allows fitting a local model at any location within the spatial domain
	\begin{align*}
		\mbox{BIC}_i &= -2 \sum_{i'=1}^n \ell_{ii'}  + \log \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i\\
		&= \sum_{i'=1}^n w_{ii'} \left\{ \log \left(2 \pi \right) + \log{ \left(\hat{\sigma}^2\right)} + \hat{\sigma}^{-2} \left(y_{i'} - \bm{x}_{i'}' \hat{\bm{\beta}}_{i'} \right)^2 \right\} \\
		&+ \log \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i
	\end{align*}


\note{Variable selection via the adaptive elastic net requires selecting the tuning parameter $\lambda$. For the GWEN, $\lambda$ is selected by the BIC.

We treat the sum of the weights around the sampling location as the number of observations for the local BIC. The number of variables selected for the model is the degrees of freedom of the local model.

This BIC can be computed for a model at any location within the domain, allowing the GWEN to be used for imputing missing data or interpolating a model between observation locations.}
\end{frame}









\begin{comment}
\begin{frame}{Local variable selection and  parameter estimation}
\subt{Bandwidth parameter estimation}

\bigskip
Letting $H_i = \bm{W}_i \bm{X}\left(\bm{X}'\bm{W}_i\bm{X}\right)^{-1}\bm{X}'$, where $\bm{W}_i$ is the diagonal weight matrix diag$(w_{ii'})$,
 
\begin{align*}
  \hat{\bm{y}} = \bm{H} \bm{y}
\end{align*}

\note{note}
\end{frame}
\end{comment}



\begin{frame}{Local variable selection and  parameter estimation}
\subt{Bandwidth parameter estimation}

\bigskip 
\begin{itemize}
  \item Traditional GWR:
  \begin{itemize}
    \item $\hat{\bm{y}} = \bm{H} \bm{y}$
    \item So traditional GWR is a linear smoother
    \item $\nu = $ tr$(\bm{H})$ is the degrees of freedom for the model
  \end{itemize}

  \item GWEN:
  \begin{itemize}
    \item $\hat{\bm{y}} = \bm{H}^{*}\bm{y} - \bm{T}$
    \item Where $T_i = \left\{ \bm{W}_i^{1/2} \bm{X} \left(\bm{X}' \bm{W}_i \bm{X} + (1-\alpha)\lambda I_p\right)^{-1} \frac{\lambda_i}{\bm{\gamma}_i}\right\}_i$
  \end{itemize}  
  \item GWEN is not a linear smoother
  \item Solution: use GWEN for selection then fit local model for the selected variables via traditional GWR
\end{itemize}

\note{As in the case of traditional GWR, it is necessary to estimate the bandwidth parameter for a GWEN model.

For traditional GWR we can use the corrected AIC or generalized cross validation. Both methods use the the trace of the projection matrix as the degrees of freedom for the model.

Because it incorporates an $\mathcal{L}_1$ penalty, the fitted values from the GWEN are note a linear combination of the observed response. So the adaptive elastic net is not a linear smoother and so there is no projection matrix for a GWEN model.

For this reason, the GWEN is used for local variable selection and the coefficients of the resulting local model are fit using weighted least squares, as in traditional GWR.}
\end{frame}





\begin{frame}{Local variable selection and  parameter estimation}
\subt{Locally linear coefficient estimation}

\bigskip
\begin{itemize}
  \item GWR, GWEN, GWAL: coefficients locally constant
  \begin{itemize}
    \item as in Nadaraya-Watson kernel smoother
    \item Leads to bias where there is a gradient at the boundary 
    \item Counter with locally linear coefficients
  \end{itemize}
  \item Augment with covariate-by-location interactions:
\end{itemize}

  \begin{align*}
    Z_i = \left(\tilde{X}_i \;\; L_i\tilde{X}_i \;\; M_i\tilde{X}_i\right)
  \end{align*}

  Where
  \begin{itemize}
    \item $\tilde{X}_i$ is the matrix of covariates selected for the model at location $\bm{s}_i$
    \item $L_i = \text{diag}\{s_{i',x} - s_{i,x}\}$ for $i' = 1, \dots, n$
    \item $M_i = \text{diag}\{s_{i',y} - s_{i,y}\}$ for $i' = 1, \dots, n$
  \end{itemize}

\note{The traditional GWR fits models with locally constant coefficients, and can be thought of as a Nadaraya-Watson kernel smoother for the regression coefficients. Such a smoother is known to exhibit a "boundary effect", meaning that the smoother produces estimates that are biased near the boundary of the domain, especially when there is a gradient at the boundary.

To reduce the boundary effect, the GWEN can be fit with coefficient estimates that are locally linear, rather than locally constant. This is done after variable selection by augmenting the selected covariates with covariate-by-location interactions.

The interactions are on both the x and y coordinates, so each covariate that is selected for the model will appear three times in the model.

Now I'd like to discuss a simulation study.}
\end{frame}




\section{Simulation study}




\begin{frame}{Simulation study}
\subt{Simulating covariates}

\bigskip
\begin{itemize}
  \item $30 \times 30$ grid on $[0,1] \times [0,1]$
  \item Five covariates $\tilde{X}_1, \dots, \tilde{X}_5$ 
  \item Gaussian random fields:
  \begin{align*}
    \tilde{X}_j &\sim N\left(0, \bm{\Sigma}\right) \text{ for } j = 1, \dots, 5 \notag \\
    \left\{\Sigma\right\}_{i,i'} &= \exp \{ -\tau^{-1} \delta_{ii'}\} \text{ for } i,i' = 1, \dots, n 
  \end{align*}
  \item Colinearity: $\rho$
  \begin{itemize}
    \item none ($\rho=0$)
    \item moderate ($\rho=0.5$)
  \end{itemize}
\end{itemize}

\note{In order to assess the utility of the GWEN for variable selection and coefficient estimation in a varying coefficients model, I performed a simulation study.

Spatial data were simulated on a 30 by 30 grid covering the domain [0,1]x[0,1]. Five covariates were simulated using Gaussian random fields with an exponential covariance function.

The marginal variance of the covariates was one, and the range of the covariance function was 0.1.

The covariates were simulated at two levels of collinearity: none, and moderate, for which the correlation was set to 0.5}
\end{frame}


\begin{frame}{Simulation study}
\subt{Simulating the response}

\bigskip
\begin{itemize}
    \item $Y(\bm{s}) = \bm{X}(\bm{s})'\bm{\beta}(\bm{s}) = \sum_{j=1}^5 \beta_j(\bm{s}) X_j(\bm{s}) + \varepsilon(\bm{s})$
    \item $\beta_1(\bm{s})$, the coefficient function for $X_1$, is nonzero in part of the domain.
    \item Coefficients for $X_2, \dots, X_5$ are zero everywhere
    \item $\varepsilon(\bm{s}) \sim \; iid \;\; N(0,\sigma^2)$
    \begin{itemize}
      \item Low noise: $\sigma = 0.5$
      \item High noise: $\sigma = 1$
    \end{itemize} 
\end{itemize}

\note{With the covariates in hand, the response variable was generated via a linear model. 

There are five covariates; four of them ($\beta_2$ through $\beta_5$) have a coefficient of zero everywhere on the domain.

For all the simulation settings, the coefficient of $\beta_1$ ranges within the domain from a minimum of zero to a maximum of one.

The random noise added to the linear model was iid Gaussian noise at two different settings for the variance. The low-noise setting was $\sigma=0.5$ and the high-noise setting was $\sigma=1$.}
\end{frame}



\begin{frame}{Simulation study}
\subt{Coefficient functions: step, gradient, and parabola}

\bigskip

\begin{figure}
    \begin{center}
    \ig[width=0.35\textwidth]{../../figures/practice-talk/step.jpg}
    \ig[width=0.35\textwidth]{../../figures/practice-talk/gradient.jpg}
    \ig[width=0.35\textwidth]{../../figures/practice-talk/parabola.jpg}
    \end{center}
\end{figure}

\note{The response variable was simulated for three different types of coefficient surface $\beta_1$.

First is a step function where the "step" is on a slope rather than a discontinuity.

Second is a constant gradient.

Third is a parabola centered at the center of the domain.}
\end{frame}




\begin{frame}{Simulation study}
\subt{Simulation settings}

Each setting simulated 100 times:
    \begin{table}[h!]
        \begin{center}
        \begin{tabular}{cccc}
            \hline
            Setting & function & $\rho$ & $\sigma^2$ \\ 
            \hline
            1 & step & 0 & 0.25 \\ 
            2 & step & 0 & 1 \\ 
            3 & step & 0.5 & 0.25 \\ 
            4 & step & 0.5 & 1 \\ 
            \hline
            5 & gradient & 0 & 0.25 \\ 
            6 & gradient & 0 & 1 \\ 
            7 & gradient & 0.5 & 0.25 \\ 
            8 & gradient & 0.5 & 1 \\ 
            \hline
            9 & parabola & 0 & 0.25 \\ 
            10 & parabola & 0 & 1 \\ 
            11 & parabola & 0.5 & 0.25 \\ 
            12 & parabola & 0.5 & 1 
        \end{tabular}
        \end{center}
    \end{table}
\note{The table lists the twelve settings for the simulation study, where once again the parameters being varied across the settings are the coefficient surface $\beta_1$, the amount of colinearity in the covariates, and the variance of the random noise.

Each setting was simulated 100 times and each time, and...}
\end{frame}




\begin{frame}{Simulation study}
\subt{Estimation methods}

    \begin{table}[h!]
        \begin{center}
        \begin{tabular}{l|cc}
method & selection & locally linear \\ 
\hline
GWR & NA & \\
oracular GWR & oracle & x \\
GWEN & GWEN & \\
GWAL & GWAL & \\
GWEN-LLE & GWEN & x\\
GWAL-LLE & GWAL & x
        \end{tabular}
        \end{center}
    \end{table}

\note{The following methods were used to estimate the varying coefficient regression model:

traditional GWR

oracular GWR (with locally linear estimation of the coefficients)

the GWEN with locally constant coefficients

the GWAL with locally constant coefficients

the GWEN with locally linear coefficients

the GWAL with locally linear coefficients.}
\end{frame}





\begin{frame}{Simulation results}
\subt{Summary locations}

\begin{figure}
    \begin{center}
    \ig[width=0.65\textwidth]{../../figures/simulation/summary-locations}
    \end{center}
\end{figure}

\note{The results of the simulation were summarized at these five locations.

Locations two and four are at the "corners" of the step function.

The correct result of selection is ambiguous at some locations where the summary location is at the spot where the true coefficient surface $\beta_4$ changes from zero to nonzero.

In particular, selection is ambiguous at location four for the step function, at location five for the gradient, and at locations one and five for the parabola.

We'll ignore selection accuracy of $\beta_1$ where that is ambiguous

First we will consider the variable selection results of the simulation.}
\end{frame}



\begin{comment}

\begin{frame}{Simulation results}
\subt{Selection}
\input{../../output/prelim-talk/selection}
\note{note}
\end{frame}




\begin{frame}{Simulation results}
\subt{MSE of $\beta_1(\bm{s})$ - step coefficient surface}
\input{../../output/prelim-talk/msex-step}
\note{note}
\end{frame}



\begin{frame}{Simulation results}
\subt{MSE of $\beta_1(\bm{s})$ - gradient coefficient surface}
\input{../../output/prelim-talk/msex-gradient}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{MSE of $\beta_1(\bm{s})$ - parabola coefficient surface}
\input{../../output/prelim-talk/msex-parabola}
\note{note}
\end{frame}





\begin{frame}{Simulation results}
\subt{Variance of $\beta_1(\bm{s})$ - step coefficient surface}
\input{../../output/prelim-talk/varx-step}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Variance of $\beta_1(\bm{s})$ - gradient coefficient surface}
\input{../../output/prelim-talk/varx-gradient}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Variance of $\beta_1(\bm{s})$ - parabola coefficient surface}
\input{../../output/prelim-talk/varx-parabola}
\note{note}
\end{frame}




\begin{frame}{Simulation results}
\subt{Bias of $\beta_1(\bm{s})$ - step coefficient surface}
\input{../../output/prelim-talk/bx-step}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Bias of $\beta_1(\bm{s})$ - gradient coefficient surface}
\input{../../output/prelim-talk/bx-gradient}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Bias of $\beta_1(\bm{s})$ - parabola coefficient surface}
\input{../../output/prelim-talk/bx-parabola}
\note{note}
\end{frame}


\end{comment}




\begin{frame}{Simulation results}
\subt{Selection performance}

\bigskip
\begin{itemize}
  \item GWEN selection (60 cases):
  \begin{itemize}
    \item 21 with no false positives
    \item 30 with no false negatives 
    \item 13 with neither
  \end{itemize}

  \item GWAL selection (60 cases):
  \begin{itemize}
    \item 27 with no false positives
    \item 26 with no false negatives
    \item 17 with neither
  \end{itemize}

  \item Selection errors almost always below five percent
  \item Worst false positive rate: 8\% at location three of the step function (GWEN selection)
  \item Worst false negative rate: 13\% (same location, GWAL selection)
  \item No consistent difference between GWEN and GWAL
\end{itemize}

\note{The GWEN and the GWAL seem to perform well in selection in the simulation.

considering each simulation case to be a specific location for a specific setting,

the GWEN produced 13 cases out of 60 with no selection errors over 100 simulations. The GWAL had 17 cases with no selection errors.

Increased noise variance and Increased colinearity both led to worse selection performance

Several of the cases without selection errors occurred for the settings with moderate colinearity. Only four of the cases without selection errors occurred in settings with high noise variance.
}
\end{frame}



	
\begin{comment}
\begin{frame}{Simulation results}
\subt{Estimation performance}

Best MSE$(\hat{\beta}_1)$ (out of 60 cases):

    \begin{table}[h!]
        \begin{center}
        \begin{tabular}{l|c}
method & count \\ 
\hline
GWR & 7\\
oracular GWR & \textbf{38}\\
GWEN & 11\\
GWAL & 3\\
GWEN-LLE & 5\\
GWAL-LLE & 4
        \end{tabular}
        \end{center}
    \end{table}

\note{note}
\end{frame}
\end{comment}



\begin{frame}{Simulation results}
\subt{Estimation performance}

\begin{itemize}
  \item Cases where oracular GWR clearly had the best performance
  \begin{itemize}
    \item Minimum MSE$(\hat{\beta}_1)$: 38 of 60
    \item Minimum var$(\hat{\beta}_1)$: 44 of 60
  \end{itemize}
  \item Generally small differences between GWR, oracular GWR, GWEN-LLE, and GWAL-LLE
  \item Methods with locally constant coefficients had larger bias at the boundaries
  \item Fitting $\hat{y}$: MSE nearest $\sigma^2$ split between GWAL-LLE, oracle, and GWR
\end{itemize}
\note{The method or oracular selection produced the best performance in terms of the MSE in estimating $\beta_1$, producing the smallest MSE in 38 of the 60 summarized simulation locations. It also had the lowest variance of estimating $\beta_1$ for 44 of 60 cases.

In most cases the estimation accuracy in terms of MSE was not much different between  GWR, oracular GWR, GWEN-LLE, and GWAL-LLE.

At the boundaries of the domain, especially for the parabola coefficient function, the lower bias of the locally linear estimators led them to have lower MSE than the methods that fit locally constant coefficients.

Incerased noise variance led to worse estimation accuracy

Increased colinearity in the covariates led to worse estimation accuracy}
\end{frame}
	


\section{Data example: poverty rate in the upper midwest}



\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Revisiting the motivating example}

\bigskip
\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/poverty-covariates}
\end{center}

\note{We return to the data example that was introduced at the beginning of the talk.

Again, these are the covariates for a model of the county-level poverty rate.}
\end{frame}







\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Data description}

\bigskip
  \begin{itemize}
    \item Response: logit-transformed poverty rate in the Upper Midwest states of the U.S.
    \begin{itemize}
      \item Minnesota, Iowa, Wisconsin, Illinois, Indiana, Michigan
    \end{itemize}
    \item Covariates: employment structure (raw proportion employed in:)
    \begin{itemize}
      \item agriculture
      \item finance, insurance, and real estate
      \item manufacturing
      \item mining
      \item services
      \item other professions
    \end{itemize}
    \item Data source: U.S. Census Bureau's decennial census of 1970
  \end{itemize}

\note{The covariates are the proportion of the county's population working in the economic sectors of agriculture, finance, manufacturing, mining, services, and other professions.

The response variable in this model is the logit-transformed county-level poverty rate from the 1970 US census.

The model's domain is the upper midwest states of Minnesota, Iowa, Wisconsin, Illinois, Indiana, and Michigan.}
\end{frame}



\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Data description}

\bigskip
  \begin{itemize}
    \item Data aggregated to the county level
    \begin{itemize}
      \item counties are areal units
    \end{itemize}
    \item county centroid treated as sampling location
  \end{itemize}

\note{Since data is aggregated on counties and the counties exactly divide the domain, this is actually areal data.

We'll treat is as geostatistical data by assuming each counties data is sampled at the county's centroid.

The data is modeled using both traditional GWR and the GWEN with locally linear coefficient estimates.}
\end{frame}


\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Results from traditional GWR}

\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/1970-GWR-coefficients}
\end{center}

\note{Here are plots of the coefficient estimates from the model fit by traditional GWR.}
\end{frame}




\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Results from GWEN}

\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/1970-GWEN-coefficients}
\end{center}

\note{These are plots of the coefficients estimates from the model fit by the GWEN with locally linear coefficients.

The GWEN has selected just a few covariates as being important predictors of the county level poverty rate.}
\end{frame}




\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Comparing the coefficients from GWR and the GWEN}

\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/1970-GWEN-GWR-comparison}
\end{center}

\note{This slide plots the estimates from the GWEN against those from traditional GWR. Each plot has a 1-1 line, along which the data points would lie if the GWEN and GWR produced identical results.

The GWEN resulted in coefficient estimates that are less variable than traditional GWR.}
\end{frame}




\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Results from GWEN-LLE}

\begin{itemize}
  \item Relatively constant compared to GWR
  \item Not associated with poverty rate: Services, "other professions" sector
  \item Manufacturing: negative coefficient everywhere
  \item Finance, insurance, and real estate negative coefficient everywhere
  \begin{itemize}
    \item Largest magnitude (min: -20, next-largest: -3)
    \item GWR comparable to GWEN-LLE
  \end{itemize}

  \item Manufacturing: negative coefficient everywhere
  \begin{itemize}
    \item GWR: coefficient greater than zero near Chicago and in NW Minnesota
  \end{itemize}

  \item Agriculture: nonzero in western Iowa
  \begin{itemize}
    \item North-south gradient to coefficient
    \item ranges positive to negative
  \end{itemize}

  \item Mining: nonzero in parts south 
  \begin{itemize}
    \item Associated with increased poverty rate
    \item Comparable to GWR within far southern range
  \end{itemize}
\end{itemize}

\note{Some observations about the models produced by traditional GWR and the GWEN-LLE:

per the GWEN, Employment in services and the "other professions" sectors did not affect the poverty rate anywhere.

Employment in manufacturing and finance were both associated with a decreased poverty rate across the entire domain. This was not dissimilar to the relationship estimated by traditional GWR.

Agricultural employment was a selected as a meaningful predictor of the poverty rate only in western Iowa. Within that region there was a north-south gradient to the coefficient.}
\end{frame}


\section{Future work}


\begin{frame}{Future work}
\begin{itemize}
  \item Apply the GWEN to models for non-Gaussian response variable
  \item Investigate the asymptotic properties of the GWEN
  \item Incorporate spatial autocorrelation in the model
  \item PalEON project: modeling and mapping tree biomass in the upper midwest
\end{itemize}
\note{note}
\end{frame}



\begin{frame}{Thank you!}

\note{Thank you all, that concludes my presentation. I'd like to especially acknowledge Jun for her patient work advising my research, and Katherine for her collaboration on the analysis of the county-level poverty data.}
\end{frame}




\end{document}
