\documentclass[12pt,t]{beamer}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{multirow}
\setbeameroption{hide notes}
\setbeamertemplate{note page}[plain]

% get rid of junk
\usetheme{default}
\beamertemplatenavigationsymbolsempty
\hypersetup{pdfpagemode=UseNone} % don't show bookmarks on initial view

% tables
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

% font
\usefonttheme{professionalfonts}
\usefonttheme{serif}
\usepackage{fontspec}
\setmainfont{Helvetica Neue}
\setbeamerfont{note page}{family*=pplx,size=\footnotesize} % Palatino for notes
\setbeamerfont{frametitle}{size=\large}

% named colors
\definecolor{offwhite}{RGB}{249,242,215}
\definecolor{foreground}{RGB}{25,25,25}
\definecolor{background}{RGB}{250,250,245}
\definecolor{title}{RGB}{20,20,20}
\definecolor{gray}{RGB}{80,80,80}
\definecolor{subtitle}{RGB}{20,20,20}
\definecolor{hilight}{RGB}{255,127,0}
\definecolor{vhilight}{RGB}{255,111,207}
\definecolor{lolight}{RGB}{155,155,155}
%\definecolor{green}{RGB}{125,250,125}

% use those colors
\setbeamercolor{titlelike}{fg=title}
\setbeamercolor{subtitle}{fg=subtitle}
\setbeamercolor{institute}{fg=gray}
\setbeamercolor{normal text}{fg=foreground,bg=background}
\setbeamercolor{item}{fg=foreground} % color of bullets
\setbeamercolor{subitem}{fg=gray}
\setbeamercolor{itemize/enumerate subbody}{fg=gray}
\setbeamertemplate{itemize subitem}{{\textendash}}
\setbeamerfont{itemize/enumerate subbody}{size=\footnotesize}
\setbeamerfont{itemize/enumerate subitem}{size=\footnotesize}

% page number
\setbeamertemplate{footline}{%
    \raisebox{5pt}{\makebox[\paperwidth]{\hfill\makebox[20pt]{\color{gray}
          \scriptsize\insertframenumber}}}\hspace*{5pt}}

% add a bit of space at the top of the notes page
\addtobeamertemplate{note page}{\setlength{\parskip}{12pt}}

% a few macros
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ig}{\includegraphics}
\newcommand{\subt}[1]{{\footnotesize \color{subtitle} {#1}}}

% title info
\title{Local variable selection and parameter estimation for spatially varying coefficient models}
%\subtitle{A researcher's perspective}
\author{\href{http://www.somesquares.org}{Wesley Brooks}}
\institute{\href{http://www.stat.wisc.edu}{Department of Statistics} \\[2pt] \href{http://www.wisc.edu}{University of Wisconsin{\textendash}Madison}}


%bibliography stuff
\usepackage[autostyle]{csquotes}
\usepackage[backend=biber,style=authoryear,natbib=true]{biblatex}
\addbibresource{../../references/gwr.bib}


\setbeamerfont{section title}{parent=title,size=\large}
\setbeamercolor{section title}{parent=titlelike}
\defbeamertemplate*{mod section page}{default}[1][]
{
  \centering
    \vspace{3cm}
    \begin{beamercolorbox}[sep=8pt,center,#1]{section title}
      \usebeamerfont{section title}\insertsection\par
    \end{beamercolorbox}
}
\newcommand*{\modsectionpage}{\usebeamertemplate*{mod section page}}

\AtBeginSection{\frame{\modsectionpage}}



\begin{document}

% title slide
{
  \setbeamertemplate{footline}{} % no page number here
  \frame{
    \titlepage
    \note{These slides were prepared for a practice version of my preliminary exam to advance to Ph.D candidacy in statistics at the University of Wisconsin{\textendash}Madison.}
  }
}



\section{Motivation}



\begin{frame}{Motivation}
\subt{Take a look at some data}

\begin{center}
  \ig[width=0.6\textwidth]{../../figures/practice-talk/poverty-response}
\end{center}

\note{This is the county-level poverty rate from 1970, as well as the proportion of people who worked in manufacturing, agriculture, and services.

How is this data to be analyzed?}
\end{frame}




\begin{frame}{Motivation}
\subt{Take a look at some data}

\bigskip
\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/poverty-covariates}
\end{center}

\note{
This is the county-level poverty rate from 1970, as well as the proportion of people who worked in manufacturing, agriculture, and services.

How is this data to be analyzed?
}
\end{frame}





\begin{frame}{Motivation}
\subt{Sensible questions about the data}

\bigskip
\begin{itemize}
    \item Which of the economic-structure variables is associated with poverty rate?
    \item What are the sign and magnitude of that association?
    \item Is poverty rate associated with the same economic-structure variables across the entire region?
    \item Are the sign and magnitude of the associations constant across the region?
\end{itemize}

\note{
We're going to aim at answering these questions with the work I present today.

There are several other methods to answer at least some of these questions, which we'll cover next.
}
\end{frame}



\section{Introduction}



\begin{frame}{Introduction}
\subt{A review of existing methods}

\bigskip
\begin{itemize}
    \item Spatial regression
    \item Varying coefficient regression
    \begin{itemize}
        \item Splines
        \item Kernels
        \item Wavelets
    \end{itemize}
    \item Model selection via regularization
\end{itemize}

\note{
Behind the methodology that I'm discussing is a wide range of literature.
}
\end{frame}






\begin{frame}{Introduction}
\subt{Some definitions}

\bigskip
\begin{itemize}
    \item Univariate spatial response process $\left\{ Y(\bm{s}) : \bm{s} \in \mathcal{D} \right\}$
    \item Multivariate spatial covariate process $\left\{ \bm{X}(\bm{s}) : \bm{s} \in \mathcal{D} \right\}$
    \item $n = $ number of observations
    \item $p = $ number of covariates
    \item Location (2-dimensional) $\bm{s}$
    \item Spatial domain $\mathcal{D}$
\end{itemize}


\note{
We'll use these variables throughout. 
}
\end{frame}





\begin{frame}{Introduction}
\subt{Further definitions}

\bigskip
\begin{itemize}
    \item Geostatistical data:
    \begin{itemize}
        \item Observations are made at sampling locations $\bm{s}_i$ for $i = 1, \dots, n$
        \item E.g. elevation, temperature
    \end{itemize}
    \item Areal data:
    \begin{itemize}
        \item Domain is partitioned into $n$ regions $\left\{ D_1, \dots, D_n \right\}$
        \item The regions do not overlap, and they divide the domain completely: $\mathcal{D} = \bigcup_{i=1}^n D_i$
        \item Sampling locations $\bm{s}_i$ for $i = 1, \dots, n$ are the centroids of the regions
        \item E.g. poverty rate, population, spatial mean temperature
    \end{itemize}
\end{itemize}


\note{
The method I'm describing applies to geostatistical data, or to areal data when the observations are assumed to be located at the centroid.

The poverty data example is areal data, the simulation study is based on simulated geostatistical data.
}
\end{frame}








\begin{frame}{Introduction}
\subt{Spatial regression \citep{Cressie:1993}}

\bigskip
\begin{itemize}
    \item The typical spatial regression
\end{itemize}

\begin{align}
    Y(\bm{s}) &= \bm{X}(\bm{s})'\bm{\beta} + W(\bm{s}) + \varepsilon(\bm{s}) \notag \\
    \text{cov}(W(\bm{s}), W(\bm{t})) &= \Gamma\left(\delta(\bm{s}, \bm{t})\right) \notag \\
    \delta(\bm{s}, \bm{t}) &= \sqrt{\|\bm{t} - \bm{s}\|_2} \notag \\
    \text{E.g. } \Gamma(\delta(\bm{s}, \bm{t})) &= \exp \{ -\phi^{-1} \delta(\bm{s}, \bm{t}) \}    
\end{align}

\begin{itemize}
    \item $W(\bm{s})$ is a spatial random effect that accounts for autocorrelation in the response variable
    \item The coefficients $\bm{\beta}$ are constant
    \item Relies on \emph{a priori} global variable selection
\end{itemize}

\note{
This is the form of the usual spatial regression from e.g. Cressie (1993).

The spatial random effect W captures autocorrelation of the response, while the white noise is iid error

The Gamma function is a Matern-class covariance function, such as the exponential covariance function (listed here)
}
\end{frame}






\begin{frame}{Introduction}
\subt{Spatially varying coefficient process \citep{Gelfand:2003}}

\bigskip
\begin{itemize}
    \item Making model more flexible: coefficients in a spatial regression model can be allowed to vary
\end{itemize}

\[
    Y(\bm{s}) = \bm{X}(\bm{s})' \bm{\beta}(\bm{s}) + \varepsilon(\bm{s})
\]

\begin{itemize}
    \item The spatial random effect has been incorporated into the spatially varying intercept
    \item $\{ \beta_1(\bm{s}) : \bm{s} \in \mathcal{D}\}, \dots, \{\beta_p(\bm{s}) : \bm{s} \in \mathcal{D}\}$ are stationary spatial processes with Mat\`{e}rn covariance functions
    \item Still relies on \emph{a priori} global variable selection
\end{itemize}

\note{note}
\end{frame}





\begin{frame}{Introduction}
\subt{Varying coefficients regression \citep{Hastie:1993a}}

\[
    Y(\bm{s}) = \bm{X}(\bm{s})' \bm{\beta}(\bm{s}) + \varepsilon(\bm{s})
\]

\begin{itemize}
    \item Assume an effect modifying variable $S$
    \item Coefficients are functions of $S$
\end{itemize}

\note{note}
\end{frame}




\begin{frame}{Introduction}
\subt{Spline-based VCR models \citep{Wood:2006}}

\bigskip
\begin{itemize}
    \item Splines are a way to parameterize smooth functions
    \item Splines can be incorporated into a generalized additive model (GAM):
    \begin{itemize}
        \item $E\{Y(t)\} = f\{X_1(t)\} + \cdots + f\{X_p(t)\}$
    \end{itemize}
    \item It is possible to parameterize a VCR model with splines for the coefficient functions:
    \begin{itemize}
        \item $E\{Y(t)\} = \beta_1(t) X_1(t) + \cdots + \beta_p(t) X_p(t)$
    \end{itemize}
\end{itemize}

\note{note}
\end{frame}




\begin{frame}{Introduction}
\subt{Global selection in spline-based VCR models \citep{Wang:2008a, Antoniadis:2012b}}

\bigskip
Regularization methods for global variable selection in VCR models:
\begin{itemize}
  \item The integral of a function squared (e.g. $\int \{f(t)\}^2 dt$) is zero if and only if the function is zero everywhere.
  \item Use regularization to encourage coefficient functions to be zero
  \begin{itemize}
    \item SCAD penalty
    \item Non-negative garrote penalty
  \end{itemize}
\end{itemize}

\note{These selection methods are all global - that is, they select variables for the entire domain simultaneously}
\end{frame}




\begin{frame}{Introduction}
\subt{Wavelet methods for VCR models \citep{Shang-2011,Zhang-2011}}

\bigskip
\begin{itemize}
  \item Wavelet methods: decompose coefficient function into local frequency components
  \item Selection of nonzero local frequency components with nonzero coefficients:
  \begin{itemize}
    \item Bayesian variable selection
    \item Lasso
  \end{itemize} 
  \item Sparsity in the local frequency components; not in the local covariates
\end{itemize}

\note{note}
\end{frame}




\section{Geographically weighted regression}



\begin{frame}{Geographically weighted regression}
\subt{\citep{Brundson:1998a, Fotheringham:2002}}

\bigskip
\begin{itemize}
  \item Consider observations at sampling locations $\bm{s}_1, \dots, \bm{s}_n$
  \item $y(\bm{s}_i) = y_i$ the univariate response at location $\bm{s}_i$
  \item $\bm{x}(\bm{s}_i) = \bm{x}_i$ the $(p+1)$-variate vector of covariates at location $\bm{s}_i$
  \item Assume $y_i = \bm{x}'_i \bm{\beta}_i + \varepsilon_i$ where $\varepsilon \overset{iid}{\sim} \mathcal{N} \left( 0,\sigma^2 \right)$
\end{itemize} 

\note{
Maximizing the local likelihood for a model of Gaussian data with iid errors can be done by weighted least squares.
}
\end{frame}





\begin{frame}{Geographically weighted regression}
\subt{\citep{Brundson:1998a, Fotheringham:2002}}

\bigskip
\begin{itemize}
  \item The total log likelihood is $\ell\left( \bm{\beta} \right) = - \left(1/2\right) \left\{ n \log \left( 2 \pi \sigma^2\right) +  \sigma^{-2}  \sum_{i=1}^n \left(y_i - \bm{x}'_i\bm{\beta}_i \right)^2  \right\}$
  \item With $n$ observations and $n(p+1)$ free parameters, the model is not identifiable.
  \item Estimate parameters by borrowing strength from nearby observations
\end{itemize} 

\note{
Maximizing the local likelihood for a model of Gaussian data with iid errors can be done by weighted least squares.
}
\end{frame}





\begin{frame}{Geographically weighted regression}
\subt{Local regression \citep{Loader:1999}}

\bigskip
Local regression uses a kernel function at each sampling location to weight observations based on their distance from the sampling location.
\begin{align}
    L_i &= \prod_{i'=1}^n \left(L_{i'}\right)^{w_{ii'}} \notag \\
    \ell_i &= \sum_{i'=1}^n w_{ii'} \left\{\log \sigma_i^{2} + \sigma_i^{-2}\left(y_{i'} - \bm{x}_{i'}'\bm{\beta}_i\right)^2 \right\} \notag
\end{align}

Given the weights, a local model is fit at each sampling location using the local likelihood

\note{note}
\end{frame}






\begin{frame}{Geographically weighted regression}
\subt{Local likelihood\citep{Loader:1999}}

\bigskip
Weights are calculated via a kernel, e.g. the bisquare kernel:
\begin{align}
	w_{ii'} = \begin{cases} \left[1-\left(\phi^{-1}\delta_{ii'}\right)^2\right]^2 &\mbox{ if } \delta_{ii'} < \phi, \\ 0 &\mbox{ if } \delta_{ii'} \geq \phi. \end{cases}
\end{align}

Where
\begin{itemize}
  \item $\phi$ is a bandwidth parameter
  \item $\delta_{ii'} = \delta(\bm{s}_i,\bm{s}_{i'}) = \|\bm{s}_i - \bm{s}_{i'}\|_2$ is the Euclidean distance
\end{itemize}

\note{
Maximizing the local likelihood for a model of Gaussian data with iid errors can be done by weighted least squares.
}
\end{frame}





\begin{frame}{Geographically weighted regression}
\subt{Bandwidth estimation via the $\text{AIC}_{\tt c}$ \citep{Hurvich:1998}}

\bigskip
\begin{itemize}
    \item Smaller bandwidth: less bias, more flexible coefficient surface
    \item Large bandwidth: less variance, less flexible coefficient surface
    \item Estimate the degrees of freedom used in estmating the coefficient surface:
    \begin{itemize}
        \item $\hat{\bm{y}} = H\bm{y}$
        \item $\nu = $ tr($H$)
    \end{itemize}
    \item Then the corrected AIC for bandwidth selection is: $\text{AIC}_{\tt c} = 2 n \log{\sigma} + n \left\{\frac{n + \nu}{n - 2 - \nu}\right\}$
\end{itemize}

\note{
Maximizing the local likelihood for a model of Gaussian data with iid errors can be done by weighted least squares.
}
\end{frame}







\section{Local variable selection and parameter estimation}



\begin{frame}{Geographically weighted regression}
\subt{Geographically weighted Lasso \citep{Wheeler:2009}}

\bigskip
Within a GWR model, using the Lasso for local variable selection is called the geographically weighted Lasso (GWL).

\begin{itemize}
    \item The GWL requires estimating a Lasso tuning parameter for each local model
    \item \cite{Wheeler:2009} estimates the local Lasso tuning parameter at location $\bm{s}_i$ by minimizing a jacknife criterion: $|y_i - \hat{y}_i|$
    \item The jacknife criterion can only be calculated where data are observed, making it impossible to use the GWL to impute missing data or to estimate the value of the coefficient surface at new locations
    \item Also, the Lasso is known to be biased in variable selection and suboptimal for coefficient estimation
\end{itemize}

\note{GWL does local variable selection}
\end{frame}




\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted elastic net (GWEN)}

\bigskip
\begin{itemize}
    \item Local variable selection in a GWR model using the adaptive elastic net (AEN) \citep{Zou:2009}
    \item Under suitable conditions, the AEN has an oracle property for selection
\end{itemize}

\begin{align}
		\mathcal{S}(\bm{\beta}_i) &= -2 \ell_i(\bm{\beta}_i) + \mathcal{J}_2(\bm{\beta}_i) \notag \\
		&= \sum_{i'=1}^n w_{ii'}  \left\{ \log{\sigma^2_i}  + \left(\sigma^2_i\right)^{-1}  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right\}   \notag \\
		&+ \alpha_i \lambda_i \sum_{j=1}^p |\beta_{ij}| / \gamma_{ij} \notag \\
		&+ (1-\alpha_i) \lambda^*_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2 \notag
	\end{align}

\note{
The adaptive weights $\bm{\gamma}_i = \left(\gamma_{i1}, \dots, \gamma_{ip}\right)'$ are defined in the same way as for the AL, and the elastic net parameter $\alpha_i \in [0,1]$ controls the balance between $\ell_1$ penalty $\lambda^*_i\sum \limits_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ and $\ell_2$ penalty $\lambda^*_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2$.
}
\end{frame}



\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted elastic net (GWEN)}

\bigskip
where $\sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2$ is the weighted sum of squares minimized by traditional GWR, and $\mathcal{J}_2(\bm{\beta}_i) = \alpha_i \lambda^*_i \sum_{j=1}^p |\beta_{ij}| / \gamma_{ij} + (1-\alpha_i) \lambda^*_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2$ is the AEN penalty.

\note{
The adaptive weights $\bm{\gamma}_i = \left(\gamma_{i1}, \dots, \gamma_{ip}\right)'$ are defined in the same way as for the AL, and the elastic net parameter $\alpha_i \in [0,1]$ controls the balance between $\ell_1$ penalty $\lambda^*_i\sum \limits_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ and $\ell_2$ penalty $\lambda^*_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2$.
}
\end{frame}





\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted elastic net (GWEN)}

\bigskip
It is necessary to estimate an AEN tuning parameter for each local model. Using the local BIC allows fitting a local model at any location within the domain
	\begin{align}\label{eq:BIC_loc}
		\mbox{BIC}_i &= -2 \sum_{i'=1}^n \ell_{ii'}  + \log \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i\notag\\
		&= -2 \sum_{i'=1}^n \log \left[ \left(2 \pi \hat{\sigma}_i^2\right)^{-1/2} \exp \left\{-\frac{1}{2} \hat{\sigma}_i^{-2} \left(y_{i'} - \bm{x}_{i'}' \hat{\bm{\beta}}_{i'} \right)^2\right\} \right]^{w_{ii'}} \notag \\
		&+ \log \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i\notag\\
	\end{align}


\note{
The adaptive weights $\bm{\gamma}_i = \left(\gamma_{i1}, \dots, \gamma_{ip}\right)'$ are defined in the same way as for the AL, and the elastic net parameter $\alpha_i \in [0,1]$ controls the balance between $\ell_1$ penalty $\lambda^*_i\sum \limits_{j=1}^p |\beta_{ij}| / \gamma_{ij}$ and $\ell_2$ penalty $\lambda^*_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2$.
}
\end{frame}







\begin{frame}{Local variable selection and  parameter estimation}
\subt{Geographically weighted elastic net (GWEN)}

\bigskip
	\begin{align}
		&= \sum_{i'=1}^n w_{ii'} \left\{ \log \left(2 \pi \right) + \log \hat{\sigma}_i^2 + \hat{\sigma}_i^{-2} \left(y_{i'} - \bm{x}_{i'}' \hat{\bm{\beta}}_{i'} \right)^2 \right\} \notag \\
		&+ \log \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i \notag
	\end{align}


\note{We treat the sum of the weights around the sampling location as the number of observations for the local BIC.}
\end{frame}




\section{Simulation study}




\begin{frame}{Simulation study}
\subt{Simulating covariates}

\bigskip
Five covariates $\tilde{X}_1, \dots, \tilde{X}_5$ were simulated by Gaussian random fields on the domain $[0,1] \times [0,1]$ on a $30 \times 30$ grid of sampling locations:
\begin{align}
    \tilde{X}_j &\sim N(0, \Sigma) \text{ for } j = 1, \dots, 5 \notag \\
    \left\{\Sigma\right\}_{i,i'} &= \exp \{ -\tau^{-1} \delta_{ii'}\} \text{ for } i,i' = 1, \dots, n \notag
\end{align}

Where the covariates were simulated with colinearity, the colinearity was induced by multiplying the design matrix by the square root of the colinearity matrix:
\begin{align}
    \text{diag}(\Omega_{5\times5}) = 1 \notag \\
    \text{off-diag}(\Omega_{5\times5}) = \rho \notag \\
    X = \tilde{X} R
\end{align}
Where $\Omega_{5\times5} = R'R$ is the Cholesky decomposition.

\note{note}
\end{frame}


\begin{frame}{Simulation study}
\subt{Simulating the response}

\bigskip
\begin{itemize}
    \item $Y(\bm{s}) = X(\bm{s})'\bm{\beta}(\bm{s}) = \sum_{j=1}^5 \beta_j(\bm{s}) X_j(\bm{s}) + \varepsilon(\bm{s})$
    \item $\bm{\varepsilon} \sim \; iid \;\; N(0,\sigma^2) $ 
    \item $\beta_1(\bm{s})$, the coefficient function for $X_1$, is nonzero in part of the domain.
    \item Coefficients for $X_2, \dots, X_5$ are zero everywhere
\end{itemize}

\note{note}
\end{frame}



\begin{frame}{Simulation study}
\subt{Coefficient functions}

\bigskip
Call these functions step, gradient, and parabola:

\begin{figure}
    \begin{center}
    \ig[width=0.35\textwidth]{../../figures/practice-talk/step.jpg}
    \ig[width=0.35\textwidth]{../../figures/practice-talk/gradient.jpg}
    \ig[width=0.35\textwidth]{../../figures/practice-talk/parabola.jpg}
    \end{center}
\end{figure}

\note{note}
\end{frame}




\begin{frame}{Simulation study}
\subt{Simulation settings}
    % latex table generated in R 2.15.1 by xtable 1.7-0 package
    % Fri Jan 18 10:19:47 2013
    \begin{table}[h!]
        \begin{center}
        \begin{tabular}{cccc}
            \hline
            Setting & function & $\rho$ & $\sigma^2$ \\ 
            \hline
            1 & step & 0 & 0.25 \\ 
            2 & step & 0 & 1 \\ 
            3 & step & 0.5 & 0.25 \\ 
            4 & step & 0.5 & 1 \\ 
            \hline
            5 & gradient & 0 & 0.25 \\ 
            6 & gradient & 0 & 1 \\ 
            7 & gradient & 0.5 & 0.25 \\ 
            8 & gradient & 0.5 & 1 \\ 
            \hline
            9 & parabola & 0 & 0.25 \\ 
            10 & parabola & 0 & 1 \\ 
            11 & parabola & 0.5 & 0.25 \\ 
            12 & parabola & 0.5 & 1 
        \end{tabular}
        \end{center}
        \caption{Simulation parameters for each setting.\label{table:simulation_settings}}
    \end{table}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Selection}

\begin{figure}
    \begin{center}
    \ig[width=0.65\textwidth]{../../figures/simulation/summary-locations}
    \end{center}
\end{figure}

\note{note}
\end{frame}



\begin{comment}

\begin{frame}{Simulation results}
\subt{Selection}
\input{../../output/prelim-talk/selection}
\note{note}
\end{frame}




\begin{frame}{Simulation results}
\subt{MSE of $\beta_1(\bm{s})$ - step coefficient surface}
\input{../../output/prelim-talk/msex-step}
\note{note}
\end{frame}



\begin{frame}{Simulation results}
\subt{MSE of $\beta_1(\bm{s})$ - gradient coefficient surface}
\input{../../output/prelim-talk/msex-gradient}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{MSE of $\beta_1(\bm{s})$ - parabola coefficient surface}
\input{../../output/prelim-talk/msex-parabola}
\note{note}
\end{frame}





\begin{frame}{Simulation results}
\subt{Variance of $\beta_1(\bm{s})$ - step coefficient surface}
\input{../../output/prelim-talk/varx-step}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Variance of $\beta_1(\bm{s})$ - gradient coefficient surface}
\input{../../output/prelim-talk/varx-gradient}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Variance of $\beta_1(\bm{s})$ - parabola coefficient surface}
\input{../../output/prelim-talk/varx-parabola}
\note{note}
\end{frame}




\begin{frame}{Simulation results}
\subt{Bias of $\beta_1(\bm{s})$ - step coefficient surface}
\input{../../output/prelim-talk/bx-step}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Bias of $\beta_1(\bm{s})$ - gradient coefficient surface}
\input{../../output/prelim-talk/bx-gradient}
\note{note}
\end{frame}


\begin{frame}{Simulation results}
\subt{Bias of $\beta_1(\bm{s})$ - parabola coefficient surface}
\input{../../output/prelim-talk/bx-parabola}
\note{note}
\end{frame}


\end{comment}




\begin{frame}{Simulation results}
	Table \ref{table:selection} lists the results of variable selection. The correct covariate was usually included in the local models, and the unimportant covariates were usually excluded. Ignore for now the ambiguous locations where the true $\beta_1$ surface transitions from zero to nonzero. Of the eighty simulated cases where $\beta_1(\bm{s})$ is unambiguously nonzero, more than half (52) saw no false negatives (over 100 simulations). The number with no false negatives and no false positives (i.e. exactly the correct model was recovered in all 100 simulations) was 26. Of the 120 total simulated cases, 72 had no false positives (i.e. no variable whose true coefficient is zero was included in the model during any of the 100 simulation runs). 
	
	Selection performance was more affected by an increase in the noise variance from $\sigma^2_{\varepsilon}=0.25$ to $\sigma^2_{\varepsilon}=1$ than by an increase in colinearity from $\rho = 0$ to $\rho = 0.5$. Of the 44 cases where model selection recovered exactly the correct model in all 100 runs of the simulation, only five arose from cases where $\sigma^2_{\varepsilon}=1$, while 19 arose from cases where $\rho = 0.5$. The worst error rates that were observed in these unambiguous cases were a false positive rate of 6\% (location one of the step function with $\sigma^2_{\varepsilon}=1$, $\rho=0.5$, and selection via the elastic net) and a false negative rate of 16\% (location three of the step function with $\sigma^2_{\varepsilon}=1$, $\rho=0.5$, and selection via the lasso).
			
	Model selection was ambiguous at locations where the true $\beta_1(\bm{s})$ transitions from zero to nonzero. At location four of the step function, the selection rate of $\beta_1$ ranged from 43\% to 60\% among the different simulation settings. At location five of the gradient, the range of selection rates was 63\% to 82\%, and the selection rate across locations one and five of the parabola ranged from 27\% to 66\%. 

	There is no indication that the GWEN performed better in selection than the GWAL, even in cases where the covariates were moderately correlated ($\rho=0.5$).
	
    The mean squared error, bias, and variance of $\hat{\beta}_1$ (MSE$(\beta_1)$, bias$(\hat{\beta}_1)$, var$(\hat{\beta}_1)$) are listed in Tables \ref{table:msex}, \ref{table:bx}, and \ref{table:varx}, respectively. The method of oracular selection led to the best MSE$(\hat{\beta}_1)$ in 41 of the 60 cases.
	
	In terms of MSE$(\hat{\beta}_1)$, while oracular selection clearly was the most accurate estimation method in most cases, the difference in accuracy between the estimation methods was modest in most cases. There were a few cases when the difference in MSE$(\hat{\beta}_1)$ between estimation methods amounted to at least an order of magnitude. At locations one and five of the parabola, oracular selection produces much more accurate estimation than GWEN, GWAL, or GWR because locations one and five are on the domain boundary where the parabola has a strong gradient, and those methods don't use locally linear fits to account for the boundary effect. This can also be seen from the fact that the bias$(\bm{\beta}_1)$ of GWEN, GWAL, and GWR is large at locations one and five of the parabola, where it is nearly zero for GWEN-LLE, GWAL-LLE, and oracular GWR.
	
	A similar boundary effect is apparent at location five of the gradient, where GWEN, GWAL, and GWR produce a bias$(\hat{\beta}_1)$ and MSE$(\hat{\beta}_1)$ that are an order of magnitude or more greater than those of GWEN-LLE, GWAL-LLE, and oracular GWR (the differences in var$(\hat{\beta}_1)$ are smaller).
	
	At location one of the step function, the MSE$(\bm{\beta}_1)$ and var$(\bm{\beta}_1)$ for GWR are much smaller than for the other estimation methods, including oracular GWR, while the bias$(\bm{\beta}_1)$ doesn't vary much between estimation methods. It is not clear why this is the case.
		
	As was the case for selection, accuracy in coefficient estimation seemed to suffer more from an increase in the noise variance than from increased correlation in the covariates. Once again, this effect is probably most apparent at location three of the step function.	
	
The MSE of the $\hat{y}$, MSE$(\hat{y})$, is listed in Table \ref{table:msey}. Nominally, MSE$(\hat{y})$ should be equal to the noise variance, $\sigma_{\varepsilon}^2$, which is 1 for odd-numbered rows and 0.25 for even numbered rows. Of the 60 simulation cases, GWR's MSE$(\hat{y})$ was nearest to the known noise variance for 16, compared to 15 for oracular GWR, 14 for the GWAL-LLE, nine for each of the GWEN and the GWAL, and seven for the GWAL-LLE.

\note{note}
\end{frame}
	


\section{Data example: poverty rate in the upper midwest}



\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Revisiting the introductory example}

\bigskip
\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/poverty-covariates}
\end{center}

\note{
This is the county-level poverty rate from 1970, as well as the proportion of people who worked in manufacturing, agriculture, and services.

How is this data to be analyzed?
}
\end{frame}






\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Results from traditional GWR}

\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/1970-GWR-coefficients}
\end{center}

\note{note}
\end{frame}




\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Data description}

  \begin{itemize}
    \item Response: logit-transformed poverty rate in the Upper Midwest states of the U.S.
    \begin{itemize}
      \item Minnesota, Iowa, Wisconsin, Illinois, Indiana, Michigan
    \end{itemize}
    \item Covariates: employment structure (raw proportion employed in:)
    \begin{itemize}
      \item agriculture
      \item finance, insurance, and real estate
      \item manufacturing
      \item mining
      \item services
      \item other professions
    \end{itemize}
    \item Data source: U.S. Census Bureau's decennial census of 1970
  \end{itemize}

\note{note}
\end{frame}



\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Data description}

  \begin{itemize}
    \item Data aggregated to the county level
    \begin{itemize}
      \item counties are areal units
    \end{itemize}
    \item county centroid treated as sampling location
  \end{itemize}

\note{note}
\end{frame}


\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Results from GWEN}

\begin{center}
  \ig[width=\textwidth]{../../figures/practice-talk/1970-GWEN-coefficients}
\end{center}

\note{note}
\end{frame}


\begin{frame}{Data example: poverty rate in the upper midwest}
\subt{Results from GWEN}
	The coefficient surfaces estimated by the GWEN-LLE are relatively constant as compared to the those estimated by GWR. The GWEN-LLE indicates that the proportion of residents employed in services or in the ``other professions" category does not affect the poverty rate anywhere within the Upper Midwest states, while the proportion of residents employed in manufacturing or in finance, insurance, and real estate have negative coefficients (meaning a negative association with poverty rate) in all but one county of the Upper Midwest states (that one county is at the extreme northwest corner of Minnesota).

The coefficient of employment in finance, insurance, and real estate has a larger magnitude than the other covariates (minimum value near -20, as opposed to the next-largest-magnitude coefficient, that of manufacturing employment, with a minimum near -3), indicating that counties with many workers in this sector tended to have low poverty rates in 1970. The local coefficients for employment in finance, insurance and real estate as estimated by GWR are comparable to those estimated by the GWEN-LLE.

Manufacturing employment was also associated with reduced poverty rates across the entire Upper Midwest in 1970. While the coefficient is smaller than for employment in the finance, insurance and real estate sector, there were many more people employed in manufacturing. In the eastern part of the Upper Midwest, many counties have about half of their workers employed in manufacturing, while there are just a few counties scattered through the Upper Midwest that approach the maximum of ten percent of workers employed in finance, insurance, and real estate (Figure \ref{fig:1970-covariates}). Like the GWEN-LLE, traditional GWR estimates that the coefficient of manufacturing employment was negative across most of the Upper Midwest, but traditional GWR estimates that the coefficient was very slightly greater than zero around Chicago and in northwestern Minnesota.

The proportion of residents employed in agriculture is estimated to have affected the poverty rate of 1970 only in the western half of Iowa. In the counties where agricultural employment was associated with the poverty rate, there was a north-south gradient to the coefficient surface, from 0.5 in the northern part of western Iowa to -1 in the southern part. In those counties, the proportion of residents employed in agriculture is between 0.25 and 0.5. In this part of western Iowa, GWR finds a very slight positive association between agricultural employment and the poverty rate. The coefficient estimated by GWR is of a greater magnitude almost everywhere else in the domain than in western Iowa.

There were relatively few people employed in mining in the Upper Midwest in 1970. In the extreme northern part of the domain, some sparsely populated counties had about 25\% of residents working in mining, and in the extreme southern part of the domain, several counties had about 10\% of residents employed in mining. According to the GWEN-LLE, the coefficient of mining employment is zero except in the far southern part of the domain. There, mining employment is associated with an increased poverty rate (the coefficient is about 3). Within the far southern part of the domain, GWR and the GWEN-LLE produce similar estimates of the coefficient of mining employment, but in parts of the Upper Midwest where the GWEN-LLE says that mining employment is not associated with the poverty rate, GWR estimates large local coefficients for mining employment, ranging between 20 and -15.
\note{note}
\end{frame}


\section{Future work}


\begin{frame}{Future work}
\begin{itemize}
    \item Apply the GWEN to data with non-Gaussian response variable
    \item Incorporate spatial autocorrelation in the model (simulated errors were iid)
\end{itemize}
\note{note}
\end{frame}



\begin{frame}{Acknowledgements}

\end{frame}




\end{document}
