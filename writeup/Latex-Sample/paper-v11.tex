%
% Date: 9/15/2012 Rao
% Date: 9/18/2012 Jun
%
%
\documentclass[authoryear,review, 12pt]{elsarticle}

\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.5in}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{color}
\usepackage{bm}

\newtheorem{theorem}{Theorem}

\newcommand\im{\textup{Im\,}}
\newcommand\re{\textup{Re\,}}
\newcommand\prsp{(\Omega, \mathcal F, \mathbf P)}
\newcommand\pr{\mathbf P}
\newcommand{\fej}[1]{\noindent\textsc{#1}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\E}{\mathbb{E}}

\journal{JABES}

\\begin{document}

\begin{frontmatter}

\title{Fast Estimation and Selection of Autologistic Regression Models via Penalized Pseudolikelihood}

\author[rfu]{Rao Fu}
\ead{rfu7@wisc.edu}

\author[athurman]{Andrew L. Thurman}
\ead{athurman@wisc.edu}

%\address[athurman]{Department of Statistics, University of Wisconsin at Madison, Madison, WI 53706}

\author[msteen]{Michelle M. Steen-Adams}
\ead{msteenadams@une.edu}

\author[jzhu]{Jun~Zhu}
\ead{jzhu@stat.wisc.edu}

\address[rfu]{Department of Statistics, University of Wisconsin, Madison, WI 53706}
\address[athurman]{Department of Statistics, University of Wisconsin, Madison, WI 53706}
\address[msteen]{Department of Environmental Studies, University of New England, Biddeford, ME 04005}
\address[jzhu]{Department of Statistics and Department of Entomology, University of Wisconsin, Madison, WI 53706}

\begin{abstract}
Autologistic regression models are suitable for relating spatial binary responses in ecology to covariates such as environmental factors.  Model parameters can be estimated via maximum likelihood or Bayesian methods, although they often involve Monte Carlo simulation and tend to be computationally intensive.  For big ecological data, pseudolikelihood estimation is appealing due to its ease of computation, but at least two challenges remain.  Although an important issue, it is unclear how model selection may be carried out under pseudolikelihood.  In addition, for assessing the variation of pseudolikelihood estimates, parametric bootstrap using Monte Carlo simulation is often used but may be infeasible for very large data sizes.  Here both these issues are addressed by developing a penalized pseudolikelihood estimation method and an approximation of the variance of the parameter estimates.   A simulation study is conducted to evaluate the performance of the proposed method, followed by a data example in a study of land cover in relation to land ownership characteristics.  These models and methods are further extended to deal with spatial-temporal binary data.
\end{abstract}

\begin{keyword}
Binary data; Markov random field; model selection; spatial statistics.
\end{keyword}

\end{frontmatter}

%
% \label{sec:intro}
%
\section{Introduction}
\label{sec:intro}
 
Autologistic regression is suitable for modeling the relationship between a spatial binary response and covariates in environmental and ecological studies (see, e.g., \cite{gumpertzg97}; \cite{hufferw98}; \cite{hez03}).  For big ecological data, however, there are several challenging issues in terms of both computation and methodology for statistical inference. This paper aims to address some of these issues and in particular, we develop a new, computationally efficient method for selection of covariates and estimation of model parameters.

Several approaches are possible for modeling spatial binary response in relation to covariates.  One possibility is spatial generalized linear mixed models (GLMM), where the response variable is categorical and is linked to a regression on covariates and a spatial random effect as two additive components in a link function.  Bayesian hierarchical modeling provides a general framework for drawing statistical inference about the regression coefficients and the spatial random effects, although maximum likelihood estimation is also possible \citep{diggler07, banerjeec03}.  Bayesian or maximum likelihood estimation for spatial GLMM tends to be computationally intensive, as the computation often involves Markov chain Monte Carlo (MCMC) algorithms.  

Autologistic regression offers an attractive alternative. For an autologistic model (without regression), maximum pseudolikelihood (MPL) estimation is easy to implement  and fast to compute \citep{besag72, besag74}.  Maximum likelihood estimation is a challenging problem because of a normalizing constant that is analytically intractable in the likelihood function.  The idea of Monte Carlo maximum likelihood (MCML) can be adopted to deal with the normalizing constant problem \citep{geyer94}.  Various researchers have considered Bayesian estimation using innovative ideas such as auxiliary variables \citep{mollerp06}, Monte Carlo approximations \citep{sunc08}, and reduced dependence approximations \citep{frielp09}.  These methods tend to focus on autoregression but not as much on regression.  

For an autologistic model with regression, the idea of MPL can be applied \citep{gumpertzg97} as well as MCML \citep{hufferw98}.  More recently, \cite{carageak09} suggested to center the autocovariates in the model, which allows more meaningful interpretation of the coefficients in the autologistic model.  Here we will consider the centering suggested by \cite{carageak09} and focus on MPL.  Compared with MCML, MPL is much faster to compute, which can be a great advantage for the analysis of big data \citep{hughesh11, wangz12, zhengz08}.  There are, however, at least two issues that appear to be unresolved.  First, there appear to be no existing methods for the selection of covariates using MPL. For autologistic regression, \cite{zhuz08} applied MCML for regression and used an approximate information criterion for model comparison.  When the number of candidate models is large, however, the usefulness of information criteria is fairly limited due to a high computational cost.  Second, the estimates of the variance of the MPL estimates are largely based on parametric bootstrap where the data in the bootstrap are simulated using MCMC \citep{gumpertzg97, zhuh05}.  There clearly is a need to improve these aspects of the MPL method, if it is to be used for analyzing big ecological data. 

%For autologistic regression, model parameters can be estimated via maximum likelihood or Bayesian methods, although they involve Monte Carlo simulation and tend to be computationally intensive.  For big ecology data, pseudolikelihood estimation is appealing due to its ease of computation, but a couple of challenges remain.  Although an important issue, it is unclear how model selection may be carried out under pseudolikelihood.  In addition, for assessing the variation of pseudolikelihood estimates, parametric bootstrap using Monte Carlo simulation is often used but may be infeasible for very large data sizes.  

Different approaches can be taken for variable selection in a standard linear regression assuming independent response variables.  Regularization (or, penalized) methods for standard linear regression are becoming popular using, for example, least absolute shrinkage and selection operator (Lasso) \citep{tibshirani96}, adaptive Lasso \citep{zou06}, and least angle regression (LARS) algorithm \citep{efronh04}.  Penalized methods have also been developed for spatial data with continuous response variables (see, e.g., \cite{huangh10}, \cite{zhul09}, \cite{zhuh10}) but not much with discrete responses.  In this paper, we utilize the idea of adaptive Lasso and develop a penalized pseudolikelihood estimation method for selection of covariates.  In addition, unlike the existing approach to estimate the variance of the parameter estimates by bootstrapping, we propose an analytical form based on asymptotic results.  

In the remainder of the paper, we will present the autologistic regression models in Section~\ref{sec:model} and describe our method in Section~\ref{sec:infer}.  A simulation study is conducted in Section~\ref{sec:simu} to evaluate the performance of the proposed method, followed by a data example in a study of land cover in relation to land ownership characteristics in Section~\ref{sec:data}.  Conclusions and a discussion about extending our method to deal with spatial-temporal binary data are in Section~\ref{sec:disc}.

%
% \label{sec:model}
%
\section{Autologistic Regression Models} 
\label{sec:model}

\subsection{Autologistic Regression}
\label{subsec:model:reg}

For $i=1,\ldots,n$, let $Z_i$ denote the response variable at the $i$th site on a spatial lattice, such that $Z_i=0$ or $1$.  Let $\bm{Z}=(Z_1, \ldots, Z_n)'$ denote the binary response variables at all $n$ sites on this lattice and $\bm{Z}_{-i}=(Z_1,\ldots,Z_{i-1},Z_{i},\ldots,Z_n)'$ denote the vector that has all the response variables of $\bm{Z}$ except for $Z_i$.  Further, consider  a pre-specified spatial neighborhood structure.  For example, the first-order neighborhood consists of the four nearest neighbors on a regular grid.  Let ${\cal N}_i$ denote the set of indices of the neighbors of site $i$ and let $i'\thicksim i$ denote that site $i'$ is a neighbor of site $i$ such that $i'\in{\cal N}_i$.

To model the response variables $\bm{Z}$, we assume a Markov random field in the sense that the probability of the $i$th response, $Z_i$, conditional on $\bm{Z}_{-i}$ depends on only the responses in the neighborhood, $Z_{i'}$, where $i'\in{\cal N}_i$.  That is,
\begin{eqnarray}
p(Z_i|\bm{Z}_{-i}) &=& p(Z_i|Z_{i'}: i'\thicksim i).
\label{eqn:Zi}
\end{eqnarray}
Further, we assume that the conditional distribution $p(Z_i|Z_{i'}: i'\thicksim i)$ is Bernoulli with success probability $\pi_i$ such that
\begin{eqnarray}
\pi_i &=& p(Z_i=1|Z_{i'}: i'\thicksim i)
\label{eqn:pii}
\end{eqnarray}
where $\pi_i$ depends on $Z_{i'}$ for $i'\in{\cal N}_i$ via a logit link function
\begin{eqnarray}
\mbox{logit}(\pi_i) &=& \log\frac{\pi_i}{1-\pi_i}=\bm{x}'_i\bm{\beta}+\sum_{i'\thicksim i}\eta_{ii'}Z_{i'}.
\label{eqn:logit}
\end{eqnarray}
In (\ref{eqn:logit}),  $\bm{x}_i$ denotes a $(p+1)$-dimensional vector of intercept 1 and $p$ covariates at site $i$ and $\bm{\beta}$ denotes the corresponding vector of the regression coefficients.  Also, $\eta_{ii'}$ for $i'\thicksim i$ denotes the autoregression coefficient between sites $i$ and $i'$ and the term $Z_{i'}$ can be thought of as an autocovariate.  In the special case that all the autoregression coefficients are zero ($\eta_{ii'}=0$), the model (\ref{eqn:Zi})--(\ref{eqn:logit}) reduces to a traditional logistic regression with independent responses.  

We will restrict our attention to a constant autoregression coefficient $\eta_{ii'}=\eta$ for all $i'\in{\cal N}_i$ and the logit link becomes
\begin{eqnarray}
\mbox{logit}(\pi_i) &=& \log\frac{\pi_i}{1-\pi_i}=\bm{x}'_i\bm{\beta}+\sum_{i'\thicksim i}\eta Z_{i'}.
\label{eqn:logitConst}
\end{eqnarray}
However, this assumption may be relaxed to include different orders of neighborhood \citep{zhuh10}.  

\subsection{Centered Model}
\label{subsec:model:regCenter}

\cite{carageak09} proposed to center the autocovariate around its expected value to achieve more meaningful interpretations for regression purposes.  Here we consider this centered autologistic regression model by modifying (\ref{eqn:logitConst}) to
\begin{eqnarray}
\mbox{logit}(\pi_i)&=&\log\frac{\pi_i}{1-\pi_i}=\bm{x}'_i\bm{\beta}+\sum_{i'\thicksim i}\eta(Z_{i'}-\mu_{i'}),
\label{eqn:logitCenter}
\end{eqnarray}
where $\mu_{i'}$ is the expectation of $Z_{i'}$ assuming independence (i.e., $\eta=0$) and thus
\begin{eqnarray}
\mu_{i'}&=&\frac{\exp(\bm{x}_{i'}'\bm{\beta})}{1+\exp(\bm{x}_{i'}'\bm{\beta})}.
\end{eqnarray}     
The terms $\{\mu_i\}_{i=1}^n$ may be interpreted as the large-scale structure of the random field $\bm{Z}$ and with regression, relate the response variables to covariates.  The difference between the autocovariate and $\mu_i$ is $Z_i-\mu_i$ and represents the small-scale structure after adjusting for the large-scale structure.  The effects of covariates are therefore captured in the regression coefficients $\bm{\beta}$ and the local spatial dependence in the autoregression coefficient $\eta$.  In contrast, the uncentered autologistic regression model (\ref{eqn:logit}) does not permit such a clear interpretation \citep{carageak09}.

\subsection{Alternative Coding}
\label{subsec:model:code}

Although it is common to use 0 or 1 to code failure or success in the Bernoulli trial, in the uncentered model, the positive-valued autocovariates would artificially inflate the chance of success \citep{hughesh11}.  An alternative coding is $-1$ for failure and $+1$ for success \citep{gaetang10}. We believe that the issue with the 0-1 coding can be mitigated by the $\pm 1$ coding, however.  If there are more $+1$ autocovariates than $-1$ in a neighborhood, then the sum of autocovariates is positive and thus the success probability increases (or decreases) with a positive (or negative) autoregression coefficient $\eta$; and the opposite holds when there are more $-1$ autocovariates than $+1$ in the neighborhood. 

Here, we let $\tilde{Z}_i=2Z_i-1$ denote the binary response variable at site $i$ under this alternative coding.  Define $\tilde{\bm{Z}}$ and $\tilde{\bm{Z}}_{-i}$ as the counterparts of $\bm{Z}$ and $\bm{Z}_{-i}$.  Let $\tilde{\pi}_i=P(\tilde{Z}_i=+1|\tilde{Z}_{i'}:i'\thicksim i)$ denote the success probability.  The logit link function in the uncentered model is
\begin{eqnarray}
\mbox{logit}(\tilde{\pi}_i)&=&\log\frac{\tilde{\pi}_i}{1-\tilde{\pi}_i}=\bm{x}'_i\tilde{\bm{\beta}}+\sum_{i'\thicksim i}\tilde{\eta}\tilde{Z}_{i'},
\label{eqn:logitPM}
\end{eqnarray}
but in the centered model is
\begin{eqnarray}
\mbox{logit}(\tilde{\pi}_i)&=&\log\frac{\tilde{\pi}_i}{1-\tilde{\pi}_i}=\bm{x}'_i\tilde{\bm{\beta}}+\sum_{i'\thicksim i}\tilde{\eta}(\tilde{Z}_{i'}-\tilde{\mu}_{i'}),
\label{eqn:logitPMcenter}
\end{eqnarray}
where it can be shown that the expectation of $\tilde{Z}_i$ assuming independence is
\begin{eqnarray}
\tilde{\mu}_{i'}&=&\frac{\sinh(\bm{x}_{i'}'\tilde{\bm{\beta}})}{\cosh(\bm{x}_{i'}'\tilde{\bm{\beta}})}.
\end{eqnarray}     

In fact, there is a one-to-one correspondence between the coefficients under the two different codings.  For uncentered models, we have $\tilde{\eta}=\eta/4$ and $\bm{x}_i'\tilde{\bm{\beta}}=\bm{x}_i'\bm{\beta}/2+|{\cal N}_i|\eta/4$, where $|{\cal N}_i|$ denotes the cardinality of ${\cal N}_i$ (i.e., the number of neighbors of the $i$th site). A similar connection can be made for centered models. See Appendix for more details. 

%
% \label{sec:infer}
%
\section{Estimation and Selection of Autologistic Models}
\label{sec:infer}

\subsection{Maximum Pseudolikelihood}
\label{subsec:infer:pseudo}

Let $\bm{\theta}=(\bm{\beta}',\eta)'$ denote the vector of all the parameters for either centered or uncentered models using the 0-1 coding of the response variable.  Let $\tilde{\bm{\theta}}=(\tilde{\bm{\beta}}',\tilde{\eta})'$ be analogous to $\bm{\theta}$ but for the models using $\pm 1$ coding.  We adopt maximum pseudolikelihood here for estimating the model parameters such that the pseudolikelihood function is the product of the full conditional probabilities at all sites \citep{cressie93}.  

In an uncentered model, the full conditional distribution of $Z_i$ is 
\begin{eqnarray}
p(Z_i=1|\bm{Z}_{-i}; \bm{\theta})=p(Z_i=1|Z_{i'}:i'\thicksim i; \bm{\theta}) =\frac{\exp({\bm{x}'_i\bm{\beta}+\eta\sum_{i'\thicksim i}Z_{i'}})}{1+\exp(\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}Z_{i'})}
\label{eqn:uncp}
\end{eqnarray}
and $p(Z_i=0|\bm{Z}_{-i}; \bm{\theta})=1/\{1+\exp(\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}Z_{i'})\}$.  Thus, the log-pseudolikelihood function for the uncentered model is 
\begin{eqnarray}
\ell_{\rm p}^{\rm u}(\bm{\theta}|\bm{Z})&=&\sum_{i=1}^n\log\frac{\exp\{{Z_i(\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}Z_{i'}})\}}{ 1+\exp(\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}Z_{i'})}.
\label{eqn:plZ}
\end{eqnarray}
Similarly, in a centered model, the full conditional distribution of $Z_i$ is 
\begin{eqnarray}
p(Z_i=1|\bm{Z}_{-i}; \bm{\theta})=\frac{\exp{\{\bm{x}'_i\bm{\beta}+\eta\sum_{i'\thicksim i}(Z_{i'}-\mu_{i'})}\}}{1+\exp\{\bm{x}'_i\bm{\beta}+\eta\sum_{i'\thicksim i}(Z_{i'}-\mu_{i'})\}}
\label{eqn:cp}
\end{eqnarray}
and thus, the log-pseudolikelihood function is
\begin{eqnarray}
\ell_{\rm p}^{\rm c}(\bm{\theta}|\bm{Z})&=&\sum_{i=1}^n\log\frac{\exp[{Z_i\{\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}(Z_{i'}-\mu_{i'})}\}]}{1+\exp\{\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}(Z_{i'}-\mu_{i'})\}}.
\label{eqn:plZcenter}
\end{eqnarray}
Maximizing the log-pseudolikelihood function gives the maximum pseudolikelihood estimate (MPLE) of $\bm{\theta}$ and we denote them as $\widehat{\bm{\theta}}_{\rm p}=\mbox{argmax}\{\ell_{\rm p}(\bm{\theta}|\bm{Z})\}$ for either $\ell_{\rm p}^{\rm u}(\bm{\theta}|\bm{Z})$ in (\ref{eqn:plZ}) or $\ell_{\rm p}^{\rm c}(\bm{\theta}|\bm{Z})$ in (\ref{eqn:plZcenter}).

Under the alternative $\pm 1$ coding of responses, the full conditional distribution of $\tilde{Z}_i$ in an uncentered model is
\begin{eqnarray}
p(\tilde{Z}_i=1|\tilde{\bm{Z}}_{-i}; \tilde{\bm{\theta}})&=&\frac{\exp(\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i'\thicksim i}\tilde{Z}_{i'})}{2\cosh(\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i'\thicksim i}\tilde{Z}_{i'})}
\end{eqnarray}
and in a centered model is
\begin{eqnarray}
p(\tilde{Z}_i=1|\tilde{\bm{Z}}_{-i}; \tilde{\bm{\theta}})&=&\frac{\exp\{\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i'\thicksim i}(\tilde{Z}_{i'}-\tilde{\mu}_{i'})\}}{2\cosh\{\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i'\thicksim i}(\tilde{Z}_{i'}-\tilde{\mu}_{i'})\}}.
\end{eqnarray}
The log-pseudolikelihood function for the uncentered model is
\begin{eqnarray}
\ell_{\rm p}^{\rm u}(\tilde{\bm{\theta}})&=&\sum_{i=1}^n\log\frac{\exp\{\tilde{Z}_i(\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i'\thicksim i}\tilde{Z}_{i'})\}}{2\cosh(\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i'\thicksim i}\tilde{Z}_{i'})}
\label{eqn:plY}
\end{eqnarray}
and for the centered model is
\begin{eqnarray}
\ell_{\rm p}^{\rm c}(\tilde{\bm{\theta}})&=&\sum_{i=1}^n\log\frac{\exp[\tilde{Z}_i\{\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i'\thicksim i}(\tilde{Z}_{i'}-\tilde{\mu}_{i'})\}]}{2\cosh\{\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i'\thicksim i}(\tilde{Z}_{i'}-\tilde{\mu}_{i'})\}}.
\label{eqn:plYcenter}
\end{eqnarray}
We denote the MPLE of $\tilde{\bm{\theta}}$ by $\widehat{\tilde{\bm{\theta}}}_{\rm p}=\mbox{argmax}\{\ell_{\rm p}(\tilde{\bm{\theta}}|\tilde{\bm{Z}})\}$ for either $\ell_{\rm p}^{\rm u}(\tilde{\bm{\theta}}|\tilde{\bm{Z}})$ in (\ref{eqn:plY}) or $\ell_{\rm p}^{\rm c}(\tilde{\bm{\theta}}|\tilde{\bm{Z}})$ in (\ref{eqn:plYcenter}).

\subsection{Variable Selection}
\label{subsec:infer:varsel}

For an uncentered or centered model using the 0-1 coding of the response, we propose a penalized log-pseudolikelihood function 
\begin{eqnarray}
\ell_{\rm pp}(\bm{\theta})= \ell_{\rm p}(\bm{\theta})-n\sum_{j=1}^{p}\lambda_j|\beta_j|
\label{eqn:ppl}
\end{eqnarray}
where in the second term of (\ref{eqn:ppl}), the regression coefficients are subject to an $\ell_1$-penalty function and  $\lambda_j$ is a regularization parameter for the $j$th regression coefficient $\beta_j$, where $j=1,\ldots,p$.  That is, no penalty is applied to the intercept $\beta_0$ nor the autoregression coefficient $\eta$.  Maximizing (\ref{eqn:ppl}) would enable variable selection and parameter estimation simultaneously, because the regression coefficients of the ``unimportant" covariates will be shrunk to zero under the $\ell_1$-penalty and nonzero estimates of the other parameters will be given.  Because the regularization parameter $\lambda_j$ varies by $j$, the penalization is akin to adaptive Lasso \citep{zou06}.

To maximize (\ref{eqn:ppl}), we propose a one-step approximation. We first set the MPLE of $\bm{\theta}$ to be the initial value.  That is, $\widehat{\bm{\theta}}^{(0)}=\widehat{\bm{\theta}}_{\rm p}\equiv(\widehat{\bm{\beta}}^{(0)'},\widehat{\eta})'$.  We approximate the penalized log-pseudolikelihood function (\ref{eqn:ppl}) up to a constant by
\begin{eqnarray}
\ell_{\rm pp}(\bm{\beta})&=&(\bm{\beta}-\widehat{\bm{\beta}}^{(0)})'\frac{\partial \ell_{\rm p}(\widehat{\bm{\theta}}^{(0)})}{\partial\bm{\beta}}-(1/2)(\bm{\beta}-\widehat{\bm{\beta}}^{(0)})'  \bm{\mathcal{I}}(\widehat{\bm{\beta}}^{(0)}) (\bm{\beta}-\widehat{\bm{\beta}}^{(0)})\nonumber\\
&&\quad\quad\quad\quad-n\sum_{j=1}^{p}\lambda_j|\beta_j|
\end{eqnarray}
where $\bm{\mathcal{I}}(\bm{\beta})=- \frac{\partial^2 \ell_{\rm p}(\bm{\theta})}{\partial\bm{\beta} \partial\bm{\beta}'}$ is the negative of the second-order derivative of $\ell_{\rm p}(\bm{\theta})$ with respect to $\bm{\beta}$. We update $\widehat{\bm{\beta}}^{(0)}$ to be $\widehat{\bm{\beta}}^{(1)}=  \underset{\bm{\beta}}{\arg\max}\{\ell_{\rm pp}(\bm{\beta})\}$. It can be shown that this solution can be attained equivalently by 
\begin{eqnarray}
\widehat{\bm{\beta}}^{(1)}&=& \underset{\bm{\beta}}{\arg\min}\left\{(1/2)(\bm{y}^{\ast}-\bm{X}^{\ast}\bm{\beta}^{\ast})' (\bm{y}^{\ast}-\bm{X}^{\ast}\bm{\beta}^{\ast}) +n\sum_{j=1}^{p}|\beta_j^\ast|\right\}
\label{eqn:quad}
\end{eqnarray}
where 
\begin{eqnarray*}
\bm{y}^{\ast}&=&(\bm{B}^{-1})'\left\{\frac{\partial\ell_{\rm p}(\widehat{\bm{\theta}}^{(0)})}{\partial\bm{\beta}} + \bm{\mathcal{I}}(\bm{\widehat{\beta}}^{(0)})\bm{\widehat{\beta}}^{(0)}\right\},
\bm{X}^{\ast}=\bm{B}{\rm diag}\{\lambda_j^{-1}\}_{j=1}^p,\\
\bm{\beta}^{\ast}&=&{\rm diag}\{\lambda_j\}_{j=1}^p\bm{\beta},\ \mathcal{I}(\bm{\widehat{\beta}}^{(0)})=\bm{B}'\bm{B}.
\end{eqnarray*}
The minimization in (\ref{eqn:quad}) can be solved by a LARS algorithm and the computation is generally fast. We approximate the maximum penalized pseudolikelihood estimate (MPPLE), or $\widehat{\bm{\beta}}_{\rm pp}$, by $\widehat{\bm{\beta}}^{(1)}$ and let $\widehat{\bm{\theta}}_{\rm pp}=(\widehat{\bm{\beta}}_{\rm pp}',\widehat{\eta})'$.  In particular, some of the entries of $\widehat{\bm{\beta}}_{\rm pp}$ are zero (or nonzero), indicating that the corresponding covariates are selected out of (or kept in) the final selected model.

In addition, for estimating the regularization parameters $\{\lambda_j\}_{j=1}^p$, we let $\lambda_j=\lambda\log(n)/(n|\widehat{\beta}_j|)$ and compute a Bayesian information criterion (BIC) type criterion for determining $\lambda$.  In particular, define
\begin{eqnarray}
\mbox{BIC}_{\rm p}(\lambda)=-2\ell_{\rm p}(\widehat{\bm{\theta}}_{\rm pp};\lambda)+e(\lambda)\log(n)
\end{eqnarray}
where $e(\lambda)$ denotes the number of nonzero estimates in $\widehat{\bm{\beta}}_{\rm pp}$.  We estimate $\lambda$ by $\widehat\lambda=\mbox{argmin}_{\lambda}\{\mbox{BIC}_{\rm p}(\lambda)\}$.  Even though $\mbox{BIC}_{\rm p}(\lambda)$ is not the actual BIC under maximum likelihood, it is in the same spirit and our empirical study via simulation will demonstrate that the method works well as a variable selection technique.

For models using $\pm 1$ as the response variables, the MPPLE of $\tilde{\bm{\theta}}$ can be obtained analogously by considering $\ell_{\rm p}(\tilde{\bm{\theta}})$ and applying the same penalty as in (\ref{eqn:ppl}).  The resulting maximum penalized pseudolikelihood estimates are denoted as $\widehat{\tilde{\bm{\theta}}}_{\rm pp}=(\widehat{\tilde{\bm{\beta}}}_{\rm pp}',\widehat{\tilde{\eta}})'$.  

\subsection{Variance Estimation}
\label{subsec:infer:se}

Let $\bm{\mathcal{J}}(\bm{\beta})
%=\frac{\partial\ell_{{\rm p}}(\bm{\theta})}{\partial\bm{\beta}}\{\frac{\partial\ell_{{\rm p}}(\bm{\theta})}{\partial\bm{\beta}}\}'$. 
=\sum_{i=1}^n\sum_{i'\thicksim i, i'=i}\frac{\partial\ell_{{\rm p}i}(\bm{\theta})}{\partial\bm{\beta}}\{\frac{\partial\ell_{{\rm p}i'}(\bm{\theta})}{\partial\bm{\beta}}\}'$, where $\ell_{{\rm p}i}$ is the log-pseudolikelihood of site $i$. 
By arguments similar to \cite{gaetang10}, the following central limit theorem holds for the MPLE $\widehat{\bm{\beta}}_{\rm p}$:
\begin{eqnarray}
\{\bm{\mathcal{J}}(\widehat{\bm{\beta}}_{\rm p})\}^{-1/2} \bm{\mathcal{I}}(\widehat{\bm{\beta}}_{\rm p})\{\widehat{\bm{\beta}}_{\rm p}-\bm{\beta}\} \rightarrow_d \ \mathcal{N}_p(0,\bm{I}_{p+1}).
\label{eqn:CLT}
\end{eqnarray}
Therefore, an estimate of the variance of $\bm{\widehat{\beta}}_{\rm p}$ is
\begin{eqnarray}
\widehat{Var}(\widehat{\bm{\beta}}_{\rm p})\approx\bm{\mathcal{I}}(\widehat{\bm{\beta}}_{\rm p})^{-1}\bm{\mathcal{J}}(\widehat{\bm{\beta}}_{\rm p})\bm{\mathcal{I}}(\widehat{\bm{\beta}}_{\rm p})^{-1}.
\label{eqn:var}
\end{eqnarray}  
For the MPPLE, we replace the MPLE $\widehat{\bm{\beta}}_{\rm p}$ in the variance formula (\ref{eqn:var}) with the vector of non-zero entries of the MPPLE $\widehat{\bm{\beta}}_{\rm pp}$. Thus, the operations involved in the variance estimation are of dimension up to $(p+1)\times(p+1)$ and generally manageable. We will show by a simulation study that these variance estimates perform reasonably well for finite samples.  

More specifically, for the uncentered model and the 0-1 coding, the $\bm{\mathcal{I}}(\bm{\beta})$ and $\bm{\mathcal{J}}(\bm{\beta})$ are
\begin{eqnarray*}
\bm{\mathcal{I}}(\bm{\beta})&=&\sum_{i=1}^n\bm{x}_i\bm{x}_i'\frac{\exp({\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}Z_{i'}})}{\{1+\exp(\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}Z_{i'})\}^2}\quad\mbox{and} \\
\bm{\mathcal{J}}(\bm{\beta})&=&\sum_{i=1}^n\sum_{i'\thicksim i, i'=i}\left[\bm{x_i}\left\{Z_i-\frac{\exp(\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}Z_{i'})}{1+\exp(\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}Z_{i'})}\right\}\right]\\ 
&&\quad\quad\quad\quad\quad\quad\left[\bm{x}_{i'}\left\{Z_{i'}-\frac{\exp({\bm{x}_{i'}'\bm{\beta}+\eta\sum_{i''\thicksim i'}Z_{i''}})}{1+\exp(\bm{x}_{i'}'\bm{\beta}+\eta\sum_{i''\thicksim i'}Z_{i''})}\right\}\right]'.
\end{eqnarray*}
For the centered model, the $\bm{\mathcal{I}}(\bm{\beta})$ and $\bm{\mathcal{J}}(\bm{\beta})$ are
\begin{eqnarray*}
&&\bm{\mathcal{I}}(\bm{\beta})=\sum_{i=1}^n \left[ \bm{x}_i-\eta \sum_{i' \thicksim i} \frac{\exp(\bm{x}_{i'}'\bm{\beta})}{\{1+\exp(\bm{x}_{i'}'\bm{\beta})\}^2}\bm{x}_{i'}\right]\left[ \bm{x}_i-\eta \sum_{i' \thicksim i} \frac{\exp(\bm{x}_{i'}'\bm{\beta})}{\{1+\exp(\bm{x}_{i'}'\bm{\beta})\}^2}\bm{x}_{i'}\right]'\\ 
&&\quad\quad\quad\quad\quad\quad\frac{\exp({\bm{x}_i'\bm{\beta}+\eta\sum_{i' \thicksim i}(Z_{i'}-\mu_{i'})})} {\{1+\exp(\bm{x}_i'\bm{\beta}+\eta\sum_{i' \thicksim i}(Z_{i'}-\mu_{i'}))\}^2}\\
&&+\sum_{i=1}^n \sum_{ i' \thicksim i} \eta \left\{Z_i-\frac{\exp({\bm{x}_i'\bm{\beta}+\eta\sum_{i'' \thicksim i}(Z_{i''}-\mu_{i''})})}{1+\exp(\bm{x}_i'\bm{\beta}+\eta\sum_{i'' \thicksim i}(Z_{i''}-\mu_{i''}))}\right\}\frac{\exp(\bm{x}_{i'}'\bm{\beta})-\{\exp(\bm{x}_{i'}'\bm{\beta})\}^2}{\{1+\exp(\bm{x}_{i'}'\bm{\beta})\}^3}\bm{x}_{i'}\bm{x}_{i'}'
\\
&&\bm{\mathcal{J}}(\bm{\beta})=\sum_{i=1}^n\sum_{i' \thicksim i, i'=i}\\
&&\left[ \left\{ \bm{x}_i-\eta \sum_{i' \thicksim i} \frac{\exp(\bm{x}_{i'}'\bm{\beta})}{(1+\exp(\bm{x}_{i'}'\bm{\beta}))^2}\bm{x}_{i'} \right\} \left\{Z_i-\frac{\exp({\bm{x}_i'\bm{\beta}+\eta\sum_{i' \thicksim i}(Z_{i'}-\mu_{i'})})}{1+\exp(\bm{x}_i'\bm{\beta}+\eta\sum_{i' \thicksim i}(Z_{i'}-\mu_{i'}))}\right\}\right]\\ 
&&\quad\quad\left[ \left\{ \bm{x}_{i'}-\eta \sum_{i'' \thicksim {i'}} \frac{\exp(\bm{x}_{i''}'\bm{\beta})}{(1+\exp(\bm{x}_{i''}'\bm{\beta}))^2}\bm{x}_{i''} \right\} \left\{Z_{i'}-\frac{\exp({\bm{x}_{i'}'\bm{\beta}+\eta\sum_{i'' \thicksim {i'}}(Z_{i''}-\mu_{i''})})}{1+\exp(\bm{x}_{i'}'\bm{\beta}+\eta\sum_{i'' \thicksim {i'}}(Z_{i''}-\mu_{i''}))}\right\}\right]'.
\end{eqnarray*}

With the alternative $\pm 1$ coding, the variance estimation can be obtained in analogy to (\ref{eqn:var}):
\begin{eqnarray}
\widehat{Var}(\widehat{\tilde{\bm{\beta}}}_{\rm p})\approx\bm{\mathcal{I}}(\widehat{\tilde{\bm{\beta}}}_{\rm p})^{-1}\bm{\mathcal{J}}(\widehat{\tilde{\bm{\beta}}}_{\rm p})\bm{\mathcal{I}}(\widehat{\tilde{\bm{\beta}}}_{\rm p})^{-1}.
\label{eqn:varCenter}
\end{eqnarray}  
For the uncentered model, the $\bm{\mathcal{I}}(\tilde{\bm{\beta}})$ and $\bm{\mathcal{J}}(\tilde{\bm{\beta}})$ are
\begin{eqnarray*}
\bm{\mathcal{I}}(\tilde{\bm{\beta}})&=&\sum_{i=1}^n \bm{x}_i\bm{x}_i' \left[1-\left\{\frac{ \sinh({\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i' \thicksim i}\tilde{Z}_{i'}})}{\cosh(\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i' \thicksim i}\tilde{Z}_{i'})}\right\}^2\right]\\
\bm{\mathcal{J}}(\tilde{\bm{\beta}})&=&\sum_{i=1}^n \sum_{i'\thicksim i, i'=i}\left[\bm{x}_i\left\{\tilde{Z}_i-\frac{\sinh({\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i' \thicksim i}\tilde{Z}_{i'}})}{\cosh(\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i' \thicksim i}\tilde{Z}_{i'})}\right\}\right]\\
&&\quad\quad\quad\quad\quad\quad\left[\bm{x}_{i'}\left\{\tilde{Z}_{i'}-\frac{\sinh({\bm{x}_{i'}'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i'' \thicksim i'}\tilde{Z}_{i''}})}{\cosh(\bm{x}_{i'}'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i'' \thicksim i'}\tilde{Z}_{i''})}\right\}\right]'.
\end{eqnarray*}
For the centered model, the $\bm{\mathcal{I}}(\tilde{\bm{\beta}})$ and $\bm{\mathcal{J}}(\tilde{\bm{\beta}})$ are
\begin{eqnarray*}
&&\bm{\mathcal{I}}(\tilde{\bm{\beta}})=\sum_{i=1}^n \left[ \bm{x}_i-\tilde{\eta} \sum_{i'\thicksim i} \frac{1}{\{\cosh(\bm{x}_{i'}'\tilde{\bm{\beta}})\}^2}\bm{x}_{i'}\right]  
\left[ \bm{x}_i-\tilde{\eta} \sum_{i'\thicksim i} \frac{1}{\{\cosh(\bm{x}_{i'}'\tilde{\bm{\beta}})\}^2}\bm{x}_{i'}\right]'\\
&&\quad\quad\cosh[\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i'\thicksim i}(\tilde{Z}_{i'}-\tilde{\mu}_{i'})]^{-2}\\ 
&&\quad\quad-\sum_{i=1}^n \sum_{i'\thicksim i}2 \tilde{\eta} \left\{\tilde{Z}_i-\frac{\sinh({\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i''\thicksim i}(\tilde{Z}_{i''}-\tilde{\mu}_{i''})})}{\cosh(\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i''\thicksim i}(\tilde{Z}_{i''}-\tilde{\mu}_{i''}))}\right\}
\frac{\sinh(\bm{x}_{i'}'\tilde{\bm{\beta}})}{\{\cosh(\bm{x}_{i'}'\tilde{\bm{\beta}}\}^3}\bm{x}_{i'}\bm{x}_{i'}'\\
&&\bm{\mathcal{J}}(\tilde{\bm{\beta}})= \\
&&\sum_{i=1}^n\sum_{i'\thicksim i, i'=i}\left[\left\{ \bm{x}_i-\tilde{\eta} \sum_{i'\thicksim i} (\cosh(\bm{x}_{i'}'\tilde{\bm{\beta}}))^{-2}\bm{x}_{i'} \right\} \left\{\tilde{Z}_i-\frac{\sinh({\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i' \thicksim i}(\tilde{Z}_{i'}-\tilde{\mu}_{i'})})}{\cosh(\bm{x}_i'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i'\thicksim i}(\tilde{Z}_{i'}-\tilde{\mu}_{i'}))}\right\}\right]\\ 
&&\quad\quad\left[\left\{\bm{x}_{i'}-\tilde{\eta} \sum_{i''\thicksim i'} (\cosh(\bm{x}_{i''}'\tilde{\bm{\beta}}))^{-2}\bm{x}_{i''}\right\} \left\{\tilde{Z}_{i'}-\frac{\sinh({\bm{x}_{i'}'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i''\thicksim i'}(\tilde{Z}_{i''}-\tilde{\mu}_{i''})})}{\cosh(\bm{x}_{i'}'\tilde{\bm{\beta}}+\tilde{\eta}\sum_{i''\thicksim i'}(\tilde{Z}_{i''}-\tilde{\mu}_{i''}))}\right\}\right]'.
\end{eqnarray*}

\section{Simulation Study}
\label{sec:simu}

\subsection{Simulation Set-up}

We conducted a simulation study to examine the finite-sample properties of the method developed in Sections~\ref{sec:model}--\ref{sec:infer}.  Consider an $m\times m$ square lattice, where $m=15$ or $30$, corresponding to sample sizes $n=225$ or $900$.  For spatial dependence, the neighborhood structure is of the first order and the autoregression coefficient $\eta$ is either 0.3 or 0.7, corresponding to weaker or stronger spatial dependence. 

Let $\bm{u}_j=(u_{j1},\ldots,u_{jn})'$ denote the $j$th covariate vector such that $\{u_{ji}:i=1,\ldots,n\}$ is a Gaussian random field with mean 0 and an exponential covariance function
\begin{eqnarray}
Cov(u_{ji}, u_{ji'})&=&\sigma^2 \exp(-|i-i'|/ \tau), 
\end{eqnarray}
where we let the variance parameter be $\sigma^2=1$ and the range parameter be $\tau=0.1$.  
To obtain cross-covariate correlation, let $\bm{u}_i=(u_{1i},\ldots, u_{pi})'$ and $\bm{x}_i=\bm{A}\bm{u}_i$ for site $i$, where $\bm{A}\bm{A}'=[\rho^{|j-j'|}]^{p}_{j,j'=1}$ and $\rho=0.4$.

Let $p=10$ be the number of covariates.  The regression coefficients are set to be $\bm{\beta}=(1,\beta_1,1,1,\bm{0}_7')'$, that is, 3 out of 10 coefficients are non-zero and the remaining 7 coefficients are zero.  For the 0-1 coding, we adopt the notion of an average large-scale structure as the average of $\mu_i$ over all sites and covariates \citep{carageak09}.  Let
\begin{eqnarray}
\bar{\mu}=\frac{1}{n}\sum_{i=1}^{n}\mu_i=\frac{1}{n}\sum_{i=1}^{n}\frac{\exp({\bm{x}_i'\bm{\beta}})}{1+\exp(\bm{x}_i'\bm{\beta})}
\label{eqn:averlarge1}
\end{eqnarray}
The large-scale structure is considered to be weak when $\bar{\mu}$ is around 0.5 and strong otherwise. For the $\pm 1$ coding, we define the average large-scale structure analogously as
\begin{eqnarray}
\bar{\tilde{\mu}}=\frac{1}{n}\sum_{i=1}^{n}\tilde{\mu}_i=\frac{1}{n}\sum_{i=1}^{n}\frac{\sinh({\bm{x}_i'\tilde{\bm{\beta}}})}{\cosh(\bm{x}_i'\tilde{\bm{\beta}})}
\label{eqn:averlarge2}
\end{eqnarray}
In this case, the large-scale structure is considered to be weak if $\bar{\tilde{\mu}}$ is close to 0 but strong otherwise.  Here, we let $\beta_1=1$ or 5, which corresponds to a stronger or weaker large-scale structure, respectively.  

In this simulation study, we restricted our attention to the 0-1 coding of the response variables.  For each combination of $m, \beta_1$ and $\eta$, $S=100$ samples were generated from the uncentered and centered model. In particular, the data were simulated by a Gibbs sampler using the full conditional probabilities in (\ref{eqn:uncp}) and (\ref{eqn:cp}) \citep{wasserman03}. Since the initial value for the one-step approximation algorithm may not be in a strictly convex area of the likelihood function, the matrix $ \bm{\mathcal{I}}(\cdot)$ is not always positive definite. This occurred only occasionally to only the centered model, in which case we modified the $\bm{\mathcal{I}}(\cdot)$ matrix by adding a positive diagonal matrix \citep{nocedalw00}.   
%We conducted a separate simulation to evaluate this and the results suggested that this modification works well.  

\subsection{Simulation Results}
% selection result   

Tables~\ref{tab:selstrong}--\ref{tab:selweak} provide the results of variable selection for sample size $n=225$ and $900$ in terms of the average numbers of correctly identified zero-valued and non-zero regression coefficients. The true number of non-zero and zero regression coefficients are 3 and 7, respectively.  
% n
When the sample size $n$ is larger, the number of correctly identified zero-valued (or the non-zero-valued) coefficients is closer to the truth. The number of correctly identified non-zero coefficients is closer to the truth compared with zero coefficients. 
% eta
When the sample size is smaller ($n=225$), the average number of correctly identified zero (and non-zero) regression coefficients
is largely closer to the truth when $\eta$ is smaller corresponding to weaker spatial dependence.  For the larger sample ($n=900$), the effect of $\eta$ is not as obvious.
% beta_1
For either sample size, there is no apparent effect of $\beta_1$ (i.e., large-scale structure) on the average number of correctly identified zero or non-zero regression coefficients.


% estimation result
Figures~\ref{fig:mple225}--\ref{fig:mple900} provide the results of parameter estimation for sample size $n=225$ and $900$ for the regression coefficients and the autoregression coefficient.  
% n
In general, for both uncentered and centered models, the bias and variance of the MPPLE of $\{\beta_j\}$ and $\eta$ are smaller as the sample size increases from 225 to 900,
% $\eta$
but are larger when the autoregression coefficient $\eta$ is larger.
%When $\beta_1$ is large, the large-scale structure is weak by (4.1)-(4.2).
Also, the bias and variance are larger when the large-scale structure is weaker.  This is plausible, because when the  large-scale structure effect is weaker, the  small-scale structure induced by the spatial dependence is relatively stronger.  
%For $\beta_1$ in particular, its bias is larger for larger magnitude.

% x_j, x_j' ~ ind
%Comparing to the independent simulation study in A, we can see that the bias of the MPLE of $\{\beta_j\ ,j=0,\dots ,7 \}$ and $\eta $ are generally larger in this dependent simulation study B. Since the data on lattice are correlated, it would have the same effect as the sample size is decreasing, as a result, the bias would be larger.

% standard deviation estimation result
For each simulated data set, we also computed the standard error of the  MPPLE of the nonzero coefficients $\{\beta_j: j=1,\ldots,4\}$ by taking the square root of the diagonal elements of the estimated variance matrix (\ref{eqn:var}). Figures \ref{fig:se225}--\ref{fig:se900} give the box plots of these standard errors in each of the $S=100$ simulated datasets, along with the nominal standard error of the MPPLE from this 100 simulations.
% n
The bias and variance of standard error of the  MPPLE of $\{\beta_j: j=0,\ldots, 4\}$ are smaller when the sample size is larger,
% $\eta$
but are larger when $\eta$ is larger.
% $\beta_1$
In particular, the bias and variance of the standard error of the  MPPLE of  ${\beta}_1$ are larger when $\beta_1$ is larger corresponding to a weaker large-scale structure.  Furthermore, the bias and variance of standard error of the intercept estimate $\widehat{\beta}_0$ tend to be larger than all other regression coefficients.

% x_j, x_j' ~ ind
% Comparing to the independent simulation study in A, we can see that the bias of the standard error of $\{\beta_j\ ,j=1,\dots ,4 \}$ are generally larger in this dependent simulation study B. Since the data on lattice are correlated, it would have the same effect as the sample size is decreasing, as a result, the bias would be larger.

\section{Data Example}
\label{sec:data}

In this section, we illustrate our method by a real data example in landscape ecology and environmental history.  The study is aimed at assessing the influence of past land ownership characteristics on land cover structure in northern Wisconsin, USA. Landscape ecologists are interested in the effect of land ownership because it offers a spatially specific, quantifiable approach to assess the effect of important, yet often geographically amorphous social, economic, and historical factors on landscape structure \citep{turnerw96}.  

Data were derived from historical plat maps and the Wisconsin Land Economic Inventory, a land survey. The spatial unit of analysis is a quarter section (or, 1/36 township, 160 acres $\approx$ 65 ha) and there are 1429 units in the study area. We focus on a binary response variable indicating whether the land cover in a quarter section  is aspen (\textit{Populus} spp.)-paper birch (\textit{Betula papryfera}) (APB), an early successional forest class, or not (Figure~\ref{fig:data}). 

Forest composition can be associated with several land ownership characteristics, including ownership class, parcel size, and ownership size \citep{crowh99, stanfieldb02}. The specific covariates of interest are: reservation, number of parcels, average size of parcels within a quarter section, proportion of the largest parcel, total area, and average size of all parcels in (but not necessarily all contained within) a quarter section.  More specifically, reservation ({\tt Reserv}) indicates whether the quarter section is on an Indian reservation or not.  The number of parcels ({\tt PolyNm}) in a given quarter section is a measure of parcel density.  Average size ({\tt PolyPr}) is the average size of parcel polygon, which is a measure of parcel size.  Proportion of the largest parcel ({\tt MxPolyPr}) reports the largest parcel polygon as a proportion of quarter section. Total area ({\tt TotOwn}) shows the total property area (measured in acres) in this study area associated with owner of the largest parcel polygon in a given quarter section.  Because the values of this covariate are skewed to the right, we use a log transformation.  Average size of all parcels ({\tt AvParcel}) is the average size of all parcels that lay in whole or in part within a given quarter section (measured in acres). All the covariates except reservation are standardized to have mean 0 and variance 1 before being used in the regression procedure. 

We considered the four combinations of uncentered or centered models and the 0-1 or $\pm 1$ coding of APB.  For autologistic regression, a first-order neighborhood structure is assumed as before.  The MPPLE of the regression coefficients are given in Table~\ref{tab:data}, as well as the standard errors for those MPPLE that are non-zero.  For the different models and codings, the variables selected largely agree.  {\tt Reserv} and {\tt TotOwn} were selected for all four combinations, whereas the other covariates were not selected except {\tt MxPolyPr} once in the uncentered model with 0-1 coding.  The signs of all the nonzero regression coefficients also agree and for this data set, the signs are all positive.  That is, there is a greater likelihood of APB on sites that are reservation and that contain a parcel held by a large owner in the study area.  The autoregression coefficient $\eta$ is consistently estimated to be positive, indicating a positive spatial dependence.  The magnitude of the estimates is about four times as large for the 0-1 coding than the $\pm 1$ coding.  This is perhaps expected, as given in Appendix, the exact relation is $\tilde{\eta}={\eta}/4$. 

We see that reservation ({\tt Reserv}) is a significant predictor of APB. That is, whether a site lies on the Indian reservation or not influences the likelihood that APB is the dominant land cover. This is plausible because sites on the reservation lay within a single administrative unit (the Indian Agency, later the Bureau of Indian Affairs), which historically has had authority over forest management decisions, regardless of ownership type. Indian Agency authority was especially pervasive in the early 20th century (for historical political reasons), which corresponds with the time period represented by the land cover data. Moreover, the autologistic regression model fitting finds that ownership size ({\tt TotOwn}) is a significant predictor of APB presence vs. absence. In this particular ecoregion, the land cover class of interest, APB, is strongly associated with a specific forest management practice, i.e., short-rotation, even-aged management. Large landowners are most likely to implement this practice: forest stands on some parcels could be harvested, while those on other parcels could be held for harvest. By distributing harvest rotations across a large land area, forest products could be continuously harvested. This management logic typically assumes large real estate holding, and functions less optimally on smaller ownerships. 
%Finally, autologistic model fitting finds some spatial dependence in the centered and uncentered models with the +/-1 coding; since n < 1,  we conclude that inference is valid. The map of the response variable (figure 5) reveals some clustering in the non-APB outcome (other (OTH)), as well as the aspen-paper birch outcome. Agriculture land use accounts for most of the OTH land class, as described elsewhere (Steen-Adams et al. 2011). For economic and social reasons, the establishment and survival of a farming operation was more likely when it neighbored nearby farms, than otherwise. Likewise, the economic success of short-rotation forest management, and therefore APB presence, was most likely when located near similarly managed sites.

Comparing the magnitude of the estimated nonzero regression coefficients in an uncentered model with the centered model, there are differences but no obvious pattern. However, an additional variable {\tt MxPolyPr} was selected for the 0-1 coding in the uncentered model and the magnitudes of the autoregression coefficient estimates $\widehat\eta$ are comparable.  We further estimated the average large-scale structure by plugging the parameter estimates from the logistic regression into (\ref{eqn:averlarge1}) and (\ref{eqn:averlarge2}). The estimated large-scale structure is 0.65 for the 0-1 coding and 0.14 for the $\pm 1$ coding, which indicate a somewhat strong large-scale structure when compared with 0.5 and 0, respectively.  Based on the simulation results in Section~\ref{sec:simu}, therefore, we may conclude that the results using the centered model are more reliable in this data example.



\section{Conclusions and Discussion}
\label{sec:disc}

In this paper, we have proposed maximum penalized pseudolikelihood estimation for simultaneous selection of covariates and estimation of parameters. Our simulation study has shown desirable large-sample properties in the sense that the performance of variable selection and parameter estimation improves as the sample size increases.  In addition, a variance estimation based on the limiting distribution of the parameter estimates appears to work reasonably well.  We have also illustrated our method by a data example in the intersection of landscape ecology and environmental history.  
For future research, it would be interesting to adopt some of the innovations in computation such as Monte Carlo maximum likelihood and reduced dependence approximation.

We may further consider spatial-temporal autologistic models as an extension of the spatial-only model above. For site $i=1,\ldots,n$ and time $t=1,\ldots,T$, let $Z_{i,t}=0$ or 1 denote the response variable and define the set of indices of spatial-temporal neighbors as
\begin{eqnarray}
\mathcal{N}_{i,t}=\{(i',t-l):i'\thicksim i,\ l=0,\ldots,L\}\cup\{(i,t-l): l=1,\ldots,L\}
\end{eqnarray}
where recall that $i'\thicksim i$ denotes that $i'\in\mathcal{N}_i$ by a pre-specified spatial neighborhood and $L$ denotes the maximum time lag in the temporal neighborhood.  That is, the spatial-temporal neighborhood contains the spatial neighbors of site $i$ at times $t, t-1, \ldots, t-L$ and site $i$ itself in the past $t-1, \ldots, t-L$.  For a given time point $t$, let $\bm{Z}_t=(Z_{1,t},\ldots, Z_{n,t})'$ denote the binary response variables at all $n$ sites.  

To model the response variables $\{Z_{i,t}: i=1,\dots,n,\ t=1,\dots,T\}$, we first assume that conditional on $\{\bm{Z}_{t'}: t'<t\}$, the probability of $\bm{Z}_t$ depends on only the most recent $L$ time points:
\begin{eqnarray}
p(\bm{Z}_t|\bm{Z}_{t'}: t'<t) &=& p(\bm{Z}_t|\bm{Z}_{t'}: t'=t-1,\ldots,t-L).
\label{eqn:stMarkov}
\end{eqnarray}
For time $t$, further assume a Markov random field such that
\begin{eqnarray}
p(Z_{i,t}|Z_{i',t}: i'\neq i; \bm{Z}_{t'}: t'=t-1,\ldots,t-L) &=& p(Z_{i,t}|Z_{i',t'}: (i',t')\in\mathcal{N}_{i,t}).
\label{eqn:stMRF}
\end{eqnarray}
The conditional distribution $p(Z_{i,t}|Z_{i',t'}: (i',t')\in\mathcal{N}_{i,t})$ is Bernoulli with success probability $\pi_{i,t}=p(Z_{i,t}=1|Z_{i',t'}: (i',t')\in\mathcal{N}_{i,t})$.  

The logit function of the success probability can be defined analogously to the spatial-only autologistic regression model.  For an uncentered model, let
\begin{eqnarray}
{\rm logit}(\pi_{i,t}) = \log\frac{\pi_{i,t}}{1-\pi_{i,t}} &=&  \bm{x}_{i,t}'\bm{\beta}+\sum_{(i',t')\in\mathcal{N}_{i,t}}\eta_{i,i',t,t'}Z_{i',t'}
\label{eqn:stlogit}
\end{eqnarray}
for a centered model, let
\begin{eqnarray}
{\rm logit}(\pi_{i,t}) = \log\frac{\pi_{i,t}}{1-\pi_{i,t}} &=&  \bm{x}_{i,t}'\bm{\beta}+\sum_{(i',t')\in\mathcal{N}_{i,t}}\eta_{i,i',t,t'}(Z_{i',t'}-\mu_{i',t'})
\label{eqn:stlogitCenter}
\end{eqnarray}
where $\bm{x}_{i,t}$ denotes the covariate vector at site $i$ and time $t$ and $\mu_{i,t}=\exp(\bm{x}_{i,t}'\bm{\beta})/\{1+\exp(\bm{x}_{i,t}'\bm{\beta})\}$ is the expectation of $Z_{i,t}$ assuming independence.

Let $\bm{\theta}=(\bm{\beta}',\eta_{i,i',t,t'})'$ denote the vector of the regression coefficients $\bm{\beta}$ and autoregression coefficients $\{\eta_{i,i',t,t'}\}$. The log-pseudolikelihood function for the uncentered model is 
\begin{eqnarray}
\ell_{\rm p(st)}^{\rm u}(\bm{\theta}|\bm{Z})&=&\sum_{t=L+1}^T\sum_{i=1}^n\log\frac{\exp\{Z_{i,t}(\bm{x}_{i,t}'\bm{\beta}+\sum_{(i',t')\thicksim(i,t)}\eta_{i,i',t,t'} Z_{i',t'})\}}{1+\exp(\bm{x}_{i,t}'\bm{\beta}+\sum_{(i',t')\thicksim(i,t)}\eta_{i,i',t,t'})}
\end{eqnarray}
and for the centered model is
\begin{eqnarray}
\ell_{\rm p(st)}^{\rm c}(\bm{\theta}|\bm{Z})&=&\sum_{t=L+1}^T\sum_{i=1}^n\log\frac{\exp[Z_{i,t}\{ \bm{x}_{i,t}'\bm{\beta}+\sum_{(i',t')\thicksim(i,t)}\eta_{i,i',t,t'}(Z_{i',t'}-\mu_{i',t'})\}]}{1+\exp\{\bm{x}_{i,t}'\bm{\beta}
+\sum_{(i',t')\thicksim(i,t)}\eta_{i,i',t,t'}(Z_{i',t'}-\mu_{i',t'})\}}
\end{eqnarray}

For simultaneous variable selection and parameter estimation, a penalized log-pseudolikelihood function similar to (\ref{eqn:ppl}) can be defined:
\begin{eqnarray}
\ell_{\rm pp(st)}(\bm{\theta})= \ell_{\rm p(st)}(\bm{\theta})-N\sum_{j=1}^{p}\lambda_j|\beta_j|
\label{eqn:pplt}
\end{eqnarray}
where $N=n(T-L)$.  Maximization of (\ref{eqn:pplt}) can be attained following the computation algorithm in Section~\ref{sec:infer} and similarly, the variance estimation of the MPPLE can be derived.  In both cases, fast computation is attainable given the computational efficiency of adaptive Lasso and relatively low dimension of the variance matrix.

\section*{Appendix: The relationship between 0-1 coding with  $\pm 1$ coding}

For an uncentered model, we have
      \begin{eqnarray*}
 p(Z_i|Z_{i'}: i'\thicksim i)&=&\frac{\exp\{ {Z_i(\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}Z_{i'}}) \} }{1+\exp(\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}Z_{i'})}
\\
&=& \frac{\exp [ { \{(\tilde{Z}_i+1)/2 \} \{\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}(\tilde{Z}_{i'}+1)/2} \}] }{1+\exp\{ \bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}(\tilde{Z}_{i'}+1)/2\}}
\\
% &=& \frac{\exp{\{(\tilde{Z}_i/2) \bm{x}_i'\bm{\beta}+(|{\cal N}_i|/4)\eta \tilde{Z}_i+(|{\cal N}_i|/4)\tilde{Z}_i\sum_{j \thicksim i}\tilde{Z}_j\}}}  %{2\cosh\{\bm{x}_i'\bm{\beta}/2+\eta\sum_{j \thicksim i}(\tilde{Z}_j+1)/4\}} &&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
\\
&=&\frac{\exp[\tilde{Z}_i \{(\bm{x_i'}{\bm\beta}/2+|{\cal N}_i|\eta/4)+(\eta/4 )\sum_{i'\thicksim i}\tilde{Z}_{i'}\}]}  {2\cosh\{(\bm{x_i'}{\bm\beta}/2+|{\cal N}_i|\eta/4)+(\eta/4 )\sum_{i'\thicksim i}\tilde{Z}_{i'}\}}
%=\frac{\exp\{ {\tilde{Z}_i(\bm{x}_i'\bm{\tilde{\beta}}+\tilde{\eta}\sum_{j \thicksim i}\tilde{Z}_j})\}}{2\cosh(\bm{x}_i'\bm{\tilde{\beta}}+\tilde{\eta}\sum_{j %\thicksim i}\tilde{Z}_j)}
\end{eqnarray*}
Thus, $\tilde{\eta}=\eta/4$ and $\bm{x}_i'\tilde{\bm{\beta}}=\bm{x}_i'\bm{\beta}/2+|{\cal N}_i|\eta/4$.

Now, for a centered model, we have
      \begin{eqnarray*}
 p(Z_i|Z_{i'}: i'\thicksim i)&=& \frac{\exp[ Z_i\{ \bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}(Z_{i'}-\mu_{i'}) \} ] }{1+\exp\{\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}(Z_{i'}-\mu_{i'}) \}} 
\\
&=& \frac{\exp [ { \{(\tilde{Z}_i+1)/2 \} \{\bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}(\tilde{Z}_{i'}/2+1/2-\mu_{i'})} \}] }{1+\exp\{ \bm{x}_i'\bm{\beta}+\eta\sum_{i'\thicksim i}(\tilde{Z}_{i'}/2+1/2-\mu_{i'})\}}
\\
&=&\frac{\exp[\tilde{Z}_i \{\bm{x_i'}{\bm\beta}/2+|{\cal N}_i|\eta/4-\eta\sum_{i'\thicksim i}\mu_{i'}/2+\eta\sum_{i'\thicksim i}\tilde{\mu}_{i'}/4+(\eta/4 )\sum_{i'\thicksim i}(\tilde{Z}_{i'}-\tilde{\mu}_{i'})\}]}{2\cosh\{\bm{x_i'}{\bm\beta}/2+|{\cal N}_i|\eta/4-\eta\sum_{i'\thicksim i}\mu_{i'}/2+\eta\sum_{i'\thicksim i}\tilde{\mu}_{i'}/4+(\eta/4 )\sum_{i'\thicksim i}(\tilde{Z}_{i'}-\tilde{\mu}_{i'}) \}} 
%= \frac{\exp [ { (\tilde{Z}_i/2)\{\bm{x}_i'\bm{\beta}+\eta\sum_{j \thicksim i}(\tilde{Z}_j/2+1/2-\mu_j)} \}] }{2\cosh\{ \bm{x}_i'\bm{\beta}/2+\eta\sum_{j %\thicksim i}(\tilde{Z}_j/2+1/2-\mu_j)/2 \}} 
%\\
%=\frac{\exp[\tilde{Z}_i \{\bm{x_i'}{\bm\beta}/2+|{\cal N}_i|\eta/4-|{\cal N}_i|\eta\mu_j/2+(\eta/4 )\sum_{j \thicksim i}\tilde{Z}_j \}]}  {2\cosh\{\bm{x_i'}%{\bm\beta}/2+|{\cal N}_i|\eta/4-|{\cal N}_i|\eta\mu_j/2+(\eta/4 )\sum_{j \thicksim i}\tilde{Z}_j \}} 
%\\
%=\frac{\exp[ {\tilde{Z}_i\{\bm{x}_i'\bm{\tilde{\beta}}+\tilde{\eta}\sum_{j \thicksim i}(\tilde{Z}_j-\tilde{\mu}_j)}\}]}{2\cosh\{\bm{x}_i'\bm{\tilde{\beta}}+\tilde{\eta}\sum_{j \thicksim i}(\tilde{Z}_j-\tilde{\mu}_j)\}}
 \end{eqnarray*}
Thus, $\tilde{\eta}=\eta/4$ and $\bm{x}_i'\tilde{\bm{\beta}}=\bm{x_i'}{\bm\beta}/2+|{\cal N}_i|\eta/4-\eta\sum_{i'\thicksim i}\mu_{i'}/2+\eta\sum_{i'\thicksim i}\tilde{\mu}_{i'}/4$

%\section*{Appendix B: Modified information matrix}

%If the information matrix $ \mathcal{I}(\bm{\beta})$ is not positive definite, the modified information matrix will be  $\mathcal{I}(\bm{\beta})+\tau I$, where $\tau>0$. Follow the algorithm 3.3 in Jorge Nocedal (2010), we obtain a successively large value of $\tau$ such that $\mathcal{I}(\bm{\beta})+\tau I$ is positive definite. \\
%A simulation study is performed to verify the consistence and effectiveness . We consider $m \times m$ square lattices, where m=15, and sample sizes is 225. The neighbor structure is the first-order. For regression, we have seven covariates following a standard normal distribution, and the regression coefficients are set to be $\bm{\beta}=(1,1,1,1,0,0,0)$, and  $\eta$ is 0.3. Data sets of size 100 is generated from uncentered and centered model,and we use uncentered and centered fitting method to each data set using the original information matrix. Then i select the data sets producing a non-positive definite information matrix, and we modify those information matrix to obtain some new estimates. The result are showed in Table2, in terms of misclassification rate and box plot of bias of using original information matrix and modified ones respectively. \\

%Generally speaking, the misclassification rate from modified cases is small than the original cases, in particular, the mis xxx rate of modified cases is ten times smaller than the original one. And the box plot of bias shows that the estimates with modified information matrix is consistent. 

\section*{Acknowledgments}

Funding has been provided for this research from a USDA Cooperative State Research, Education and Extension Service (CSREES) Hatch project.
The authors thank Professors~David J. Mladenoff and Nancy Langston for guidance and support to develop the northern Wisconsin land cover study.  A National Science Foundation IGERT Grant 9870703 (Human Dimensions of Social and Aquatic Systems Interactions) contributed to database development.

\begin{thebibliography}{00}

\bibitem[Banerjee~\textit{et al.}(2003)]{banerjeec03} Banerjee, S., Carlin, B.P., and Gelfand, A.E. (2003)  \textit{Hierarchical Modeling and Analysis for Spatial Data}. Chapman and Hall: Boca Raton.

\bibitem[Besag(1972)]{besag72} Besag, J. (1972) Nearest-neighbour systems and the auto-logistic model for binary data. \textit{Journal of the Royal Statistical Society Series B}, \textbf{34}, 75--83.

\bibitem[Besag(1974)]{besag74} Besag, J. (1974) Spatial interaction and the statistical analysis of lattice systems (with discussion). \textit{Journal of the Royal Statistical Society Series B}, \textbf{36}, 192--236.

\bibitem[Caragea and Kaiser(2009)]{carageak09} Caragea, P.C. and Kaiser, M.S. (2009) Autologistic models with interpretable parameters. \textit{Journal of Agricultural, Biological, and Environmental Statistics}, \textbf{14}, 281--300.

\bibitem[Cressie(1993)]{cressie93} Cressie, N. (1993)  \textit{Statistics for Spatial Data, Rev Ed}. Wiley: New York.

\bibitem[Crow~\textit{et al.}(1999)]{crowh99} Crow, T.R., Host, G.E., and Mladenoff, D.J. (1999)  Ownership and ecosystem as sources of spatial heterogeneity in a forested landscape, Wisconsin USA.  \textit{Landscape Ecology}, \textbf{14}, 449--463. 

\bibitem[Diggle and Ribeiro(2007)]{diggler07} Diggle, P.J. and Ribeiro, P.J. (2007) \textit{Model-based Geostatistics}. New York: Springer.

\bibitem[Efron~\textit{et al.}(2004)]{efronh04} Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004) Least angle regression (with discussion). \textit{Annals of Statistics}, \textbf{32}, 407--499.

%\bibitem[Fan and Li(2001)]{fanl01} Fan, J. and Li, R. (2001) Variable Selection via nonconcave penalized likelihood and its oracle properties. \textit{Journal of the American Statistical Association}, \textbf{96}, 1348--1360.

%\bibitem[Fan and Li(2004)]{fanl04} Fan, J. and Li, R. (2004) New estimation and model selection procedures for semiparametric modeling in longitudinal data analysis. \textit{Journal of the American Statistical Association}, \textbf{99}, 710--723.

\bibitem[Friel~\textit{et al.}(2009)]{frielp09} Friel, N., Pettitt, A.N., Reeves, R. and Wit, E. (2009) Bayesian inference in hidden {M}arkov random fields for binary data defined on large lattices. \textit{Journal of Computational and Graphical Statistics}, \textbf{18}, 243--261.

\bibitem[Gaetan and Guyon(2010)]{gaetang10} Gaetan, C. and Guyon, X. (2010) \textit{Spatial Statistics and Modeling}. New York: Springer.

\bibitem[Geyer(1994)]{geyer94} Geyer, C. J. (1994) On the convergence of {M}onte {C}arlo maximum likelihood calculations. \textit{Journal of the Royal Society of Statistics Series B}, \textbf{56}, 261--274.

\bibitem[Gumpertz~\textit{et al.}(1997)]{gumpertzg97} Gumpertz, M.L., Graham, J.M., and Ristaino, J.B. (1997) Autologistic model of spatial pattern of \textit{Phytophthora} epidemic in bell pepper: {E}ffects of soil variables on disease presence. \textit{Journal of Agricultural, Biological, and Environmental Statistics}, \textbf{2}, 131--156.


\bibitem[He~\textit{et al.}(2003)]{hez03} He, F. , Zhou, J., and Zhu, H. (2003) Autologistic regression model for the distribution of vegetation. \textit{Journal of Agricultural, Biological and Environmental Statistics}, \textbf{8}, 205--222.

\bibitem[Huang~\textit{et al.}(2010)]{huangh10} Huang, H.-C., Hsu, N.-J., Theobald, D.M., and Breidt, F.J. (2010) Spatial {LASSO} with applications to {GIS} model selection. \textit{Journal of Computational and Graphical Statistics}, \textbf{19}, 963--983.

\bibitem[Huffer and Wu(1998)]{hufferw98} Huffer, F.W. and Wu, H. (1998) Markov chain {M}onte {C}arlo for autologistic regression models with application to the distribution of plant species. \textit{Biometrics}, \textbf{54}, 509--524.

\bibitem[Hughes~\textit{et al.}(2011)]{hughesh11} Hughes, J., Haran, M., and Caragea, P.C. (2011) Autologistic models for binary data on a lattice. \textit{Environmetrics}, \textbf{22},  857--871.

\bibitem[Moeller~\textit{et al.}(2006)]{mollerp06} Moeller, J., Pettitt, A.N., Reeves, R., and Berthelsen, K.K. (2006) An efficient {M}arkov chain {M}onte {C}arlo method for distributions with intractable normalising constants. \textit{Biometrika}, \textbf{93}, 451--458.

\bibitem[Nocedal and Wright(2000)]{nocedalw00} Nocedal, J. and Wright, S.J. (2000) \textit{Numerical Optimization, 2nd Ed}. New York: Springer.

%\bibitem[Pettitt~\textit{et al.}(2003)]{pettittfr03} Pettitt, A.N.,  Friel, N., and Reeves, R. (2003) Efficient calculation of the normalizing constant of the autologistic and related models on the cylinder and lattice. \textit{Journal of the Royal Statistical Society Series B}, \textbf{65}, 235--246.

\bibitem[Stanfield~\textit{et al.}(2002)]{stanfieldb02} Stanfield, B.J., Bliss, J.C. and Spies, T.A. (2002) Land ownership and landscape structure: {A} spatial analysis of sixty-six Oregon (USA) Coast Range watersheds. \textit{Landscape Ecology}, \text{17}, 685--697. 

\bibitem[Sun and Clayton(2008)]{sunc08} Sun, L. and Clayton, M.K. (2008) Bayesian analysis of cross-classified spatial data with autocorrelation. \textit{Biometrics}, \textbf{64}, 74--84.

\bibitem[Tibshirani(1996)]{tibshirani96} Tibshirani, R. (1996) Regression shrinkage and selection via the {L}asso. \textit{Journal of the Royal Statistical Society Series B}, \textbf{58}, 267--288.

\bibitem[Turner~\textit{et al.}(1996)]{turnerw96} Turner, M.G., Wear, D.N., and Flamm, R.O.  (1996) Land ownership and land-over change in the {S}outhern {A}ppalachian {H}ighlands and the {O}lympic {P}eninsula. \textit{Ecological Applications},  \textbf{6}, 1150--1172. 

\bibitem[Wasserman(2003)]{wasserman03} Wasserman, L. (2003) \textit{All of Statistics: A Concise Course in Statistical Inference}. New York: Springer.

\bibitem[Wang and Zheng(2012)]{wangz12} Wang, Z. and Zheng, Y. (2012) Analysis of binary data via a centered spatial-temporal autologistic regression model. \textit{Environmental and Ecological Statistics} (In press).

\bibitem[Zheng and Zhu(2008)]{zhengz08} Zheng Y. and Zhu J. (2008) Markov chain {M}onte {C}arlo for spatial-temporal autologistic regression model. \textit{Journal of Computational and Graphical Statistics}, \textbf{17}, 123--127.

\bibitem[Zhu~\textit{et al.}(2010)]{zhuh10} Zhu, J., Huang, H.-C., and Reyes, P.E. (2010) On selection of spatial linear models for lattice data. \textit{Journal of Royal Statistics Society Series B}, \textbf{72}, 389--402.

\bibitem[Zhu~\textit{et al.}(2005)]{zhuh05} Zhu, J., Huang, H.-C., and Wu, J.-P. (2005) Modeling spatial-temporal binary data using {M}arkov random fields. \textit{Journal of Agricultural, Biological, and Environmental Statistics}, \textbf{10}, 212--225.

\bibitem[Zhu~\textit{et al.}(2008)]{zhuz08} Zhu, J., Zheng, Y., Carroll, A.L., and Aukema, B.H. (2008) Autologistic regression analysis of spatial-temporal binary data via {M}onte {C}arlo maximum likelihood. \textit{Journal of Agricultural, Biological, and Environmental Statistics}, \textbf{13}, 84--98.

\bibitem[Zhu and Liu(2009)]{zhul09} Zhu, Z. and Liu, Y. (2009) Estimating spatial covariance using penalized likelihood with weighted ${L}_1$ penalty. \textit{Journal of Nonparametric Statistics}, \textbf{21}, 925--942.

\bibitem[Zou(2006)]{zou06} Zou, H. (2006) The adaptive {LASSO} and its oracle properties. \textit{Journal of the American Statistical Association}, \textbf{101}, 1418--1429.

\end{thebibliography}




\newpage
%
% Tables
%
\begin{table}[!htbp]
\caption{Average number of correctly identified non-zero and zero regression coefficients when $\beta_1=5$ (weak large-scale structure) for uncentered and centered model, sample size $n=225$ or 900, and antoregression coefficient $\eta=0.3$ or 0.7.} % title of Table
\centering  % used for centering table
\begin{tabular}{rccccc} % centered columns
\hline\hline                        %inserts double horizontal lines
& $\{\beta_j\}$ & \multicolumn{2}{c}{Number of non-zero estimates}  & \multicolumn{2}{c}{Number of zero estimates}\\ [0.5ex]  % inserts table 
%heading
\multicolumn{1}{c}{Model} & $n$ & $\eta=0.3$ & $\eta=0.7$ & $\eta=0.3$ & $\eta=0.7$ \\
\hline                  % inserts single horizontal line

Uncentered &  225  & 2.77 & 2.69 &  6.12 & 6.14 \\
 &  900  & 3.00 & 3.00 & 6.81 & 6.88\\
Centered &  225  & 2.75 & 2.67& 6.21  & 6.17\\ 
  &  900 & 3.00 & 3.00 & 6.87 & 6.82\\ [1ex]      % [1ex] adds vertical space
\hline %inserts single line
\end{tabular}
\label{tab:selstrong} % is used to refer this table in the text
\end{table}
 
\begin{table}[!htbp]
\caption{Average number of correctly identified non-zero and zero regression coefficients when $\beta_1=1$ (strong large-scale structure) for uncentered and centered model, sample size $n=225$ or 900, and antoregression coefficient $\eta=0.3$ or 0.7.} % title of Table
\centering  % used for centering table
\begin{tabular}{rccccc} % centered columns
\hline\hline                        %inserts double horizontal lines
& $\{\beta_j\}$ & \multicolumn{2}{c}{Number of non-zero estimates}  & \multicolumn{2}{c}{Number of zero estimates}\\ [0.5ex]  % inserts table 
%heading
\multicolumn{1}{c}{Model} & $n$ & $\eta=0.3$ & $\eta=0.7$ & $\eta=0.3$ & $\eta=0.7$ \\
\hline                  % inserts single horizontal line
Uncentered &  225  & 2.94 & 2.52 & 6.09 & 6.12\\
 &  900  & 3.00 & 3.00 & 6.88 & 6.95\\
Centered  &  225 & 2.92 & 2.90 & 6.31 & 6.16\\ 
  &  900 & 3.00 & 3.00 & 6.87 & 6.92\\ [1ex]      % [1ex] adds vertical space
\hline %inserts single line
\end{tabular}
\label{tab:selweak} % is used to refer this table in the text
\end{table}

\begin{table}[!htbp]
\caption{Maximum penalized pseudolikelihood estimates (MPPLE) of the regression coefficients and the autoregression coefficient $\eta$ for uncentered or centered models and 0-1 or $\pm 1$ coding of the response variable. The standard errors of MPPLE of the regression coefficients are given in the parentheses.} % title of Table
\centering  % used for centering table
\begin{tabular}{rcccc} % centered columns
\hline\hline                        %inserts double horizontal lines
 &  \multicolumn{2}{c}{0-1 coding} & \multicolumn{2}{c}{$\pm$1 coding}\\
\multicolumn{1}{c}{Covariates} & Uncentered & Centered & Uncentered & Centered \\ [0.5ex]  % inserts table 
%heading
\hline                  % inserts single horizontal line
%Intercept  &  $-2.368$ (0.063) &  0.416 (0.281)&  $-0.116$ (0.016) &  0.208 (0.315)  \\
{\tt Reserv}        &  0.728 (0.132) & 1.889 (0.302)&  0.230 (0.042) & 0.944 ( 0.072)\\ 
{\tt PolyNm}        &  -- & --& -- & --\\
{\tt PolyPr}        &  -- & --& -- & --\\
{\tt MxPolyPr}       &  0.114 (0.088) & --& -- & --\\
$\log$({\tt TotOwn})        &  0.420 (0.095) & 0.178 (0.068)&  0.174 (0.023) & 0.090 (0.025)\\
{\tt AvParcl}        &  -- & --&  -- & --\\
$\eta$        &  1.217 & 1.302 & 0.331 & 0.325\\\hline %[1ex]   % [1ex] adds vertical space
\end{tabular}
\label{tab:data} % is used to refer this table in the text
\end{table}

%
% Figures
%
\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.6]{./Figures/MPLE225.pdf}
%\includegraphics[scale=0.6]{MPLE225.pdf}
\end{center}
\caption{Box plot of the MPPLE $\widehat{\beta}_0$ (row 1), $\widehat{\beta}_1$ (row 2), $\widehat{\beta}_2$ (row 3) and $\widehat{\eta}$ (row 4) from the 100 simulations with sample size $n=225$. Column (a): uncentered model with $\beta_1=5$ (weak large-scale structure); (b): uncentered model with $\beta_1=1$ (strong large-scale structure); (c): centered model with $\beta_1=5$ (weak large-scale structure); (d): centered model with $\beta_1=1$ (strong large-scale structure). The true values are $\beta_0=1$, $\beta_1=1$ or 5, $\beta_2=1$, $\beta_3=1$, and $\eta=$ 0.3 or 0.7. The box plot of $\widehat{\beta}_3$ is similar to $\widehat{\beta}_2$ but omitted to save space.}
\label{fig:mple225}
\end{figure}

\begin{figure}[!htbp]
\begin{center} 
\includegraphics[scale=0.6]{./Figures/MPLE900.pdf}
%\includegraphics[scale=0.6]{MPLE900.pdf}
\end{center}
\caption{Box plot of the MPPLE $\widehat{\beta}_0$ (row 1), $\widehat{\beta}_1$ (row 2), $\widehat{\beta}_2$ (row 3) and $\widehat{\eta}$ (row 4) from the 100 simulations with sample size $n=900$. Column (a): uncentered model with $\beta_1=5$ (weak large-scale structure); (b): uncentered model with $\beta_1=1$ (strong large-scale structure); (c): centered model with $\beta_1=5$ (weak large-scale structure); (d): centered model with $\beta_1=1$ (strong large-scale structure). The true values are $\beta_0=1$, $\beta_1=1$ or 5, $\beta_2=1$, $\beta_3=1$, and $\eta=$ 0.3 or 0.7. The box plot of $\widehat{\beta}_3$ is similar to $\widehat{\beta}_2$ but omitted to save space.}
\label{fig:mple900}
\end{figure}

\begin{figure}[!htbp]
\begin{center}
%\includegraphics[angle=-90,width=6in]{Figures/fig.1.eps}
\includegraphics[scale=0.6]{./Figures/SE225.pdf}
%\includegraphics[scale=0.6]{SE225.pdf}
\end{center}
\caption{Box plot of the standard error of $\widehat{\beta}_0$ (row 1), $\widehat{\beta}_1$ (row 2), $\widehat{\beta}_2$ (row 3) and $\widehat{\beta}_3$ (row 4) from the 100 simulations with sample size $n=225$.  Column (a): uncentered model with $\beta_1=5$ (weak large-scale structure); (b): uncentered model with $\beta_1=1$ (strong large-scale structure); (c): centered model with $\beta_1=5$ (weak large-scale structure); (d): centered model with $\beta_1=1$ (strong large-scale structure). The nominal standard error of the MPPLE from the 100 simulations are given along the $y$-axis on the left.}
\label{fig:se225}
\end{figure}

\begin{figure}[!htbp]
\begin{center}
%\includegraphics[angle=-90,width=6in]{Figures/fig.1.eps}
\includegraphics[scale=0.6]{./Figures/SE900.pdf}
%\includegraphics[scale=0.6]{SE900.pdf}
\end{center}
\caption{Box plot of the standard error of $\widehat{\beta}_0$ (row 1), $\widehat{\beta}_1$ (row 2), $\widehat{\beta}_2$ (row 3) and $\widehat{\beta}_3$ (row 4) from the 100 simulations with sample size $n=900$. Column (a): uncentered model with $\beta_1=5$ (weak large-scale structure); (b): uncentered model with $\beta_1=1$ (strong large-scale structure); (c): centered model with $\beta_1=5$ (weak large-scale structure); (d): centered model with $\beta_1=1$ (strong large-scale structure). The nominal standard error of the MPPLE from the 100 simulations are given along the $y$-axis on the left.}
\label{fig:se900}
\end{figure}

\begin{figure}[!htbp]
\begin{center}
%\includegraphics[angle=-90,width=6in]{Figures/fig.1.eps}
\includegraphics[scale=0.5]{./Figures/nothernWI.pdf}
%\includegraphics[scale=0.6]{nothernWI.pdf}
\end{center}
\caption{Map of the response variable in a study area of northern Wisconsin: Each of the 1,429 quarter sections is classified to be aspen-paper birch (APB) or other (OTH).}
\label{fig:data}
\end{figure}

\end{document}

%\bibitem[Wang~\textit{et al.}(2007)]{wangl07a} Wang, H., Li, G., and Tsai, C.-L. (2007) Regression coefficients and autoregressive order shrinkage and selection via the lasso. \textit{Journal of Royal Statistical Society Series B}, \textbf{69}, 63--78.

%\bibitem[Xue~\textit{et al.}(2012)]{xuez12} Xue, L., Zou, H., and Cai, T. (2012) Nonconcave penalized composite conditional likelihood estimation of sparse {I}sing models. \textit{Annals of Statistics}, \textbf{40}, 1403--1429.

\bibitem[Steen-Adams(2005)]{steen05} Steen-Adams, M.M. (2005) \textit{Change on a Northern Wisconsin Landscape: Legacies of Human History}. PhD Dissertation, University of Wisconsin, Madison, Wisconsin, USA.

