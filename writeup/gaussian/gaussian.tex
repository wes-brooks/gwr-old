\documentclass[authoryear, review, 11pt]{elsarticle}

\setlength{\textwidth}{6.5in}
%\setlength{\textheight}{9in}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{bm}

%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{verbatim}

\usepackage{relsize}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{fullpage}
\usepackage{booktabs}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\bw}{\mbox{bw}}
\DeclareMathOperator*{\df}{\mbox{df}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\E}{\mathop{\mathbb E}}


\title{Local variable selection for varying-coefficients models in the context of geographically-weighted regression}
\author{Wesley Brooks}
\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}





\section{Introduction}
	Varying-coefficients regression \citep{Hastie:1993a} is a technique used in spatial statistics to model a non-stationary process. Geographically weighted regression (GWR) \citep{Fotheringham:2002} is a method of fitting varying-coefficients regression models for spatial data that uses kernel-weighted regression with weights based on the distance between observation locations. The presentation of GWR in \cite{Fotheringham:2002} follows the development of local likelihood in \cite{Loader:1999}.
	
	GWR can be thought of as a kernel smoother for regression coefficients, and hence GWR coefficient estimates are likely to exhibit bias near the boundary of the region being modeled \citep{Hastie:1993b}. Modeling the coefficient surface as locally linear rather than locally constant (by including coefficient-by-location interactions) can reduce this boundary-effect bias \citep{Hastie:1993b}. Adding these interactions to the GWR model is analogous to a transition from kernel smoothing to local regression, and was introduced in \cite{Wang:2008b}.
	
	There is interest among practitioners of GWR not only in estimating the spatially-varying coefficient surface, but in doing variable selection to estimate which covariates are important predictors of the output variable, and in what regions they are important (citations?).
	
	Some recent research has focused on variable selection in varying-coefficients models. In the context of varying-coefficients regression models, global variable selection (in which one compares the hypothesis that the coefficient on a given variable is zero everywhere against the hypothesis that the coefficient is nonzero somewhere) is distinguished from local variable selection (in which one compares the hypothesis that the coefficient on a given variable is zero at a given location against the hypothesis that the coefficient at that location is nonzero). Global variable selection for models where the varying coefficients are estimated using splines is addressed in \cite{Fan:1999} for response variables that belong to an exponential-family distribution (as in the generalized linear model), and in \cite{Wang:2008a} for models with repeated measurements. \cite{Antoniadis:2012a} estimates the coefficient functions with P-splines, and then uses the nonnegative garrote of \cite{Breiman:1995} to do local variable selection by selecting P-spline bases.
	
	Here we discuss a method of local variable selection in GWR models using the adaptive lasso of \cite{Zou:2006}. The idea first appears in the literature as the geographically-weighted LASSO (GWL) of \cite{Wheeler:2009}, which uses a jackknife criterion for selection of the lasso tuning parameters. Because the jackknife criterion can only be computed at locations where the response variable is observed, the GWL cannot be used for imputation of missing data nor for interpolation between observation locations. We avoid this limitation of the GWL by using a penalized-likelihood criterion to select the lasso tuning parameters (specifically the AIC, but in principle one could use the BIC, \emph{et cetera}). The AIC allows us to easily adapt our method to the setting of a generalized linear model. The local AIC presented here is based on an \emph{ad hoc} calculation of the degrees of freedom used to estimate the spatially-varying coefficient surfaces.
	
\section{Geographically-weighted regression models \label{section:GWR}}

	\subsection{Model}
	Consider $n$ data observations, made at locations $s_1, \dots, s_n$. For $i = 1, \dots, n$, let $y(s_i)$ and $\bm{x}(s_i)$ be the univariate outcome of interest, and a $(p+1)$-variate vector of covariates measured at location $s_i$, respectively. At each location $s_i$, assume that the outcome is related to the covariates by a linear model with coefficients $\bm{\beta}_i(s_i)$ that may be spatially-varying.

	\begin{eqnarray}
		y(s_i) = \bm{x}'(s_i) \bm{\beta}(s_i) + \epsilon(s_i)
	\label{eq:lm(s)}
	\end{eqnarray}
	
	Further assume that the error term $\epsilon(s)$ is normally distributed with zero mean and a possibly spatially-varying variance $\sigma^2(s)$
	\begin{eqnarray}
		\epsilon(s_i) \sim \mathcal{N} \left( 0,\sigma^2(s_i) \right)
	\label{eq:err}
	\end{eqnarray}
	
	In order to simplify the notation, let subscripts denote the values of data or parameters at the locations where data is observed. Thus, $\bm{x}(s_i) \equiv \bm{x}_i \equiv \left( 1, x_{i1}, \dots, x_{ip} \right)'$, $\bm{\beta}(s_i) \equiv \bm{\beta}_i \equiv \left(\beta_{i0}, \beta_{i1}, \dots, \beta_{ip} \right)'$, $y(s_i) \equiv y_i$, and $\sigma^2(s_i) \equiv \sigma^2_i$. Let $\bm{X} = \left( \bm{x}_1, \dots, \bm{x}_n \right)'$ and $\bm{Y} = \left( y_1, \dots, y_n \right)'$. Now (\ref{eq:lm(s)}) - (\ref{eq:err}) can be rewritten
	\begin{eqnarray}
		y_i = \bm{x}'_i \bm{\beta}_i + \epsilon_i\\
		\epsilon_i \sim \mathcal{N} \left( 0,\sigma_i^2 \right)
	\end{eqnarray}
	
	Assume that, given the covariates $\bm{X}$, observations of the output at different locations are statistically independent of each other. Then the total log-likelihood of the observed data is the sum of the log-likelihood of each individual observation.
	 \begin{eqnarray}
	 	\ell\left( \bm{\beta} \right) = - \frac{1}{2} \sum_{i=1}^n \left\{  \log \left( 2 \pi \sigma^2_i\right) +  \sigma^{-2}_i  \left(y_i - \bm{x}'_i\bm{\beta}_i \right)^2  \right\}
	\end{eqnarray}
	
	With $n$ observations and $n \times (p+1)$ free parameters, the model is overdetermined so it is not possible to directly maximize the total likelihood. To effectively reduce the number of parameters, assume that the spatially-varying coefficients $\bm{\beta}(s)$ are \emph{smoothly} varying, and use a kernel smoother to make pointwise estimates of the coefficients by maximizing the local likelihood. In the setting of spatial data and with the kernel smoother based on the physical distance between observation locations, this method is called geographically-weighted regression (GWR).
		
	\subsection{Geographically-weighted regression}
	Geographically-weighted regression estimates the value of the coefficient surface $\bm{\beta}(s)$ at each location $s_i$. Assume for now that there are known weights $w_{ii'}$ based on the distance $\|s_i  -s_{i'}\|$ between locations $s_i$ and $s_{i'}$ for all $i, i'$.
	
	Coefficient estimation is done by maximizing the local likelihood at each location \citep{Fotheringham:2002}.	
	\begin{eqnarray}
		L_i\left(\bm{\beta}_i\right) &=& \prod_{i'=1}^n \left\{ \left(2 \pi \sigma^2_i  \right)^{-1/2}  \exp\left[-\frac{1}{2} \sigma^{-2}_i  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right] \right\} ^ {w_{ii'}}
	\end{eqnarray}
			
	\begin{eqnarray}
		\ell_i\left(\bm{\beta}_i\right) &\propto& - \frac{1}{2} \sum_{i'=1}^n w_{ii'} \left\{ \log{\sigma^2_i}  + \sigma^{-2}_i  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right\}
	\end{eqnarray}
	
	The first and second derivatives of the local log-likelihood are
	\begin{eqnarray}
		\left\{\frac{\partial \ell_i}{\partial \bm{\beta}_i} \right\}_j =   \sum_{i'=1}^n \left\{ x_{i'j} w_{ii'} \sigma^{-2}_i \left( y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right) \right\} \\
		\left\{\frac{\partial^2 \ell_i}{\partial \bm{\beta}_i \partial \bm{\beta}'_i} \right\}_{j,k} = -\sum_{i'=1}^n \left\{ x_{i'j} x_{i'k} w_{ii'} \sigma^{-2}_i \right\}
	\end{eqnarray}
	
	So the observed Fisher information in the locally weighted sample is
	\begin{eqnarray}
		\bm{\mathcal{J}}_i &=& \sigma^{-2}_i \left( \begin{array}{ccc} \sum_{i'=1}^n  w_{ii'} x^2_{i'1}   & \dots & \sum_{i'=1}^n w_{ii'} x_{i'1} x_{i'p}   \\ \vdots & \ddots & \vdots \\ \sum_{i'=1}^n  w_{ii'} x_{i'p} x_{i'1}    & \dots & \sum_{i'=1}^n  w_{ii'} x^2_{i'p}  \end{array} \right) \\
		&=& \sigma^{-2}_i \sum_{i'=1}^n w_{ii'}\left( \begin{array}{ccc}  x^2_{i'1} & \dots & x_{i'1} x_{i'p} \\ \vdots & \ddots & \vdots \\ x_{i'p} x_{i'1} & \dots &  x^2_{i'p} \end{array} \right) \\
		&=& \sigma^{-2}_i \sum_{i'=1}^n w_{ii'} \bm{x}_{i'} \bm{x}'_{i'}
	\end{eqnarray}	
	
	The form of the observed Fisher information suggests that the information in the data $\bm{x}_{i'}$ about the coefficients at location $s_i$ is proportional to the weight $w_{ii'}$.
	
	At each location $s_i$, the ordinary geographically-weighted regression estimator minimizes the objective function:
	\begin{eqnarray}
		\sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2
	\end{eqnarray}
	
	Letting the weight matrix $\bm{W}_i$ be	
	\begin{eqnarray}
		\bm{W}_i =  \left( \begin{array}{ccc} w_{i1} & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & w_{in} \end{array} \right)
	\end{eqnarray}
	
	estimation of the ordinary geographically-weighted regression coefficient surface is by weighted least squares:	
	\begin{eqnarray}
		\hat{\bm{\beta}}_{i, \text{GWR}} = \left( \bm{X}'\bm{W}_i\bm{X} \right)^{-1} \bm{X}'\bm{W}_i\bm{Y}
	\end{eqnarray}
	
	 
	 \subsection{Smoothing kernel}
	 	The bisquare kernel function is used to generate geographic weights based on the distance between observation locations. For estimating the value of the coefficient surface at location $s_i$, the weight given to the observation at location $s_{i'}$ is	
	\begin{eqnarray}
		w_{ii'} = \begin{cases} \left[ 1-\left( \bw^{-1} \|s_i-s_{i'}\| \right)^2 \right]^2 & \mbox{ if } \|s_i-s_{i'}\| < \bw \\ 0 & \mbox{ if } \|s_i-s_{i'}\| \geq \bw \end{cases}
	\end{eqnarray}
	
	where $\bw$ is the kernel bandwidth.\\
	
\section{Model selection and shrinkage \label{section:method}}
	Traditional GWR relies on \emph{a priori} model selection to decide which variables should be included in the model. In the context of ordinary least squares regression, regularization methods such as the Adaptive LASSO \citep{Zou:2006} have been shown to have appealing properties for automating variable selection, sometimes including the ``oracle" property of asymptotically selecting exactly the correct variables for inclusion in a regression model.\\
	
	The Adaptive LASSO is applied to GWR by first multiplying the design matrix $\bm{X}$ by $\bm{W}_i^{1/2}$, the diagonal matrix of geographic weights centered at $s_i$. Since some of the weights $w_{ii'}$ may be zero, the matrix $\bm{W}_i^{1/2}\bm{X}$ is not of full rank. The matrices $\bm{Y}_i^*$, $\bm{X}_i^*$, and $\bm{W}_i^*$ are formed by dropping the rows of $\bm{X}$  and $\bm{W}_i$ that correspond to observations with zero weight in the regression model at location $s_i$. Now, letting $\bm{U}_i^* = \bm{W}_i^{*1/2} \bm{X}_i^*$ and $\bm{V}_i^* = \bm{W}_i^{*1/2} \bm{Y}_i^*$, we seek the coefficients $\bm{\beta}_i$ of the regression model:
	
	\begin{eqnarray}
		\bm{V}_i^* = \bm{U}_i^* \bm{\beta}_i + \epsilon
	\end{eqnarray}
	
	To apply the Adaptive LASSO for estimating these regression coefficients, each column of $\bm{U}_i^*$ is centered around zero and rescaled to have an $\mbox{L}_2$-norm of one. Let $\widetilde{\bm{U}}_i^*$ be the centered-and-scaled version of $\bm{U}_i^*$. Adaptive weights are calculated using the OLS regression coefficients $\bm{\gamma}_i^*$ via ordinary least squares (OLS):
	
	\begin{eqnarray}\label{eq:adaptive-weights-regression}
		\bm{\gamma}_i^* = \left( \widetilde{\bm{U}}_i^{*'} \widetilde{\bm{U}}_i^* \right)^{-1} \widetilde{\bm{U}}_i^{*'} \bm{V}_i^*
	\end{eqnarray}
	
	Now a final scaling step is done: for $j=1, \dots, p$, the $j$th column of $\tilde{\bm{U}}_i^*$ is multiplied by $\left(\gamma_i^*\right)_j$, the corresponding coefficient from (\ref{eq:adaptive-weights-regression}). Call this rescaled matrix $\widecheck{\bm{U}}_i^*$.\\
	
	Finally, the Adaptive LASSO coefficient estimates at location $s_i$ are found by using the \verb!lars! algorithm \citep{Efron:2004b} to model $\bm{V}_i^*$ as a function of $\widecheck{\bm{U}}_i^*$.

	\subsection{Tuning parameter selection}
	The final task is to select the LASSO tuning parameter. \cite{Wheeler:2009} proposed selecting the tuning parameter for the LASSO at location $s_i$ to minimize the jackknife prediction error $|y_i - \hat{y}_i^{(i)}|$, but this choice restricts coefficient estimation to occur at the locations where data has been observed. We instead propose to use a locally-weighted version of the Akaike Information Criterion (AIC \citep{Akaike:1974}) to select the tuning parameter. The local AIC allows coefficients to be estimated at any location where the local likelihood can be calculated. The local AIC is calculated by adding a penalty to the local likelihood, with the sum of the weights around $s_i$, $\sum_{i'=1}^n w_{ii'}$, playing the role of the sample size and the number of nonzero coefficients in $\bm{\beta}_i$ playing the role of the ``degrees of freedom" $\left( \df_i \right)$ \citep{Zou:2007}.\\
	
	The objective minimized by the geographically-weighted adaptive lasso (GAL) is:	
	\begin{eqnarray}
		\sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 + \sum_{j=1}^p \lambda_{ij} \beta_{ij}
	\end{eqnarray}
	
	Where $\lambda_{ij}, j =1, \dots, p$ are penalties from the Adaptive LASSO \citep{Zou:2006}. Taking the derivatives with respect to $\beta$ and setting to zero, we see that
	\begin{eqnarray}
		\hat{\bm{\beta}}_{i, \text{GAL}} &=& \left( \bm{X}'\bm{W}_i\bm{X} \right)^{-1}  \bm{X}'\bm{W}_i\bm{Y}  - \frac{1}{2} \left(\bm{X}'\bm{W}_i\bm{X} \right)^{-1} \bm{\lambda}_i\\
		\hat{y}_i = \bm{x}_i \hat{\bm{\beta}}_{i, \text{GAL}} &=&  \bm{x}_i \left( \bm{X}'\bm{W}_i\bm{X} \right)^{-1}  \bm{X}'\bm{W}_i\bm{Y}  - \frac{1}{2} \bm{x}_i \left(\bm{X}'\bm{W}_i\bm{X} \right)^{-1} \bm{\lambda}_i
	\end{eqnarray}
	
	Unlike in the case of ordinary geographically-weighted regression, the fitted values $\hat{\bm{Y}}$ are not a linear combination of the observations $\bm{Y}$. Because GAL is not a linear smoother the AIC and confidence intervals as calculated in \cite{Fotheringham:2002} are not accurate for the GAL \citep{Zou:2006}. The local AIC ($\mbox{AIC}_{\mbox{loc}}$) is minimized to select the adaptive lasso tuning parameter.
	\begin{eqnarray}
		\mbox{AIC}_{\mbox{loc}, i} &=& -2 \sum_{i'=1}^n \ell_{ii'}  + 2 \mbox{df}_i\\
		&=& -2 \times \sum_{i'=1}^n \log \left\{ \left(2 \pi \hat{\sigma}_i^2\right)^{-1/2} \exp \left[-\frac{1}{2} \hat{\sigma}_i^{-2} \left(y_{i'} - \bm{x}_{i'}' \hat{\bm{\beta}}_{i'} \right)^2\right] \right\}^{w_{ii'}} + 2\mbox{df}_i\\
		&=& \sum_{i'=1}^n w_{ii'} \left\{ \log \left(2 \pi \right) + \log \hat{\sigma}_i^2 + \hat{\sigma}_i^{-2} \left(y_{i'} - \bm{x}_{i'}' \hat{\bm{\beta}}_{i'} \right)^2 \right\} + 2\mbox{df}_i \\
		&=& \hat{\sigma}_i^{-2} \sum_{i'=1}^n w_{ii'} \left( y_{i'} - \bm{x}'_{i'} \hat{\bm{\beta}}_i \right)^2 + 2 \mbox{df}_i + C_i
	\end{eqnarray}	
	Where the estimated local variance $\hat{\sigma}_i^2$ is the variance estimate from the unpenalized local model \citep{Zou:2007}, so $C_i$ does not depend on the choice of tuning parameter and can be ignored. The Maximum-Likelihood Estimate (MLE) of $\sigma_i^2$ is found by differentiating the local likelihood with respect to $\sigma_i^2$:
	
	\begin{eqnarray}
		\frac{\partial \ell_i}{\partial \sigma_i^2} \bigg|_{\hat{\beta}_i} &=& -\frac{1}{2} \sum_{i'=1}^n w_{ii'} \left\{ \left(\sigma_i^{2}\right)^{-1} - \left(\sigma_i^{2}\right)^{-2} \left( y_i - \bm{x}_i'\bm{\hat{\beta}}_i \right)^2 \right\} \\
		\hat{\sigma}_i^2 &=& \left(\sum_{i'=1}^n w_{ii'}\right)^{-1} \sum_{i'=1}^n w_{ii'} \left(y_i - \bm{x}_i'\hat{\bm{\beta}}_i\right)
	\end{eqnarray}
	 
	\subsection{Bandwidth selection}
	The bandwidth is selected to minimize the total AIC ($\mbox{AIC}_{\mbox{tot}}$). Because of the kernel weights and the application of the Adaptive LASSO, the sample size and degrees of freedom are different at each location. The total AIC is found by taking the sum over all of the observed data:	
		
	\begin{eqnarray}
		\mbox{AIC}_{\mbox{tot}} &=& -2 \times \sum_{i=1}^n \ell_i + 2 \times \mbox{df}\\
		&=& \sum_{i=1}^n \left\{ \log \hat{\sigma}_i^2 + \hat{\sigma}_i^{-2} \left(y_i - \bm{x}_i' \hat{\bm{\beta}}_i \right)^2 \right\} + 2 \times \mbox{df}\label{eq:total-AIC1}
	\end{eqnarray}	
	
	What remains is to calculate $\mbox{df}$, the number of degrees of freedom used by the model. Classical GWR, as developed in \cite{Loader:1999} and \cite{Fotheringham:2002} calculates $\mbox{df}$ using the trace of the ``hat" matrix, but because the GAL is not a linear smoother, there is no ``hat" matrix associated with GWR. Instead, notice that $\mbox{df}$ can be pulled into the summation in (\ref{eq:total-AIC1}):
		
	%This is different from the formulas for the AIC as proposed in \cite{Fotheringham:2002} and \cite{Loader:1999}. The reason is that the basic GWR estimator is linear, so the degrees of freedom can be approximated using the trace of the ``hat" matrix. The GAL, though, is not a linear estimator so some...\ However, since the GAL does local variable selection, the model may use a different number of parameters at each location $s_i$. 
			
	\begin{eqnarray}
		\mbox{df} &=& \sum_{i=1}^n \left( n^{-1} \mbox{df} \right)
	\end{eqnarray}
	
	Now, because we are considering the sum of local weights to be the sample size for the local models, we estimate $\mbox{df}$ by $\sum_{i=1}^n \left\{ \left(\sum_{i'=1}^n w_{ii'} \right)^{-1} \mbox{df}_i \right\}$, and the total AIC is then:
	
	\begin{eqnarray}\label{eq:total-AIC2}
		\mbox{AIC}_{\mbox{tot}} &=& \sum_{i=1}^n \left\{ \log \hat{\sigma}_i^2 + \hat{\sigma}_i^{-2} \left(y_i - \bm{x}'_i \hat{\bm{\beta}}_i \right)^2 + 2 \times \left(\sum_{i'=1}^n w_{ii'} \right)^{-1} \mbox{df}_i \right\}
	\end{eqnarray}
			
	The bandwidth that minimizes (\ref{eq:total-AIC2}) is found by a line search.\\
	
	\subsection{Confidence interval construction}	
	Confidence intervals for the GAL's coefficient estimates can be calculated either by the bootstrap \citep{Efron:1986} or by exploiting an assumption of normally-distributed residuals. The, e.g., 95\% confidence interval for each regression coefficient is then the (2.5, 97.5) percentiles of the coefficient estimates from the bootstrap replicates.\\
	 
	 	\subsubsection{Bootstrap confidence interval}
		To compute coefficient confidence intervals via the bootstrap, the observations with non-zero geographic weights are resampled uniformly with replacement for each of $n_B$ bootstrap replicates. For each bootstrap replicate, the GAL is used to estimate regression coefficients. The local likelihood of the bootstrap replicates may be different from that of the original sample, so the adaptive lasso tuning parameter may differ for each bootstrap replicate. Since the GAL is applied independently to each bootstrap replicate, the variables selected by GAL may be different for each replicate.
	
		 Unshrunk coefficient estimates are found by using the GAL at each location for variable selection only and then estimating the coefficients for the selected variables by GWR. An unshrunk bootstrap confidence interval is found by estimating the unshrunk coefficients for each of the $n_B$ bootstrap replicates and then calculating the percentiles as above.\\
	 
		
	 	\subsubsection{Normal approximation-based confidence interval}
		A third way to estimate the coefficient confidence intervals is to use the GAL for variable selection only and then to use GWR to calculate a confidence interval based on the assumption of an independent, identically distributed, Gaussian error structure. In this case, the standard error of the regression coefficients is 
		\begin{eqnarray}
			\hat{\mbox{se}}_{\beta_i} &=& \left( \tilde{\bm{X}}_i'\bm{W}_i \tilde{\bm{X}}_i \right)^{-1}  \tilde{\bm{X}}_i'\bm{W}_i\bm{Y}
		\end{eqnarray}
	
		where $\tilde{\bm{X}}_i$ is the model matrix including only those variables that are selected by GAL at location $i$.



\section{Simulation}
	\subsection{Simulation setup}
	A simulation study was conducted to assess the finite-sample properties of the method described in Sections \ref{section:model}-\ref{section:method}. Data was simulated on $[0,1] \times [0,1]$, which was divided into a $30 \times 30$ grid. Each of $p=5$ covariates $Z_1, \dots, Z_p$ was simulated by a Gaussian random field (GRF) with mean zero and exponential spatial covariance $Cov \left(Z_{ji}, Z_{ji'} \right) = \sigma_z^2 \exp{\left( -\tau_z^{-1} \|s_i - s_{i'} \| \right)}$ where $\sigma_z^2=1$ is the variance and $\tau_z$ is a range parameter. Correlation was induced between the covariates by multiplying the $\bm{Z}$ matrix by $\bm{R}$, where $\bm{R}$ is the Cholesky decomposition of the covariance matrix $\Sigma = \bm{R}'\bm{R}$. The covariance matrix $\bm{\Sigma}$ is a $5 \times 5$ matrix that has ones on the diagonal and $\rho$ for all off-diagonal entries, where $\rho$ is the between-covariate correlation.
		
	The simulated response is $y_i = \bm{z}'_i \bm{\beta}_i + \epsilon_i$ for $i=1, \dots, 900$ where the vector of additive errors $\bm{\epsilon}$ is generated from a GRF with spatial covariance $Cov \left(\epsilon_{i}, \epsilon_{i'} \right) = \sigma_{\epsilon}^2 \exp{\left( -\tau_{\epsilon}^{-1} \|s_i - s_{i'} \| \right)}$ where $\sigma_{\epsilon}^2=1$.
	
	The simulated data include the output $y$ and five covariates $Z_1, \dots, Z_5$. The true data-generating model uses only $Z_1$, so $Z_2, \dots, Z_5$ are included to test the variable-selection properties of GAL. The coefficient surface of $\beta_1$ is described by the ``step" function:
	\begin{eqnarray}
		\beta_1(s) = \begin{cases} 0 &\mbox{ if } s_y<0.4 \\ 5(s_y-0.4) &\mbox{ if } 0.4 \leq s_y<0.6 \\ 1 &\mbox{ o.w.} \end{cases}
	\end{eqnarray}.\\
		
	In order to evaluate the performance of GAL under a range of conditions, the data was simulated under 18 different settings (Table \ref{table:simulation_settings}): high (0.1) and low (0.03) levels of $\tau_z$, the autoregression range parameter for the covariate GRFs $Z_1, \dots, Z_5$; three levels (0, 0.5, 0.8) of between-covariate correlation $\rho$; and three levels (0, 0.03, 0.1) of the autoregression range parameter $\tau_{\epsilon}$ for the error-term GRF $\bm{\epsilon}$. Each case was simulated 100 times.
	
	For measuring performance, we look at the pointwise selection frequency of $\beta_1, \dots, \beta_5$ and the coverage frequency of $\beta_1$ (for a nominal 95\% confidence interval).
	
	As a baseline, ordinary GWR was used to estimate the coefficients using the same data but under an ``oracle" setting (oracular GWR, or O-GWR), meaning that GWR was provided with the exactly correct set of predictors as used in the data-generating process. The ratio of the coverage frequency of the GAL to the coverage frequency of O-GWR is called the relative efficiency of the GAL.\\
	
	% latex table generated in R 2.15.1 by xtable 1.7-0 package
% Fri Jan 18 10:19:47 2013
\begin{table}[h!]
	\begin{center}
	\begin{tabular}{rrrr}
		\hline
		& $\tau_z$ & $\rho$ & $\tau_{\epsilon}$ \\ 
		\hline
		1 & 0.03 & 0.00 & 0.00 \\ 
		2 & 0.03 & 0.00 & 0.03 \\ 
		3 & 0.03 & 0.00 & 0.10 \\ 
		4 & 0.03 & 0.50 & 0.00 \\ 
		5 & 0.03 & 0.50 & 0.03 \\ 
		6 & 0.03 & 0.50 & 0.10 \\ 
		7 & 0.03 & 0.80 & 0.00 \\ 
		8 & 0.03 & 0.80 & 0.03 \\ 
		9 & 0.03 & 0.80 & 0.10 \\ 
		10 & 0.10 & 0.00 & 0.00 \\ 
		11 & 0.10 & 0.00 & 0.03 \\ 
		12 & 0.10 & 0.00 & 0.10 \\ 
		13 & 0.10 & 0.50 & 0.00 \\ 
		14 & 0.10 & 0.50 & 0.03 \\ 
		15 & 0.10 & 0.50 & 0.10 \\ 
		16 & 0.10 & 0.80 & 0.00 \\ 
		17 & 0.10 & 0.80 & 0.03 \\ 
		18 & 0.10 & 0.80 & 0.10
	\end{tabular}
	\end{center}
	\caption{Simulation parameters for each setting.\label{table:simulation_settings}}
\end{table}
	
	\subsection{Simulation results}
	Results of the simulation experiment were summarized to asses the consistency in selection and estimation, as well as the coverage properties of the confidence intervals. The confidence intervals based on the bootstrap (without shrinkage) were used for the GAL because they seemed to uniformly outperform the other options.\\
	
% latex table generated in R 2.15.1 by xtable 1.7-1 package
% Thu Feb 28 18:49:03 2013
\begin{table}[ht]
\centering
\begin{tabular}{ccccc}
  \hline
AL & AL-Unshrunk & AL-Precon & AL-Precon-Unshrunk & Oracle \\ 
  \hline
0.023 & 0.017 & 0.024 & \emph{0.017} & \textbf{0.012} \\ 
  0.029 & \emph{0.023} & 0.030 & \textbf{0.023} & 0.031 \\ 
  0.027 & \emph{0.023} & 0.030 & \textbf{0.023} & 0.056 \\ 
  0.028 & 0.023 & 0.030 & \emph{0.022} & \textbf{0.012} \\ 
  0.038 & 0.034 & 0.040 & \emph{0.034} & \textbf{0.029} \\ 
  0.032 & \emph{0.028} & 0.038 & \textbf{0.028} & 0.057 \\ 
  0.041 & 0.039 & 0.042 & \emph{0.036} & \textbf{0.012} \\ 
  0.077 & 0.081 & 0.081 & \emph{0.077} & \textbf{0.030} \\ 
  0.062 & 0.060 & 0.076 & \emph{0.060} & \textbf{0.055} \\ 
  0.026 & 0.023 & 0.026 & \emph{0.023} & \textbf{0.016} \\ 
  0.040 & \textbf{0.039} & 0.041 & \emph{0.040} & 0.061 \\ 
  0.057 & \textbf{0.050} & 0.067 & \emph{0.051} & 0.125 \\ 
  0.029 & 0.026 & 0.030 & \emph{0.025} & \textbf{0.016} \\ 
  \emph{0.055} & 0.056 & 0.057 & \textbf{0.055} & 0.059 \\ 
  0.078 & \textbf{0.074} & 0.094 & \emph{0.076} & 0.130 \\ 
  0.046 & 0.046 & 0.045 & \emph{0.042} & \textbf{0.016} \\ 
  \emph{0.119} & 0.134 & 0.122 & 0.132 & \textbf{0.063} \\ 
  0.167 & \emph{0.161} & 0.210 & 0.166 & \textbf{0.125} \\ 
   \hline
\end{tabular}
\end{table}


% latex table generated in R 2.15.1 by xtable 1.7-1 package
% Thu Feb 28 14:55:22 2013
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & AL & AL-Unshrunk & AL-Precon & AL-Precon-Unshrunk & Oracle \\ 
  \hline
1 & 0.968 & \textbf{0.934} & 0.965 & 0.936 & 0.964 \\ 
  2 & 0.886 & 0.783 & 0.871 & 0.784 & \textbf{0.714} \\ 
  3 & 0.896 & 0.601 & 0.858 & 0.600 & \textbf{0.167} \\ 
  4 & 0.975 & 0.945 & 0.973 & \textbf{0.944} & 0.972 \\ 
  5 & 0.900 & 0.801 & 0.886 & 0.800 & \textbf{0.726} \\ 
  6 & 0.875 & 0.596 & 0.841 & 0.593 & \textbf{0.161} \\ 
  7 & 0.967 & 0.940 & 0.965 & \textbf{0.938} & 0.965 \\ 
  8 & 0.887 & 0.786 & 0.872 & 0.785 & \textbf{0.724} \\ 
  9 & 0.890 & 0.609 & 0.853 & 0.605 & \textbf{0.165} \\ 
  10 & 1.009 & \textbf{0.967} & 0.986 & 0.969 & 0.976 \\ 
  11 & 0.880 & 0.789 & 0.840 & 0.791 & \textbf{0.722} \\ 
  12 & 0.707 & 0.496 & 0.605 & 0.496 & \textbf{0.171} \\ 
  13 & 1.003 & \textbf{0.965} & 0.981 & 0.966 & 0.968 \\ 
  14 & 0.880 & 0.796 & 0.842 & 0.797 & \textbf{0.716} \\ 
  15 & 0.711 & 0.504 & 0.609 & 0.504 & \textbf{0.167} \\ 
  16 & 1.009 & 0.976 & 0.990 & \textbf{0.975} & 0.979 \\ 
  17 & 0.881 & 0.798 & 0.840 & 0.796 & \textbf{0.723} \\ 
  18 & 0.692 & 0.482 & 0.588 & 0.481 & \textbf{0.169} \\ 
   \hline
\end{tabular}
\end{table}

% latex table generated in R 2.15.1 by xtable 1.7-1 package
% Thu Feb 28 15:28:51 2013
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & original & preconditioned \\ 
  \hline
1 & \textbf{0.623} & 0.622 \\ 
  2 & \textbf{0.447} & 0.430 \\ 
  3 & \textbf{0.278} & 0.238 \\ 
  4 & 0.560 & \textbf{0.568} \\ 
  5 & \textbf{0.423} & 0.411 \\ 
  6 & \textbf{0.256} & 0.207 \\ 
  7 & 0.502 & \textbf{0.516} \\ 
  8 & \textbf{0.395} & 0.387 \\ 
  9 & \textbf{0.230} & 0.204 \\ 
  10 & 0.541 & \textbf{0.550} \\ 
  11 & \textbf{0.298} & 0.293 \\ 
  12 & \textbf{0.078} & 0.060 \\ 
  13 & 0.535 & \textbf{0.554} \\ 
  14 & \textbf{0.285} & 0.279 \\ 
  15 & \textbf{0.076} & 0.060 \\ 
  16 & 0.458 & \textbf{0.481} \\ 
  17 & \textbf{0.254} & 0.250 \\ 
  18 & \textbf{0.079} & 0.065 \\ 
   \hline
\end{tabular}
\end{table}

	
	\subsection{Figures}
	Figures \ref{fig:coveragemap1} - \ref{fig:coveragemap18} show the frequency with which the true value of the parameter $\beta_1$ was covered by the 95\% confidence intervals at each location under each simulation setting. The left column shows the coverage frequency of the 95\% CI of the GAL using the unshrunk-bootstrap method of CI construction. The middle column is the coverage frequency of the 95\% CI the O-GWR using the bootstrap to generate the CI. The right column is the relative efficiency of the GAL to O-GWR. In the first two columns, the color white is used to indicate areas where the nominal coverage frequency of 95\% is achieved, while blue codes areas that exceeded 95\% coverage and orange codes areas that fell short of 95\% coverage. In the third column, the color white indicates areas where the relative efficiency is unity, while orange indicates areas where the relative efficiency was less than unity and blue indicates areas where the relative efficiency exceeded unity.\\	
	
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-1.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-1-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 1\label{fig:coveragemap1}}
	\end{center}
        
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-2.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-2-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 2\label{fig:coveragemap2}}
	\end{center}
	
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-3.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-3-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 3\label{fig:coveragemap3}}
	\end{center}
	
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-4.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-1-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 4\label{fig:coveragemap4}}
	\end{center}
        
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-5.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-2-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 5}
		\label{fig:coveragemap5}
	\end{center}
	
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-6.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-3-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 6}
		\label{fig:coveragemap6}
	\end{center}
	
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-7.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-1-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 7}
		\label{fig:coveragemap7}
	\end{center}
		        
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-8.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-2-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 8}
		\label{fig:coveragemap8}
	\end{center}
	
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-9.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-3-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 9}
		\label{fig:coveragemap9}
	\end{center}
	
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-10.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-1-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 10}
		\label{fig:coveragemap10}
	\end{center}
        
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-11.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-2-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 11}
		\label{fig:coveragemap11}
	\end{center}
	
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-12.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-3-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 12}
		\label{fig:coveragemap12}
	\end{center}
	
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-13.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-1-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 13}
		\label{fig:coveragemap13}
	\end{center}
        
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-14.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-2-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 14}
		\label{fig:coveragemap14}
	\end{center}
	
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-15.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-3-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 15}
		\label{fig:coveragemap15}
	\end{center}
	
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-16.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-1-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 16}
		\label{fig:coveragemap16}
	\end{center}
        
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-17.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-2-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 17}
		\label{fig:coveragemap17}
	\end{center}
	
	\begin{center}
		%\centering
		\includegraphics[width=0.99\textwidth]{../../figures/X1-28-18.pdf}
		%\includegraphics[width=\textwidth]{../../figures/simulation/28-3-profile-coverage.pdf}
		\captionof{figure}{Coverage frequency of 95\% CIs: setting 18}
		\label{fig:coveragemap18}
	\end{center}
	
	%\begin{comment}
	%\end{comment}

			
\section{Data analysis}
	\subsection{Census poverty data}
	We present the following analysis to demonstrate one possible application of the geographically-weighted lasso in a linear regression context. We use county-level data from the US Census Bureau to select the social and demographic variables that are important predictors of the county-level poverty rate in the upper midwest, and to estimate the coefficients associated with these predictors. Data are from six censuses - the decennial censuses from 1960 to 2000, and from the American Community Survey in 2006. Selection and estimation are done for each census individually (no attempt is made here to borrow strength across years). The outcome of interest (poverty rate) is a proportion and so takes values on $[0,1]$, but to demonstrate the GAL in a linear regression context, we model the logit-transformed poverty rate. Our data set covers all counties in the states of Minnesota, Iowa, Wisconsin, Illinois, Indiana, and Michigan. The potential predictors are described in Table \ref{table:census-vars}.\\
	
	\begin{table}
		\begin{center}
		\begin{tabular}{ll}
			Variable name & Description \\
			\hline
			\verb!pag! & Proportion working in agriculture\\
			\verb!pex! &  Proportion working in extraction (mining)\\
			\verb!pman! & Proportion working in manufacturing \\
			\verb!pserve! & Proportion working in services \\
			\verb!pfire! & Proportion working in finance, insurance, and real estate \\
			\verb!potprof! & Proportion working in other professions \\
			\verb!pwh! & Proportion who are white \\
			\verb!pblk! & Proportion who are black \\
			\verb!phisp! & Proportion who are hispanic \\
			\verb!metro! & Is the county in a metropolitan area?\\
		\end{tabular}
		\caption{Description of the variables used in the census-data example\label{table:census-vars}}
		\end{center}		
	\end{table}
	
	\subsection{Figures}
	The coefficient estimates are plotted on maps of the upper midwest in Figures \ref{fig:census-coefs-1960} - \ref{fig:census-coefs-2006}. It is immediately apparent that the estimated coefficient surfaces are non-constant for most variables.\\
		
	\begin{figure}
		\begin{center}
			\includegraphics[height=8in]{../../figures/poverty/1960.linear.coefficients.pdf}
			\caption{Estimated coefficient surfaces for the 1960 census.\label{fig:census-coefs-1960}}
		\end{center}		
	\end{figure}

	\begin{figure}
		\begin{center}
			\includegraphics[height=8in]{../../figures/poverty/1970.linear.coefficients.pdf}
			\caption{Estimated coefficient surfaces for the 1970 census.\label{fig:census-coefs-1970}}
		\end{center}
	\end{figure}
	
	\begin{figure}
		\begin{center}
			\includegraphics[height=8in]{../../figures/poverty/1980.linear.coefficients.pdf}
			\caption{Estimated coefficient surfaces for the 1980 census.\label{fig:census-coefs-1980}}
		\end{center}
	\end{figure}
	
	\begin{figure}
		\begin{center}
			\includegraphics[height=8in]{../../figures/poverty/1990.linear.coefficients.pdf}
			\caption{Estimated coefficient surfaces for the 1990 census.\label{fig:census-coefs-1990}}
		\end{center}
	\end{figure}
	
	\begin{figure}
		\begin{center}
			\includegraphics[height=8in]{../../figures/poverty/2000.linear.coefficients.pdf}
			\caption{Estimated coefficient surfaces for the 2000 census.\label{fig:census-coefs-2000}}
		\end{center}
	\end{figure}

	\begin{figure}
		\begin{center}
			\includegraphics[height=8in]{../../figures/poverty/2006.linear.coefficients.pdf}
			\caption{Estimated coefficient surfaces for the 2006 census.\label{fig:census-coefs-2006}}
		\end{center}
	\end{figure}
			
\section{References}
\bibliographystyle{chicago}
\bibliography{../../references/gwr}

\end{document}  