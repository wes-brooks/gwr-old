\documentclass[authoryear, review, 11pt]{elsarticle}

\setlength{\textwidth}{6.5in}
%\setlength{\textheight}{9in}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{bm}
\usepackage{multirow}

%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{verbatim}
\usepackage{longtable}
\usepackage{rotating}
\usepackage[nolists,nomarkers]{endfloat}
\DeclareDelayedFloatFlavour{sidewaystable}{table}

\usepackage{relsize}
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{booktabs}


\usepackage{setspace}
\setstretch{2}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\bw}{\mbox{bw}}
\DeclareMathOperator*{\df}{\mbox{df}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\E}{\mathop{\mathbb E}}


\title{Local Variable Selection and Parameter Estimation of Spatially Varying Coefficient Models}
\author{Wesley Brooks}
\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}

%Pastebin:
%Spatial association - meaning that nearby locations are more alike than distant locations - is a key concept in spatial statistics.\
%for univariate output $y$, $p$-vector of covariates $\bm{x}$, and random noise $\varepsilon$ all indexed by location $\bm{s}
%where $\phi$ is a bandwidth parameter and $\delta(\bm{s}, \bm{t})$ is the Euclidean distance between locations $\bm{s}$ and $\bm{t}$.

%\begin{spacing}{2}
\section{Introduction}
	%Varying coefficient regression
	Whereas the coefficients in traditional linear regression are scalar constants, the coefficients in a varying coefficient regression (VCR) model are functions - often \emph{smooth} functions - of some effect modifying variable \citep{Hastie:1993a}. When the effect modifying variable represents location in a spatial domain, a VCR model implies a spatially local regression model such that the regression coefficients vary over space and will be referred to as a spatially varying coefficient model (SVCR). Statistical inference for the coefficients as functions of location in an SVCR model is more complicated than estimating the coefficients in a global linear regression model where the coefficients are constant across the spatial domain. This document concerns the development of new methodologies for the analysis of spatial data using SVCR.
	
	%Spatial data / spatial regression
	The methodology described herein is directly applicable only to geostatistical data, which is spatial data observed at discrete locations. Let $\mathcal{D}$ be a spatial domain on which data is collected, and let $\bm{s}$ denote a location variable that indexes the domain $\mathcal{D}$. Let univariate $\left\{Y(\bm{s}) : \bm{s} \in \mathcal{D}\right\}$ and possibly multivariate $\left\{\bm{X}(\bm{s}) : \bm{s} \in \mathcal{D}\right\}$ denote random fields called the response and the covariates, respectively. For $i = 1, \dots, n$, let $\bm{s}_i$ denote the location in $\mathcal{D}$ of the $i$th observation of the response and the covariates. Then the data are a realization of the random variables $\left\{Y(\bm{s}_1), \dots, Y(\bm{s}_n), \bm{X}(\bm{s}_1), \dots, \bm{X}(\bm{s}_n)\right\}$. Let the observed data be denoted $\left\{y(\bm{s}_1), \dots, y(\bm{s}_n), \bm{x}(\bm{s}_1), \dots, \bm{x}(\bm{s}_n)\right\}$.
	
	Areal data is a different kind of spatial data in which the spatial domain $\mathcal{D}$ consists of $n$ regions $\{r_1, \dots, r_n\}$. In the case of areal data, the random variables $\left\{Y(r_1), \dots, Y(r_n), \bm{X}(r_1), \dots, \bm{X}(r_n)\right\}$ are defined for regions instead of for points; population and spatial mean temperature are examples of areal data. The analytical method described herein can be applied to areal data if it is recast as geostatistical data by assuming that the data are point-referenced to the centroid of each region, i.e. $\left\{\bm{X}(\bm{s}_i), Y(\bm{s}_i)\right\} = \left\{\bm{X}(r_i), Y(r_i)\right\}$ where $\bm{s}_i$ is the centroid of $r_i$ for $i=i, \dots, n$. The data example in section \ref{section:data-analysis} uses areal data relating to county-level demographics in this way.
	
	 Common practice in the analysis of geostatistical and areal data is to model the response variable with a spatial linear regression model consisting of the sum of a fixed mean function, a spatial random effect, and random error all on domain $\mathcal{D}$, as in: 
\begin{align}\label{eq:spatial-regression}
    Y(\bm{s}) = \bm{X}(\bm{s})'\bm{\beta} + W(\bm{s}) + \varepsilon(\bm{s})
\end{align}
where $\bm{X}(\bm{s})'\bm{\beta}$ is the mean function consisting of $\bm{X}(\bm{s})$, a possibly multivariate spatial random field of covariates, and $\bm{\beta}$, a vector of regression coefficients. The random error $\varepsilon(\bm{s})$ denotes a white noise field such that the errors are independent and identically distributed with mean zero and variance $\sigma^2$, while the random component $W(\bm{s})$ denotes a mean-zero, second-order stationary random field that is independent of the random error. The mean function captures the large-scale systematic trend of the response, the spatial random field $W(\bm{s})$ can be thought of as a small-scale spatial random effect, and the error term $\varepsilon(\bm{s})$ captures micro scale variation \citep{Cressie:1993}. It is common to prespecify the form of a covariance function for the spatial random effect $W(\bm{s})$ \citep{Diggle:2007}. For example, the exponential covariance function (a special case of the Mat\'{e}rn class of covariance functions) has the form
\begin{align}\label{eq:exponential-covariance}
    \text{Cov}(W(\bm{s}), W(\bm{t})) = \exp\left\{-\phi^{-1} \delta(\bm{s}, \bm{t}) \right\}
\end{align}
where $\phi$ denotes a range parameter and $\delta(\bm{s}, \bm{t})$ denotes the Euclidean distance between locations $\bm{s}$ and $\bm{t}$. The general form of a Mat\'{e}rn class covariance function is
\begin{align}\label{eq:matern-covarinace}
    \text{Cov}(W(\bm{s}), W(\bm{t})) = \left\{\Gamma(\nu) 2^{\nu-1} \right\}^{-1} \left\{\delta(\bm{s}, \bm{t}) \phi^{-1}\sqrt{2\nu}\right\}^\nu K_{\nu} \left(\delta(\bm{s}, \bm{t}) \phi^{-1}\sqrt{2\nu}\right)
    \end{align}
where $\phi$ denotes a range parameter, $\nu$ denotes the degree of smoothness, $K_{\nu}$ denotes the modified Bessel equation of the second kind, and $\delta(\bm{s}, \bm{t})$ denotes the Euclidean distance between locations $\bm{s}$ and $\bm{t}$. The exponential covariance function corresponds to a Mat\'{e}rn class covariance function with $\nu = 1/2$.

	%SVCR - justification
	%Stationarity in spatial linear regression
	A spatial field is said to be stationary if the joint distribution of a sample from the field does not change when the sample locations are all shifted in space by the same amount. Let $\left\{T(\bm{s}) : \bm{s} \in \mathcal{D}\right\}$ be a random field on spatial domain $\mathcal{D}$ that takes value $T(\bm{s}_i)$ at location $\bm{s}_i \in \mathcal{D}$ for $i = 1, \dots, n$. The random field $T(\bm{s})$ is stationary if $F_n\left(T(\bm{s}_1), \dots, T(\bm{s}_n)\right) = F_n\left(T(\bm{s}_1+\bm{h}), \dots, T(\bm{s}_n+\bm{h})\right)$ where $F_n(\cdot)$ is the joint distribution of a length $n$ sample from $T(\bm{s})$. A spatial random field is second-order stationary if the joint distribution of any two observations from a sample does not change when the sample locations are shifted by the same amount.
	
	The coefficient vector $\bm{\beta}$ in (\ref{eq:spatial-regression}) is a specific example of the case where $\left\{\bm{\beta}(\bm{s}) : \bm{s} \in \mathcal{D}\right\}$ is a spatial random field. Specifically, the coefficient vector in (\ref{eq:spatial-regression}) represents the case of where $\bm{\beta}(\bm{s}) \equiv \bm{\beta}$, $\forall \bm{s} \in \mathcal{D}$, i.e. the random field is constant. Clearly such random field is stationary. It is also possible to specify a non-constant random coefficient field that is nevertheless stationary. \cite{Gelfand:2003} suggests a Bayesian hierarchical model where 

	%The spatial random effect describes the spatial pattern in the deviations from the systematic part of the model. When fitting the spatial regression model (\ref{eq:spatial-regression}), it is usually required that the the fitted values of the spatial random effect and of the residuals sum to zero, i.e. $\sum\limits_{i=1}^n\hat{W}(\bm{s}_i) = 0$ and $\sum\limits_{i=1}^n\hat{\varepsilon}(\bm{s}_i) = 0$. This mode of analysis is appropriate when the systematic part of the regression model does not vary between locations. On the other hand, a VCR model is appropriate for the case where the systematic part of the regression model does vary across locations.

	%Spatial VCR: splines	
	Both kernel-based and spline-based methods are available for fitting varying coefficient models. For example, \cite{Wood:2006} demonstrated that it is straightforward to modify a thin plate regression spline model into a VCR model. \cite{Loader:1999} developed the local likelihood as a tool for fitting generalized linear models with varying coefficients using kernel smoothing, while \cite{Fan:1999} demonstrated that the optimal kernel bandwidth estimate for a VCR model can be found via a two-step technique.
	
	Model selection in VCR models may be local or global. Global selection means including or excluding variables everywhere in the model domain, while local selection means including or excluding variables at each observation location. Two methods have been proposed for global model selection in spline-based VCR models. \cite{Wang:2008a} applied a SCAD penalty \citep{Fan:2001} for variable selection in spline-based VCR models with a univariate effect-modifying variable. \cite{Antoniadis:2012a} used the nonnegative Garrote penalty \citep{Breiman:1995} in P-spline-based VCR models having a univariate effect-modifying variable.

	%Kernel smoothing
	This document focuses on GWR, which is a kernel-based method of estimating the coefficients of a VCR model in the context of spatial data \citep{Brundson:1998a, Fotheringham:2002}. GWR uses kernel-weighted regression with weights based on the distance between observation locations. The presentation of GWR in \cite{Fotheringham:2002} followed the development of local likelihood in \cite{Loader:1999}. GWR can be thought of as a kernel smoother for regression coefficients, which tends to exhibit bias near the boundary of the region being modeled \citep{Hastie:1993b}. One way to reduce the boundery-effect bias is to model the coefficient surface as locally linear rather than locally constant by including coefficient-by-location interactions \citep{Hastie:1993b}. Adding these interactions to the GWR model is analogous to a transition from kernel smoothing to local regression, which was introduced in \cite{Wang:2008b}.
	
	Three regularization methods were used in this work. The adaptive Lasso was implemented in two ways - once via the lars algorithm \citep{Efron:2004b} which uses least squares, and once via coordinate descent using the R package glmnet \citep{Friedman:2010}. The third regularization method implemented here uses the adaptive elastic net penalty \citep{Zou:2009}, also via coordinate descent using the glmnet package.
	
	%Local variable selection
	This document aims to develop a new method of local variable selection for Geographically Weighted Regression (GWR) models, which are a class of kernel-based VCR models for geostatistical data. Traditional GWR relies on \emph{a priori} model selection to decide which variables should be included in the model. In the context of ordinary least squares regression, regularization methods such as the adaptive Lasso \citep{Zou:2006} have been shown to have appealing properties for automating variable selection, sometimes including the ``oracle" property of asymptotically selecting exactly the correct variables for inclusion in a regression model.
	
	The idea of using regularization for local variable selection in a GWR model first appeared in the literature as the geographically-weighted Lasso (GWL) of \cite{Wheeler:2009}, which used a jackknife criterion for selection of the Lasso tuning parameters. Because the jackknife criterion can only be computed at locations where the response variable is observed, the GWL cannot be used for imputation of missing data nor for interpolation between observation locations. We avoid this limitation of the GWL by using a penalized-likelihood criterion to select the Lasso tuning parameters. Here we use a version of the BIC, but in principle one could use another information criterion like the AIC. The local BIC presented here is based on the local likelihood \citep{Loader:1999} and the total BIC is based on an \emph{ad hoc} calculation of the sample size and degrees of freedom for estimating the spatially-varying coefficient surfaces.

\section{Geographically Weighted Regression \label{section:GWR}}
	\subsection{Model}	
	Consider $n$ data observations, made at sampling locations $\bm{s}_1, \dots, \bm{s}_n$ in a spatial domain $D \subset \mathbb{R}^2$. For $i = 1, \dots, n$, let $y(\bm{s}_i)$ and $\bm{x}(\bm{s}_i)$ denote the univariate response variable, and a $(p+1)$-variate vector of covariates measured at location $\bm{s}_i$, respectively. At each location $\bm{s}_i$, assume that the outcome is related to the covariates by a linear model where the coefficients $\bm{\beta}(\bm{s}_i)$ may be spatially-varying and $\varepsilon(\bm{s}_i)$ is random noise at location $\bm{s}_i$. That is,
	\begin{align}\label{eq:lm(s)}
		y(\bm{s}_i) = \bm{x}(\bm{s}_i)' \bm{\beta}(\bm{s}_i) + \varepsilon(\bm{s}_i)
	\end{align}
	
	Further assume that the error term $\varepsilon(\bm{s}_i)$ is normally distributed with zero mean and a possibly spatially-varying variance $\sigma^2(\bm{s}_i)$, and that $\varepsilon(\bm{s}_i)$, $i=1, \dots, n$ are independent.
	\begin{align} \label{eq:err}
		\varepsilon(\bm{s}_i) \overset{indep}{\sim} \mathcal{N} \left( 0,\sigma^2(\bm{s}_i) \right)
	\end{align}
	
	In order to simplify the notation, let $\bm{x}(\bm{s}_i) \equiv \bm{x}_i \equiv \left( 1, x_{i1}, \dots, x_{ip} \right)'$, $\bm{\beta}(\bm{s}_i) \equiv \bm{\beta}_i \equiv \left(\beta_{i0}, \beta_{i1}, \dots, \beta_{ip} \right)'$, $y(\bm{s}_i) \equiv y_i$, and $\sigma^2(\bm{s}_i) \equiv \sigma^2_i$. Further, let $\bm{X} = \left( \bm{x}_1, \dots, \bm{x}_n \right)'$ and $\bm{y} = \left( y_1, \dots, y_n \right)'$. Equations (\ref{eq:lm(s)}) and (\ref{eq:err}) can now be rewritten as
	\begin{align}
		y_i = \bm{x}'_i \bm{\beta}_i + \varepsilon_i \text{ and } \varepsilon_i \overset{indep}{\sim} \mathcal{N} \left( 0,\sigma_i^2 \right)
	\end{align}
	
	Thus, given the design matrix $\bm{X}$, observations of the response variable at different locations are independent of each other. Then the total log-likelihood of the observed data is the sum of the log-likelihood of each individual observation.
	 \begin{align}
	 	\ell\left( \bm{\beta} \right) = - \left(1/2\right) \sum_{i=1}^n \left\{  \log \left( 2 \pi \sigma^2_i\right) +  \left(\sigma^{2}_i\right)^{-1}  \left(y_i - \bm{x}'_i\bm{\beta}_i \right)^2  \right\}
	\end{align}
	
	Since there are a total of $n \times (p+1)$ free parameters for $n$ observations the model is not identifiable, so it is not possible to directly maximize the total likelihood. One way to effectively reduce the number of parameters is to assume that the coefficients $\bm{\beta}(\bm{s})$ are smoothly varying over space, and use a kernel smoother to make pointwise estimates of the coefficients by maximizing the local likelihood. In the setting of spatial data and with the kernel smoother based on the physical distance between observation locations, this is the traditional GWR.
		
	\subsection{Estimation}		
	In geographically weighted regression, the coefficient surface $\bm{\beta}(\bm{s})$ is estimated at each sampling location $\bm{s}_i$. First calculate the Euclidean distance $\delta_{ii'} \equiv \delta\left(\bm{s}_i, \bm{s}_{i'}\right) \equiv \|\bm{s}_i  -\bm{s}_{i'}\|_2$ between locations $\bm{s}_i$ and $\bm{s}_{i'}$ for all $i, i'$. The bi-square kernel can be used to generate spatial weights based on the Euclidean distances and a bandwidth $\phi$. The bisquare kernel assigns the maximum weight of one where $\bm{s}_i = \bm{s}_{i'}$ so $\delta_{ii'}=0$, discontinuously differentiable, and assigns zero weight to observations at distances greater than one bandwidth from $\bm{s}_i$:
	
	\begin{align}\label{eq:bisquare}
		w_{ii'} = \begin{cases} \left[1-\left(\phi^{-1}\delta_{ii'}\right)^2\right]^2 &\mbox{ if } \delta_{ii'} < \phi \\ 0 &\mbox{ if } \delta_{ii'} \geq \phi \end{cases}
	\end{align}
	
	For the purpose of estimation, define the local likelihood at each location \citep{Fotheringham:2002}:
	\begin{align}\label{eq:local-likelihood}
		\mathcal{L}_i \left(\bm{\beta}_i \right) &= \prod_{i'=1}^n \left[ \left(2 \pi \sigma^2_i  \right)^{-1/2}  \exp\left\{-\left(2 \sigma^2_i\right)^{-1}  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right\} \right] ^ {w_{ii'}}
	\end{align}
			
	Thus, the local log-likelihood function is:
	\begin{align}\label{eq:local-log-likelihood}
		\ell_i\left(\bm{\beta}_i\right) &\propto -(1/2) \sum_{i'=1}^n w_{ii'} \left\{ \log{\sigma^2_i}  + \left(\sigma^2_i\right)^{-1}  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right\}
	\end{align}
	
	The GWR coefficient estimates $\hat{\bm{\beta}}_{i,\text{GWR}}$ maximize the local likelihood at location $\bm{s}_i$. From (\ref{eq:local-likelihood}) and (\ref{eq:local-log-likelihood}), it is apparent that $\hat{\bm{\beta}}_{i,\text{GWR}}$ can be calculated using weighted least squares. Let $\bm{W}_i$ denote a diagonal weight matrix with
	\begin{align}
		\bm{W}_i =  {\rm diag}\left\{w_{ii'}\right\}_{i' = 1}^n
	\end{align}
	
	Thus, it follows that
	\begin{align}
		\hat{\bm{\beta}}_{i, \text{GWR}} = \left( \bm{X}'\bm{W}_i\bm{X} \right)^{-1} \bm{X}'\bm{W}_i\bm{y}
	\end{align}
	
	The estimate of $\sigma_i^2$ is attained by maximizing (\ref{eq:local-log-likelihood}). Thus,
	\begin{align}
		\hat{\sigma}^2_i &= \left(\bm{1}_n'\bm{w}_i \right)^{-1} \left(\bm{y}-\bm{X}\left(\bm{X}'\bm{W}_i\bm{X}\right)^{-1}\bm{X}'\bm{W}_i\bm{y}\right)' \bm{W}_i \left(\bm{y}-\bm{X}\left(\bm{X}'\bm{W}_i\bm{X}\right)^{-1}\bm{X}'\bm{W}_i\bm{y}\right) \notag \\
		&= \left(\bm{1}_n'\bm{w}_i \right)^{-1}  \left(\bm{y}-\hat{\bm{y}}\right)' \bm{W}_i \left(\bm{y}-\hat{\bm{y}}\right)
	\end{align}
	
	\begin{comment}
	Estimation of $\hat{\bm{\beta}}_i$ and $\hat{\sigma}_i$ is by maximum local likelihood, which is implemented by setting the derivatives of (\ref{eq:local-log-likelihood}) to zero:
	\begin{eqnarray}
		\left\{\frac{\partial \ell_i}{\partial \bm{\beta}_i} \right\}_j =   \sum_{i'=1}^n \left\{ x_{i'j} w_{ii'} \sigma^{-2}_i \left( y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right) \right\} \\
		\frac{\partial \ell_i}{\partial \sigma_i^2} \bigg|_{\hat{\beta}_i} &=& -\frac{1}{2} \sum_{i'=1}^n w_{ii'} \left\{ \left(\sigma_i^{2}\right)^{-1} - \left(\sigma_i^{2}\right)^{-2} \left( y_i - \bm{x}_i'\bm{\hat{\beta}}_i \right)^2 \right\} \\
		\hat{\sigma}_i^2 &=& \left(\sum_{i'=1}^n w_{ii'}\right)^{-1} \sum_{i'=1}^n w_{ii'} \left(y_i - \bm{x}_i'\hat{\bm{\beta}}_i\right)
	\end{eqnarray}
		
		\left\{\frac{\partial^2 \ell_i}{\partial \bm{\beta}_i \partial \bm{\beta}'_i} \right\}_{j,k} = -\sum_{i'=1}^n \left\{ x_{i'j} x_{i'k} w_{ii'} \sigma^{-2}_i \right\}
	\end{eqnarray}
	
	So the observed Fisher information in the locally weighted sample is
	\begin{align}
		\bm{\mathcal{J}}_i &=& \sigma^{-2}_i \left( \begin{array}{ccc} \sum_{i'=1}^n  w_{ii'} x^2_{i'1}   & \dots & \sum_{i'=1}^n w_{ii'} x_{i'1} x_{i'p}   \\ \vdots & \ddots & \vdots \\ \sum_{i'=1}^n  w_{ii'} x_{i'p} x_{i'1}    & \dots & \sum_{i'=1}^n  w_{ii'} x^2_{i'p}  \end{array} \right) \notag\\
		&=& \sigma^{-2}_i \sum_{i'=1}^n w_{ii'}\left( \begin{array}{ccc}  x^2_{i'1} & \dots & x_{i'1} x_{i'p} \\ \vdots & \ddots & \vdots \\ x_{i'p} x_{i'1} & \dots &  x^2_{i'p} \end{array} \right)\notag \\
		&=& \sigma^{-2}_i \sum_{i'=1}^n w_{ii'} \bm{x}_{i'} \bm{x}'_{i'}
	\end{align}	
	
	The form of the observed Fisher information suggests that the information in the data $\bm{x}_{i'}$ about the coefficients at location $s_i$ is proportional to the weight $w_{ii'}$.
	\end{comment}

	
	 
	
\section{Model Selection \label{section:model-selection}}
	\subsection{Variable Selection}	
	
	

	
	\subsubsection{Adaptive Lasso}
	The adaptive Lasso is an $\ell_1$ regularization method for variable selection in regression models \citep{Zou:2006}. Unlike the traditional Lasso \citep{Tibshirani:1996}, which applies an equal penalty $\lambda^*_i$ to each covariate in the local model at $\bm{s}_i$, the adaptive Lasso adjusts the penalty of each covariate based on the covariate's unpenalized local coefficient. Letting the vector of unpenalized local coefficients be $\bm{\gamma}_i$, the adaptive Lasso penalty for covariate $j$ at location $\bm{s}_i$ is $|\lambda_i / \gamma_{ij}|$, where $\lambda_i$ is a the local penalty that applies to all coefficients at location $\bm{s}_i$ and $\bm{\gamma}_i  =\left\{\gamma_{ij}\right\}$ is the vector of adaptive weights at location $\bm{s}_i$. Thus, objective minimized by GWR to fit the local model at $\bm{s}_i$ using the adaptive Lasso is
	\begin{align}\label{eq:adaptive-lasso-WLS}
		\mathcal{S}_i = \sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 +  \lambda_i \sum_{j=1}^p |\beta_{ij} / \gamma_{ij}|
	\end{align}
	where $\sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2$ is the weighted least squares objective minimized by traditional GWR, and $\lambda_i \sum_{j=1}^p |\beta_{ij} / \gamma_{ij}|$ is the adaptive Lasso penalty.
	
	To apply an adaptive Lasso to GWR, the design matrix $\bm{X}$ is first multiplied by $\bm{W}_i^{1/2}$, the diagonal matrix of geographic weights at $\bm{s}_i$. Since some of the weights $w_{ii'}$ may be zero, the matrix $\bm{W}_i^{1/2}\bm{X}$ is not of full rank. The matrices $\bm{Y}_i^*$, $\bm{X}_i^*$, and $\bm{W}_i^*$ are formed by dropping the rows of $\bm{X}$  and $\bm{W}_i$ that correspond to observations with zero weight in the regression model at location $\bm{s}_i$. Now, letting $\bm{U}_i^* = \bm{W}_i^{*1/2} \bm{X}_i^*$ and $\bm{V}_i^* = \bm{W}_i^{*1/2} \bm{Y}_i^*$, we seek to estimate the coefficients $\bm{\beta}_i$ of the regression model:	
	\begin{align}
		\bm{V}_i^* = \bm{U}_i^* \bm{\beta}_i + \bm{\varepsilon}
	\end{align}
	
	Each column of $\bm{U}_i^*$ is centered around zero and rescaled to have an $\ell_2$-norm of one. Let $\widetilde{\bm{U}}_i^*$ denote the centered-and-scaled version of $\bm{U}_i^*$. Now the adaptive weights $\bm{\gamma}_i^*$ are calculated via least squares:	
	\begin{align}\label{eq:adaptive-weights-regression}
		\bm{\gamma}_i = \left( \widetilde{\bm{U}}_i^{*'} \widetilde{\bm{U}}_i^* \right)^{-1} \widetilde{\bm{U}}_i^{*'} \bm{V}_i^*
	\end{align}
	
	For $j=1, \dots, p$, the $j$th column of $\tilde{\bm{U}}_i^*$ is multiplied by $\gamma_{ij}$, the $j^\text{th}$ element of $\bm{\gamma}_i$. Call this rescaled matrix $\widecheck{\bm{U}}_i^*$.
	
	Finally, the adaptive Lasso coefficient estimates minimizing (\ref{eq:adaptive-lasso-WLS}) at location $\bm{s}_i$ are found, either by using the \verb!lars! algorithm \citep{Efron:2004b} to model $\bm{V}_i^*$ as a function of $\widecheck{\bm{U}}_i^*$ or by using the \verb!glmnet! package to implement coordinate descent.


	\subsubsection{Adaptive Elastic Net}
	The adaptive elastic net combines the adaptive Lasso penalty with the ridge penalty \citep{Zou:2009}. Ridge regression is an $\ell_2$ regularization technique that differs from the Lasso in that the ridge penalty $\lambda^{\dagger}_i$ is applied to the sum of the squared local regression coefficients \citep{Hoerl:1970}. The ridge penalty is used to estimate coefficients in regression models with correlated covariates because it stabilizes the inversion of the covariance matrix, which robustifies the coefficient estimates \citep{Hastie:2009}. To implement the adaptive elastic net, the adaptive weights $\bm{\gamma}_i$ are calculated as for the adaptive Lasso, but there is an additional elastic net parameter $\alpha$ that controls the balance between the $\ell_1$ and $\ell_2$ penalties, so that the objective to be minimized is:

	\begin{align}
		\sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 + \alpha \lambda_i \sum_{j=1}^p |\beta_{ij} / \gamma_{ij}| + (1-\alpha) \lambda_i \sum_{j=1}^p  \left( \beta_{ij} / \gamma_{ij} \right)^2\notag\\
		= \sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 + \lambda_i \left(\alpha  \sum_{j=1}^p |\beta_{ij} / \gamma_{ij}| + (1-\alpha) \sum_{j=1}^p  \left[ \beta_{ij} / \gamma_{ij} \right]^2 \right)
	\end{align}
	
	In the simulation study (Section \ref{sec:simulation}), $\alpha$ is calculated from the maximum global (i.e. for all data without weighting) Pearson correlation between any two covariates, $\rho_{\text{max}}$: $\alpha = 1-\rho_{\text{max}}$.

	\subsection{Tuning Parameter Selection}	
	At each location $\bm{s}_i$, it is necessary to select the Lasso tuning parameter $\lambda_i$. To compare different values of $\lambda_i$, we propose a locally-weighted version of the Akaike information criterion (BIC) \citep{Akaike:1974} which we call the local BIC, or $\mbox{BIC}_{\text{loc}}$. The local BIC is calculated by adding a penalty to the local likelihood, with the sum of the weights around $\bm{s}_i$, $\sum_{i'=1}^n w_{ii'}$, playing the role of the sample size and the ``degrees of freedom" $\left( \df_i \right)$ at $\bm{s}_i$ given by the number of nonzero coefficients in $\bm{\beta}_i$ \citep{Zou:2007}.
	
	\begin{align}
		\mbox{BIC}_{\text{loc}, i} &= -2 \sum_{i'=1}^n \ell_{ii'}  + \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i\notag\\
		&= -2 \times \sum_{i'=1}^n \log \left\{ \left(2 \pi \hat{\sigma}_i^2\right)^{-1/2} \exp \left[-\frac{1}{2} \hat{\sigma}_i^{-2} \left(y_{i'} - \bm{x}_{i'}' \hat{\bm{\beta}}_{i'} \right)^2\right] \right\}^{w_{ii'}} + \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i\notag\\
		&= \sum_{i'=1}^n w_{ii'} \left\{ \log \left(2 \pi \right) + \log \hat{\sigma}_i^2 + \hat{\sigma}_i^{-2} \left(y_{i'} - \bm{x}_{i'}' \hat{\bm{\beta}}_{i'} \right)^2 \right\} + \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i \notag\\
		&= \hat{\sigma}_i^{-2} \sum_{i'=1}^n w_{ii'} \left( y_{i'} - \bm{x}'_{i'} \hat{\bm{\beta}}_i \right)^2 + \left( \sum_{i'=1}^n w_{ii'} \right) \mbox{df}_i + C_i
	\end{align}	
	
	Since the estimated local variance $\hat{\sigma}_i^2$ is the variance estimate from the unpenalized local model, $C_i$ does not depend on the choice of tuning parameter and can be ignored \citep{Zou:2007}.
	
	\cite{Wheeler:2009} proposed selecting the tuning parameter for the Lasso at location $\bm{s}_i$ to minimize the jackknife prediction error $|y_i - \hat{y}_i^{(i)}|$. Because the jackknife prediction error is undefined everywhere except for at observation locations, this choice restricts coefficient estimation to occur at the locations where data has been observed. By contrast, the local BIC can be calculated at any location where we can calculate the local likelihood. As a practical matter this allows for variable selection and coefficient surface estimation to be done at locations where no data was observed (interpolation) and for imputation of missing values of the response variable.
	 
	\subsection{Bandwidth Selection}
	The global bandwidth parameter $\phi$ in (\ref{eq:bisquare}) is selected by minimizing the total AIC:
	
	\begin{align}
		\text{AIC}_{\text{tot}} = 2 n \log{\sigma} + n \left\{\frac{n + \nu}{n - 2 - \nu}\right\}
	\end{align}
	
	where $\nu$ is the trace of the smoothing matrix $S$ \citep{Hurvich:1998}.
	
	%The bandwidth parameter $\phi$ in (\ref{eq:bisquare}) is global and so a global statistic is needed, by which prospective bandwidths can be compared. We propose the following statistic, called the total BIC ($\mbox{BIC}_{\text{tot}}$):
	%\begin{align}\label{eq:total-BIC}
	%	\mbox{BIC}_{\mbox{tot}} &= \sum_{i=1}^n \left\{ \log \hat{\sigma}_i^2 + \hat{\sigma}_i^{-2} \left(y_i - \bm{x}'_i \hat{\bm{\beta}}_i \right)^2 + \left( \sum_{i'=1}^n w_{ii'}\right) \left(\sum_{i'=1}^n w_{ii'} \right)^{-1} \mbox{df}_i \right\}
	%\end{align}
	%which is different than the BIC for traditional GWR \citep{Fotheringham:2002}. The BIC for traditional GWR is based on the trace of the projection matrix of the GWR model. But using an $\ell_1$ penalty (as in the adaptive Lasso or the adaptive elastic net) results in a non-linear smoother, as can be seen by equating the derivatives of (\ref{eq:adaptive-lasso-WLS}) with respect to $\bm{\beta}$ to zero to obtain the maximum likelihood estimates $\hat{\bm{\beta}}_i$ \citep{Zou:2007}.
	%\begin{align}
	%	\hat{\bm{\beta}}_i &= \left( \bm{X}'\bm{W}_i\bm{X} \right)^{-1}  \bm{X}'\bm{W}_i\bm{y}  - (1/2) \left(\bm{X}'\bm{W}_i\bm{X} \right)^{-1} \bm{\lambda}_i \notag \\
	%	\hat{y}_i = \bm{x}_i' \hat{\bm{\beta}}_i &=  \bm{x}_i' \left( \bm{X}'\bm{W}_i\bm{X} \right)^{-1}  \bm{X}'\bm{W}_i\bm{y}  - (1/2) \bm{x}_i' \left(\bm{X}'\bm{W}_i\bm{X} \right)^{-1} \bm{\lambda}_i \notag
	%\end{align}
	%where $\bm{\lambda}_i$ is a vector of adaptive Lasso penalties, the $j^{\text{th}}$ component of which is $|\lambda_i / \gamma_{ij}|$.
	
	%Because of the kernel weights and the application of the adaptive Lasso, the sample size and the degrees of freedom are different for each local model. The total BIC is found by summing the local likelihood and local penalty over all of the locations $\bm{s}_i$. The local penalty requires that we calculate $\text{df}_{\text{tot}}$, the total degrees of freedom used by the local models.
	
	%The expression for $\text{df}_{\text{tot}}$ is derived by analogy. It is certainly true that $\text{df}_{\text{tot}} = \sum\limits_{i=1}^n \left( n^{-1} \text{df}_{\text{tot}} \right)$, which suggests that the total degrees of freedom is the sum of the local degrees of freedom. Rather than using the mean local degrees of freedom, $n^{-1} \text{df}_{\text{tot}}$ at each location, substitute $\text{df}_i$ for $\text{df}_{\text{tot}}$ and $\sum\limits_{i'=1}^n w_{ii'}$ for $n$ to get 
	%\begin{align}
	%	\text{df}_{\text{tot}} &= \sum\limits_{i=1}^n \text{df}_i  \left( \sum\limits_{i'=1}^n w_{ii'} \right)^{-1}
	%\end{align}

\section{Simulation \label{sec:simulation}}
	\subsection{Simulation setup}
	A simulation study was conducted to assess the performance of the method described in Sections \ref{section:GWR}--\ref{section:model-selection}. 
	
	Data was simulated on $[0,1] \times [0,1]$, which was divided into a $30 \times 30$ grid. Each of $p=5$ covariates $X_1, \dots, X_5$ was simulated by a Gaussian random field (GRF) with mean zero and exponential spatial covariance $Cov \left(X_{ji}, X_{ji'} \right) = \sigma_x^2 \exp{\left( -\tau_x^{-1} \delta_{ii'} \right)}$ where $\sigma_x^2=1$ is the variance, $\tau_x = 0$ is the range parameter, and $\delta_{ii'}$ is the Euclidean distance $\|\bm{s}_i - \bm{s}_{i'}\|_2$. Correlation was induced between the covariates by multiplying the $\bm{X}$ matrix by $\bm{R}$, where $\bm{R}$ is the Cholesky decomposition of the covariance matrix $\Sigma = \bm{R}'\bm{R}$. The covariance matrix $\bm{\Sigma}$ is a $5 \times 5$ matrix that has ones on the diagonal and $\rho$ for all off-diagonal entries, where $\rho$ is the between-covariate correlation.
		
	The simulated response is $y_i = \bm{x}'_i \bm{\beta}_i + \varepsilon_i$ for $i=1, \dots, n$ where $n=900$ and for simplicity the $\varepsilon_i$'s were iid Gaussian with mean zero and variance $\sigma_\varepsilon^2$.%is generated from a GRF with spatial covariance $Cov \left(\varepsilon_{i}, \varepsilon_{i'} \right) = \sigma_{\varepsilon}^2 \exp{\left( -\tau_{\varepsilon}^{-1} \delta_{ii'} \right)}$ where $\sigma_{\varepsilon}^2=1$.
	
	The simulated data include the output $y$ and five covariates $X_1, \dots, X_5$. The true data-generating model uses only $X_1$, so $X_2, \dots, X_5$ are included to assess performance in variable-selection.
	
	There were twelve simulation settings, each of which was simulated 100 times. For each of the twelve settings, $\bm{\beta}_1(\bm{s})$, the true coefficient surface for $\bm{X}_1$, was nonzero in at least part of the simulation domain. There were four other simulated covariates, but their true coefficient surfaces were zero across the area under simulation. The twelve simulation settings are described in Table \ref{table:simulation_settings}. Three parameters were varied to produce the twelve settings: there were three functional forms for the coefficient surface $\beta_1(\bm{s})$ (step, gradient, and parabola - see Figure \ref{fig:sim-actual}); data was simulated both with ($\rho = 0.5$) and without ($\rho = 0$) correlation between the covariates; and simulations were made with low ($\sigma_\varepsilon^2 = 0.25$) and high ($\sigma_\varepsilon^2 = 1$) variance for the random noise term.\\
	
	The performance of the penalized GWR methods (adaptive Lasso via \verb!lars! and via \verb!glmnet!, and the adaptive elastic net (\verb!enet!) was compared to that of oracular GWR (O-GWR), which is ordinary GWR with ``oracular" variable selection, meaning that exactly the correct set of predictors was used to fit the GWR model at each location in the simulation. Also included in the comparison was the GWR algorithm of \cite{Fotheringham:2002} without variable selection (\verb!gwr!). Finally, there is a category of simulation results using the three penalized GWR methods for local variable selection and then ordinary GWR for coefficient estimation.\\
	
% latex table generated in R 2.15.1 by xtable 1.7-0 package
% Fri Jan 18 10:19:47 2013
\begin{table}[h!]
	\begin{center}
	\begin{tabular}{cccc}
		\hline
		Setting & function & $\rho$ & $\sigma^2$ \\ 
		\hline
		1 & step & 0 & 0.25 \\ 
		2 & step & 0 & 1 \\ 
		3 & step & 0.5 & 0.25 \\ 
		4 & step & 0.5 & 1 \\ 
		\hline
		5 & gradient & 0 & 0.25 \\ 
		6 & gradient & 0 & 1 \\ 
		7 & gradient & 0.5 & 0.25 \\ 
		8 & gradient & 0.5 & 1 \\ 
		\hline
		9 & parabola & 0 & 0.25 \\ 
		10 & parabola & 0 & 1 \\ 
		11 & parabola & 0.5 & 0.25 \\ 
		12 & parabola & 0.5 & 1 
	\end{tabular}
	\end{center}
	\caption{Simulation parameters for each setting.\label{table:simulation_settings}}
\end{table}

	\begin{figure}
		\begin{center}
			\includegraphics[width=0.32\textwidth]{../../figures/simulation/step.pdf}
			\includegraphics[width=0.32\textwidth]{../../figures/simulation/gradient.pdf}
			\includegraphics[width=0.32\textwidth]{../../figures/simulation/parabola.pdf}
			\caption{The actual $\beta_1$ coefficient surface used in the simulation.\label{fig:sim-actual}}
		\end{center}
	\end{figure}
	
	\begin{figure}
		\begin{center}
			\includegraphics[width=0.5\textwidth]{../../figures/simulation/illustrations/summary-locations.pdf}
			\caption{Locations where the variable selection and coefficient estimation of GWL were summarized.\label{fig:summary-locations}}
		\end{center}
	\end{figure}
	
	
	Results from the simulation were summarized at five locations on the simulated grid (see Figure \ref{fig:summary-locations}). The five key locations were chosen because they represent interesting regions of the $\beta_1$ coefficient surfaces. The results of variable selection  and coefficient estimation  are presented in the tables below.\\	
	Selection: Tables \ref{table:selection}\\
	MSE of $\hat{\beta}_1(s_i)$ ($i = 1, \dots, 5$): Tables \ref{table:X1-mse}\\
	Bias of $\hat{\beta}_1(s_i)$ ($i = 1, \dots, 5$): Tables \ref{table:X1-bias}\\
	Variance of $\hat{\beta}_1(s_i)$ ($i = 1, \dots, 5$): Tables \ref{table:X1-var}\\
	MSE of $\hat{Y}(s_i)$ ($i = 1, \dots, 5$): Tables \ref{table:Y-mse}\\
	
	%Results of the simulation experiment were summarized to asses the consistency in selection and estimation, as well as the coverage properties of the confidence intervals. The confidence intervals based on the bootstrap (without shrinkage) were used for the GWL because they seemed to uniformly outperform the other options.\\ table:loc1-X1-BiasX
	
		
	\subsection{Results}
	%At locations where $\beta_1$ is nonzero, $X_1$ usually selected for inclusion in all or nearly all of the model runs. An exception is at location four for the step function, where $X_1$ was included in about half of the model runs. This is probably because location four is at the very point where $\beta_1$ transitions from zero to nonzero. Selection performance was relatively poor for the step function at location one, especially for data with $\sigma^2 = 1$. For those simulations, $X_1$ was correctly included in around 85\% of the simulations. The bias, variance, and MSE of $\hat{\beta}_1$ under the same settings were also much larger than the baseline established by the standard \verb!gwr! algorithm. The reason(s) for the poor performance under those particular conditions is currently unknown.\\
	
	
	
	%Otherwise, selection performance was good, with the rate of false positive selections for $X_2$--$X_5$ (and for $X_1$ where its true coefficient was zero) usually below 0.10. Selection (also bias, variance, and MSE of $\hat{\beta}_1(\bm{s})$) tended to suffer worse by the change from low to high error variance than by the change from low to high collinearity amongst the predictors.\\
	
	
	
	%There was not a clear and consistent difference in performance between the three selection methods. It might be expected that the adaptive elastic net would outperform the adaptive Lasso under greater covariate collinearity, but if such effect is real it is not apparent from this simulation. The unshrunk coefficient-estimation methods tended to exhibit more bias than the selection-plus-shrinkage methods when the true coefficient value was near zero, and vice versa when the true coefficient was not near zero. The unshrunk methods were perhaps more consistent in their performance and for that reason they are probably preferable in practice.\\
	
	In terms of MSE and bias, the estimates of $\beta_i$ using local variable selection outperformed GWR without selection, and 
	
	%Bias in coefficient estimation was greater and variance less for the standard \verb!gwr! algorithm than for the methods described here. This is probably due to the fact that the methods described here show a preference for smaller bandwidths than those select by \verb!gwr!. Accuracy (as measured by MSE) in fitting the true Y variables was comparable for all the methods. \\	
	
	\subsection{Tables}
		\subsubsection{Selection}
		% latex table generated in R 2.15.2 by xtable 1.7-0 package
		% Thu Apr 18 16:14:00 2013		
		\begin{sidewaystable}
		\thispagestyle{empty}
		\begin{center}
		\resizebox{22cm}{!}{
		\setlength\tabcolsep{4pt}	
		\begin{tabular}{ccc|cc|cc|cc|cc|cc|cc|cc|cc}
		& \multicolumn{6}{c}{step} & \multicolumn{6}{|c|}{gradient} & \multicolumn{6}{c}{parabola} \\
		& \multicolumn{2}{c}{\texttt{lars}} & \multicolumn{2}{c}{\texttt{enet}} & \multicolumn{2}{c|}{\texttt{glmnet}} & \multicolumn{2}{c}{\texttt{lars}} & \multicolumn{2}{c}{\texttt{enet}} & \multicolumn{2}{c|}{\texttt{glmnet}} & \multicolumn{2}{c}{\texttt{lars}} & \multicolumn{2}{c}{\texttt{enet}} & \multicolumn{2}{c}{\texttt{glmnet}} \\
		location & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ & $\beta_1$ & $\beta_2$ - $\beta_5$ \\ 
   \hline
   \multirow{4}{*}{1} & 1.00 & 0.14 & 1.00 & 0.00 & 1.00 & 0.00 & 1.00 & 0.02 & 1.00 & 0.00 & 1.00 & 0.00 & 0.73 & 0.08 & 0.30 & 0.00 & 0.30 & 0.00 \\ 
   & 1.00 & 0.16 & 0.90 & 0.06 & 0.90 & 0.06 & 1.00 & 0.02 & 1.00 & 0.02 & 1.00 & 0.02 & 0.72 & 0.05 & 0.63 & 0.02 & 0.62 & 0.02 \\ 
   & 1.00 & 0.19 & 1.00 & 0.00 & 1.00 & 0.00 & 1.00 & 0.02 & 1.00 & 0.00 & 1.00 & 0.00 & 0.73 & 0.08 & 0.29 & 0.00 & 0.34 & 0.00 \\ 
   & 0.99 & 0.11 & 0.91 & 0.08 & 0.90 & 0.06 & 0.99 & 0.04 & 0.97 & 0.04 & 0.95 & 0.03 & 0.62 & 0.08 & 0.53 & 0.03 & 0.53 & 0.02 \\ 
   \cline{2-19}
   \multirow{4}{*}{2} & 1.00 & 0.06 & 1.00 & 0.00 & 1.00 & 0.00 & 1.00 & 0.01 & 1.00 & 0.00 & 1.00 & 0.00 & 1.00 & 0.03 & 1.00 & 0.00 & 1.00 & 0.00 \\ 
   & 1.00 & 0.06 & 0.97 & 0.03 & 0.97 & 0.02 & 1.00 & 0.02 & 1.00 & 0.01 & 1.00 & 0.01 & 0.99 & 0.02 & 0.97 & 0.01 & 0.97 & 0.01 \\ 
   & 1.00 & 0.04 & 1.00 & 0.00 & 1.00 & 0.00 & 1.00 & 0.02 & 1.00 & 0.00 & 1.00 & 0.00 & 1.00 & 0.02 & 1.00 & 0.00 & 1.00 & 0.00 \\ 
   & 1.00 & 0.01 & 0.98 & 0.04 & 0.97 & 0.02 & 1.00 & 0.03 & 0.96 & 0.02 & 0.94 & 0.02 & 0.95 & 0.03 & 0.97 & 0.01 & 0.95 & 0.02 \\ 
   \cline{2-19}
   \multirow{4}{*}{3} & 1.00 & 0.04 & 1.00 & 0.00 & 1.00 & 0.00 & 1.00 & 0.02 & 1.00 & 0.00 & 1.00 & 0.00 & 1.00 & 0.01 & 1.00 & 0.00 & 1.00 & 0.00 \\ 
   & 0.94 & 0.05 & 0.74 & 0.02 & 0.75 & 0.02 & 1.00 & 0.01 & 0.98 & 0.00 & 0.98 & 0.00 & 1.00 & 0.02 & 0.97 & 0.00 & 0.97 & 0.00 \\ 
   & 0.99 & 0.04 & 0.98 & 0.00 & 0.99 & 0.00 & 1.00 & 0.01 & 1.00 & 0.00 & 1.00 & 0.00 & 1.00 & 0.01 & 1.00 & 0.00 & 1.00 & 0.00 \\ 
   & 0.87 & 0.05 & 0.68 & 0.04 & 0.68 & 0.04 & 0.97 & 0.01 & 0.91 & 0.02 & 0.93 & 0.01 & 0.97 & 0.04 & 0.99 & 0.02 & 0.99 & 0.01 \\ 
   \cline{2-19}
   \multirow{4}{*}{4} & 0.72 & 0.04 & 0.39 & 0.00 & 0.40 & 0.00 & 1.00 & 0.01 & 1.00 & 0.00 & 1.00 & 0.00 & 1.00 & 0.02 & 1.00 & 0.00 & 1.00 & 0.00 \\ 
   & 0.47 & 0.03 & 0.37 & 0.01 & 0.38 & 0.01 & 1.00 & 0.01 & 0.97 & 0.01 & 0.97 & 0.01 & 0.98 & 0.03 & 0.95 & 0.00 & 0.95 & 0.00 \\ 
   & 0.58 & 0.07 & 0.34 & 0.00 & 0.35 & 0.00 & 1.00 & 0.01 & 1.00 & 0.00 & 1.00 & 0.00 & 1.00 & 0.01 & 1.00 & 0.00 & 1.00 & 0.00 \\ 
   & 0.51 & 0.06 & 0.43 & 0.03 & 0.37 & 0.04 & 0.97 & 0.02 & 0.93 & 0.02 & 0.92 & 0.02 & 0.96 & 0.05 & 0.98 & 0.02 & 0.94 & 0.02 \\ 
   \cline{2-19}
   \multirow{4}{*}{5} & 0.17 & 0.14 & 0.00 & 0.00 & 0.00 & 0.00 & 0.93 & 0.03 & 0.72 & 0.00 & 0.72 & 0.00 & 0.75 & 0.10 & 0.29 & 0.00 & 0.28 & 0.00 \\ 
   & 0.15 & 0.14 & 0.05 & 0.04 & 0.05 & 0.06 & 0.72 & 0.05 & 0.62 & 0.01 & 0.65 & 0.01 & 0.73 & 0.04 & 0.59 & 0.00 & 0.54 & 0.01 \\ 
   & 0.14 & 0.22 & 0.00 & 0.00 & 0.00 & 0.00 & 0.89 & 0.04 & 0.76 & 0.00 & 0.79 & 0.00 & 0.73 & 0.09 & 0.28 & 0.00 & 0.30 & 0.00 \\ 
   & 0.14 & 0.11 & 0.03 & 0.03 & 0.05 & 0.04 & 0.71 & 0.05 & 0.59 & 0.02 & 0.61 & 0.02 & 0.61 & 0.08 & 0.51 & 0.03 & 0.51 & 0.04 \\ 
  \end{tabular}}
		\caption{Selection frequency for the simulation experiment\label{table:selection}}
		\end{center}
		\end{sidewaystable}
		


		\subsubsection{Estimation}
		% latex table generated in R 2.15.2 by xtable 1.7-0 package
		% Thu Apr 18 14:39:33 2013
		%\begin{table}[ht]
		\begin{table}
		\thispagestyle{empty}
		\begin{center}
		\resizebox{!}{12cm}{
		\begin{tabular}{cccccccccc}
		  function & location & lars & enet & glmnet & u.lars & u.enet & u.glmnet & oracular & gwr \\ 
  \hline
  \multirow{20}{*}{step} & \multirow{4}{*}{1} & 0.016 & \emph{0.009} & \textbf{0.009} & 0.077 & 0.055 & 0.055 & 0.059 & 0.111 \\ 
   &  & \textbf{0.048} & 0.150 & 0.150 & 0.267 & 0.566 & 0.584 & \emph{0.063} & 0.110 \\ 
   &  & 0.025 & \emph{0.021} & \textbf{0.012} & 0.134 & 0.072 & 0.068 & 0.054 & 0.114 \\ 
   &  & \emph{0.078} & 0.182 & 0.171 & 0.294 & 0.434 & 0.439 & \textbf{0.064} & 0.112 \\ 
  \cline{3-10}
   & \multirow{4}{*}{2} & \emph{0.025} & 0.036 & 0.036 & \textbf{0.021} & 0.032 & 0.032 & 0.092 & 0.166 \\ 
   &  & \emph{0.056} & 0.085 & 0.082 & \textbf{0.044} & 0.069 & 0.069 & 0.093 & 0.165 \\ 
   &  & \emph{0.024} & 0.040 & 0.034 & \textbf{0.019} & 0.029 & 0.028 & 0.094 & 0.169 \\ 
   &  & \emph{0.065} & 0.095 & 0.101 & \textbf{0.048} & 0.071 & 0.081 & 0.091 & 0.164 \\ 
  \cline{3-10}
   & \multirow{4}{*}{3} & 0.007 & 0.005 & 0.005 & 0.004 & \emph{0.003} & \textbf{0.003} & 0.004 & 0.007 \\ 
   &  & 0.021 & 0.056 & 0.055 & 0.023 & 0.064 & 0.063 & \textbf{0.005} & \emph{0.009} \\ 
   &  & 0.009 & 0.008 & 0.006 & 0.007 & 0.007 & \emph{0.005} & \textbf{0.004} & 0.007 \\ 
   &  & 0.033 & 0.064 & 0.066 & 0.033 & 0.063 & 0.069 & \textbf{0.006} & \emph{0.010} \\ 
  \cline{3-10}
   & \multirow{4}{*}{4} & \textbf{0.016} & \emph{0.019} & 0.020 & 0.021 & 0.022 & 0.022 & 0.091 & 0.166 \\ 
   &  & 0.035 & \emph{0.033} & \textbf{0.032} & 0.041 & 0.038 & 0.037 & 0.094 & 0.169 \\ 
   &  & \textbf{0.014} & \emph{0.016} & 0.019 & 0.018 & 0.020 & 0.020 & 0.095 & 0.166 \\ 
   &  & 0.035 & \textbf{0.027} & \emph{0.032} & 0.042 & 0.039 & 0.034 & 0.094 & 0.171 \\ 
  \cline{3-10}
   & \multirow{4}{*}{5} & 0.005 & \textbf{0.000} & \textbf{0.000} & 0.031 & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & 0.110 \\ 
   &  & 0.021 & 0.012 & \emph{0.012} & 0.104 & 0.064 & 0.061 & \textbf{0.000} & 0.112 \\ 
   &  & 0.007 & \textbf{0.000} & \textbf{0.000} & 0.049 & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & 0.112 \\ 
   &  & 0.020 & \emph{0.008} & 0.012 & 0.129 & 0.030 & 0.040 & \textbf{0.000} & 0.117 \\ 
  \hline
  \multirow{20}{*}{gradient} & \multirow{4}{*}{1} & 0.113 & 0.090 & 0.090 & 0.007 & \emph{0.006} & 0.006 & \textbf{0.003} & 0.147 \\ 
   &  & 0.103 & 0.100 & 0.101 & \emph{0.045} & 0.083 & 0.088 & \textbf{0.010} & 0.146 \\ 
   &  & 0.108 & 0.099 & 0.100 & 0.013 & 0.004 & \emph{0.004} & \textbf{0.002} & 0.146 \\ 
   &  & 0.117 & 0.138 & 0.143 & \emph{0.079} & 0.165 & 0.136 & \textbf{0.009} & 0.153 \\ 
  \cline{3-10}
   & \multirow{4}{*}{2} & 0.005 & 0.003 & 0.003 & 0.001 & \emph{0.001} & 0.001 & \textbf{0.001} & 0.005 \\ 
   &  & 0.005 & 0.005 & 0.005 & \emph{0.003} & 0.003 & 0.003 & \textbf{0.002} & 0.005 \\ 
   &  & 0.004 & 0.004 & 0.004 & 0.001 & 0.001 & \emph{0.001} & \textbf{0.001} & 0.005 \\ 
   &  & 0.013 & 0.022 & 0.027 & 0.008 & 0.019 & 0.024 & \textbf{0.002} & \emph{0.007} \\ 
  \cline{3-10}
   & \multirow{4}{*}{3} & 0.001 & 0.001 & 0.001 & 0.001 & 0.000 & \emph{0.000} & \textbf{0.000} & 0.001 \\ 
   &  & 0.003 & 0.009 & 0.009 & 0.002 & 0.011 & 0.011 & \textbf{0.001} & \emph{0.002} \\ 
   &  & 0.001 & 0.001 & 0.000 & 0.001 & 0.000 & \emph{0.000} & \textbf{0.000} & 0.001 \\ 
   &  & 0.013 & 0.024 & 0.021 & 0.011 & 0.024 & 0.020 & \textbf{0.001} & \emph{0.002} \\ 
  \cline{3-10}
   & \multirow{4}{*}{4} & 0.004 & 0.003 & 0.003 & 0.001 & \emph{0.001} & 0.001 & \textbf{0.001} & 0.005 \\ 
   &  & 0.007 & 0.010 & 0.010 & \emph{0.003} & 0.007 & 0.007 & \textbf{0.002} & 0.007 \\ 
   &  & 0.004 & 0.003 & 0.003 & 0.002 & 0.001 & \emph{0.001} & \textbf{0.001} & 0.006 \\ 
   &  & 0.013 & 0.018 & 0.020 & 0.010 & 0.018 & 0.018 & \textbf{0.002} & \emph{0.007} \\ 
  \cline{3-10}
   & \multirow{4}{*}{5} & 0.112 & 0.079 & 0.079 & 0.005 & \emph{0.003} & 0.003 & \textbf{0.000} & 0.151 \\ 
   &  & 0.091 & 0.071 & 0.073 & 0.018 & \emph{0.012} & 0.012 & \textbf{0.000} & 0.152 \\ 
   &  & 0.102 & 0.079 & 0.087 & 0.006 & \emph{0.004} & 0.004 & \textbf{0.000} & 0.148 \\ 
   &  & 0.094 & 0.056 & 0.069 & 0.067 & \emph{0.012} & 0.015 & \textbf{0.000} & 0.153 \\ 
  \hline
  \multirow{20}{*}{parabola} & \multirow{4}{*}{1} & 0.033 & 0.019 & 0.019 & 0.026 & \textbf{0.009} & \emph{0.009} & 0.059 & 0.112 \\ 
   &  & 0.076 & 0.057 & 0.057 & 0.142 & \textbf{0.047} & \emph{0.049} & 0.067 & 0.114 \\ 
   &  & 0.032 & 0.015 & 0.020 & 0.032 & \textbf{0.008} & \emph{0.008} & 0.055 & 0.107 \\ 
   &  & 0.079 & \textbf{0.035} & 0.041 & 0.180 & \emph{0.039} & 0.042 & 0.075 & 0.115 \\ 
  \cline{3-10}
   & \multirow{4}{*}{2} & 0.004 & \emph{0.003} & \textbf{0.003} & 0.004 & 0.003 & 0.003 & 0.010 & 0.009 \\ 
   &  & 0.014 & 0.018 & 0.018 & 0.015 & 0.023 & 0.022 & \emph{0.012} & \textbf{0.010} \\ 
   &  & 0.004 & 0.005 & \textbf{0.004} & 0.005 & 0.004 & \emph{0.004} & 0.012 & 0.011 \\ 
   &  & 0.022 & 0.017 & 0.022 & 0.024 & 0.015 & 0.025 & \emph{0.011} & \textbf{0.011} \\ 
  \cline{3-10}
   & \multirow{4}{*}{3} & 0.005 & 0.005 & 0.005 & 0.005 & \emph{0.004} & \textbf{0.004} & 0.027 & 0.027 \\ 
   &  & \emph{0.022} & 0.025 & 0.024 & \textbf{0.021} & 0.024 & 0.023 & 0.027 & 0.028 \\ 
   &  & 0.007 & 0.007 & 0.006 & 0.006 & \emph{0.005} & \textbf{0.005} & 0.028 & 0.030 \\ 
   &  & 0.029 & 0.021 & \emph{0.018} & 0.027 & \textbf{0.015} & 0.022 & 0.027 & 0.029 \\ 
  \cline{3-10}
   & \multirow{4}{*}{4} & 0.004 & 0.004 & \textbf{0.004} & 0.004 & 0.004 & \emph{0.004} & 0.014 & 0.011 \\ 
   &  & 0.016 & 0.021 & 0.020 & 0.017 & 0.021 & 0.021 & \emph{0.015} & \textbf{0.011} \\ 
   &  & 0.005 & 0.006 & \textbf{0.004} & 0.005 & 0.005 & \emph{0.004} & 0.014 & 0.012 \\ 
   &  & 0.022 & 0.017 & 0.027 & 0.026 & \emph{0.015} & 0.030 & 0.015 & \textbf{0.012} \\ 
  \cline{3-10}
   & \multirow{4}{*}{5} & 0.031 & 0.018 & 0.018 & 0.019 & \textbf{0.008} & \emph{0.009} & 0.058 & 0.111 \\ 
   &  & 0.060 & 0.044 & 0.042 & 0.091 & \emph{0.024} & \textbf{0.022} & 0.060 & 0.112 \\ 
   &  & 0.035 & 0.014 & 0.019 & 0.039 & \textbf{0.009} & \emph{0.009} & 0.056 & 0.107 \\ 
   &  & 0.062 & 0.030 & 0.040 & 0.079 & \emph{0.024} & \textbf{0.020} & 0.064 & 0.116 \\ 
  \end{tabular}}
\caption{Mean squared error of $\hat{\beta_1}$ (\textbf{minimum}, \emph{next best}).\label{table:X1-mse}}
		\end{center}
		\end{table}	
 
 % latex table generated in R 2.15.2 by xtable 1.7-0 package
		% Thu Apr 18 14:39:33 2013
		\begin{table}
		\thispagestyle{empty}
		\begin{center}
		\resizebox{!}{12cm}{
		\begin{tabular}{cccccccccc}
		  function & location & lars & enet & glmnet & u.lars & u.enet & u.glmnet & oracular & gwr \\ 
  \hline
  \multirow{20}{*}{step} & \multirow{4}{*}{1} & -0.027 & -0.024 & \emph{-0.022} & \textbf{-0.001} & 0.024 & 0.025 & 0.237 & -0.333 \\ 
   &  & -0.080 & -0.166 & -0.164 & \textbf{0.006} & -0.073 & \emph{-0.073} & 0.231 & -0.329 \\ 
   &  & \textbf{-0.029} & -0.070 & -0.038 & 0.037 & \emph{0.030} & 0.032 & 0.225 & -0.335 \\ 
   &  & -0.055 & -0.163 & -0.146 & \emph{0.027} & -0.042 & \textbf{0.007} & 0.231 & -0.330 \\ 
  \cline{3-10}
   & \multirow{4}{*}{2} & \emph{-0.133} & -0.175 & -0.174 & \textbf{-0.122} & -0.166 & -0.167 & -0.302 & -0.407 \\ 
   &  & -0.179 & -0.201 & -0.190 & \emph{-0.149} & -0.153 & \textbf{-0.149} & -0.301 & -0.404 \\ 
   &  & \emph{-0.125} & -0.185 & -0.169 & \textbf{-0.110} & -0.155 & -0.154 & -0.305 & -0.409 \\ 
   &  & -0.212 & -0.246 & -0.248 & \textbf{-0.175} & \emph{-0.185} & -0.202 & -0.297 & -0.402 \\ 
  \cline{3-10}
   & \multirow{4}{*}{3} & \textbf{-0.003} & \emph{0.010} & 0.012 & 0.019 & 0.029 & 0.029 & 0.060 & 0.081 \\ 
   &  & \textbf{-0.006} & -0.080 & -0.075 & 0.039 & -0.036 & \emph{-0.030} & 0.063 & 0.084 \\ 
   &  & -0.021 & -0.025 & \textbf{0.004} & \emph{0.006} & 0.021 & 0.024 & 0.062 & 0.080 \\ 
   &  & \emph{-0.056} & -0.130 & -0.110 & \textbf{-0.031} & -0.098 & -0.092 & 0.064 & 0.086 \\ 
  \cline{3-10}
   & \multirow{4}{*}{4} & 0.074 & \textbf{0.035} & \emph{0.039} & 0.093 & 0.042 & 0.045 & 0.300 & 0.406 \\ 
   &  & 0.079 & \emph{0.055} & \textbf{0.055} & 0.094 & 0.066 & 0.067 & 0.303 & 0.408 \\ 
   &  & 0.046 & \textbf{0.017} & \emph{0.027} & 0.062 & 0.030 & 0.031 & 0.307 & 0.406 \\ 
   &  & 0.087 & \emph{0.054} & \textbf{0.054} & 0.101 & 0.076 & 0.061 & 0.303 & 0.411 \\ 
  \cline{3-10}
   & \multirow{4}{*}{5} & -0.005 & \textbf{0.000} & \textbf{0.000} & -0.002 & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & 0.331 \\ 
   &  & -0.022 & \emph{-0.012} & -0.013 & -0.048 & -0.044 & -0.043 & \textbf{0.000} & 0.331 \\ 
   &  & -0.012 & \textbf{0.000} & \textbf{0.000} & -0.053 & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & 0.332 \\ 
   &  & -0.017 & -0.011 & -0.015 & -0.062 & \emph{-0.006} & -0.033 & \textbf{0.000} & 0.337 \\ 
  \hline
  \multirow{20}{*}{gradient} & \multirow{4}{*}{1} & -0.320 & -0.282 & -0.281 & 0.009 & 0.009 & \emph{0.009} & \textbf{0.007} & -0.383 \\ 
   &  & -0.295 & -0.286 & -0.288 & \textbf{-0.003} & 0.040 & 0.039 & \emph{0.008} & -0.380 \\ 
   &  & -0.312 & -0.301 & -0.302 & \textbf{-0.001} & \emph{0.001} & 0.001 & 0.003 & -0.381 \\ 
   &  & -0.302 & -0.310 & -0.312 & -0.017 & \emph{-0.014} & -0.049 & \textbf{0.000} & -0.388 \\ 
  \cline{3-10}
   & \multirow{4}{*}{2} & -0.056 & -0.047 & -0.046 & 0.006 & \textbf{0.004} & \emph{0.004} & 0.004 & -0.068 \\ 
   &  & -0.046 & -0.045 & -0.045 & 0.010 & \emph{0.009} & 0.009 & \textbf{0.006} & -0.065 \\ 
   &  & -0.051 & -0.053 & -0.052 & 0.003 & \textbf{0.000} & \emph{0.000} & 0.001 & -0.067 \\ 
   &  & -0.072 & -0.086 & -0.089 & \emph{-0.013} & -0.025 & -0.038 & \textbf{-0.002} & -0.070 \\ 
  \cline{3-10}
   & \multirow{4}{*}{3} & 0.007 & 0.004 & 0.005 & \textbf{0.001} & \emph{0.002} & 0.002 & 0.004 & 0.015 \\ 
   &  & 0.005 & -0.002 & -0.002 & 0.006 & \emph{0.001} & \textbf{0.001} & 0.003 & 0.018 \\ 
   &  & 0.008 & 0.005 & 0.007 & \textbf{-0.000} & 0.001 & 0.001 & \emph{-0.000} & 0.016 \\ 
   &  & -0.015 & -0.048 & -0.036 & \emph{-0.011} & -0.039 & -0.028 & \textbf{-0.002} & 0.014 \\ 
  \cline{3-10}
   & \multirow{4}{*}{4} & 0.053 & 0.041 & 0.041 & \textbf{0.002} & \emph{0.002} & 0.002 & 0.003 & 0.071 \\ 
   &  & 0.035 & 0.017 & 0.018 & \emph{0.001} & -0.011 & -0.011 & \textbf{-0.001} & 0.073 \\ 
   &  & 0.051 & 0.041 & 0.046 & -0.001 & \emph{-0.000} & \textbf{-0.000} & -0.001 & 0.071 \\ 
   &  & 0.015 & -0.022 & \textbf{-0.002} & -0.017 & -0.028 & -0.025 & \emph{-0.002} & 0.070 \\ 
  \cline{3-10}
   & \multirow{4}{*}{5} & 0.315 & 0.234 & 0.234 & 0.007 & -0.001 & \emph{-0.001} & \textbf{0.000} & 0.388 \\ 
   &  & 0.250 & 0.203 & 0.212 & 0.014 & \emph{0.005} & 0.006 & \textbf{0.000} & 0.387 \\ 
   &  & 0.292 & 0.239 & 0.257 & 0.007 & 0.002 & \emph{0.000} & \textbf{0.000} & 0.384 \\ 
   &  & 0.241 & 0.173 & 0.199 & \emph{-0.009} & 0.011 & 0.022 & \textbf{0.000} & 0.387 \\ 
  \hline
  \multirow{20}{*}{parabola} & \multirow{4}{*}{1} & 0.135 & 0.049 & 0.049 & 0.057 & \emph{0.017} & \textbf{0.016} & 0.238 & 0.334 \\ 
   &  & 0.208 & 0.166 & 0.165 & 0.145 & \emph{0.107} & \textbf{0.107} & 0.239 & 0.335 \\ 
   &  & 0.131 & 0.039 & 0.058 & 0.060 & \textbf{0.013} & \emph{0.015} & 0.228 & 0.326 \\ 
   &  & 0.180 & 0.112 & 0.125 & 0.137 & \emph{0.064} & \textbf{0.063} & 0.248 & 0.334 \\ 
  \cline{3-10}
   & \multirow{4}{*}{2} & \emph{-0.040} & -0.046 & -0.046 & \textbf{-0.038} & -0.048 & -0.048 & -0.100 & -0.094 \\ 
   &  & -0.065 & -0.072 & -0.073 & \textbf{-0.056} & \emph{-0.062} & -0.064 & -0.099 & -0.093 \\ 
   &  & \emph{-0.046} & -0.064 & -0.053 & \textbf{-0.041} & -0.057 & -0.055 & -0.106 & -0.101 \\ 
   &  & -0.087 & -0.086 & -0.078 & -0.079 & \textbf{-0.063} & \emph{-0.063} & -0.096 & -0.093 \\ 
  \cline{3-10}
   & \multirow{4}{*}{3} & \emph{-0.052} & -0.061 & -0.062 & \textbf{-0.045} & -0.056 & -0.056 & -0.162 & -0.164 \\ 
   &  & -0.107 & -0.116 & -0.114 & \textbf{-0.085} & -0.094 & \emph{-0.093} & -0.162 & -0.163 \\ 
   &  & \emph{-0.058} & -0.077 & -0.066 & \textbf{-0.048} & -0.064 & -0.061 & -0.167 & -0.171 \\ 
   &  & -0.121 & -0.114 & -0.091 & -0.099 & \emph{-0.080} & \textbf{-0.056} & -0.161 & -0.162 \\ 
  \cline{3-10}
   & \multirow{4}{*}{4} & \emph{-0.040} & -0.051 & -0.050 & \textbf{-0.037} & -0.051 & -0.051 & -0.114 & -0.102 \\ 
   &  & \emph{-0.082} & -0.093 & -0.093 & \textbf{-0.078} & -0.090 & -0.089 & -0.115 & -0.101 \\ 
   &  & \emph{-0.048} & -0.067 & -0.057 & \textbf{-0.044} & -0.059 & -0.058 & -0.118 & -0.109 \\ 
   &  & -0.064 & -0.079 & -0.071 & \textbf{-0.051} & \emph{-0.060} & -0.062 & -0.114 & -0.100 \\ 
  \cline{3-10}
   & \multirow{4}{*}{5} & 0.137 & 0.046 & 0.044 & 0.038 & \emph{0.007} & \textbf{0.007} & 0.234 & 0.333 \\ 
   &  & 0.169 & 0.141 & 0.129 & \textbf{0.047} & 0.054 & \emph{0.050} & 0.224 & 0.333 \\ 
   &  & 0.129 & 0.035 & 0.049 & 0.049 & \textbf{0.018} & \emph{0.021} & 0.231 & 0.327 \\ 
   &  & 0.173 & 0.099 & 0.121 & 0.107 & \textbf{0.031} & \emph{0.053} & 0.235 & 0.336 \\ 
  \end{tabular}}
\caption{Bias of $\hat{\beta_1}$ (\textbf{minimum}, \emph{next best}).\label{table:X1-bias}}
		\end{center}
		\end{table}	
		%\end{longtable}	
		%\end{comment}

		
		
% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Wed May  1 20:51:29 2013
%\begin{table}[ht]
		\begin{table}
		\thispagestyle{empty}
		\begin{center}
		\resizebox{!}{12cm}{
		\begin{tabular}{cccccccccc}
  function & location & lars & enet & glmnet & u.lars & u.enet & u.glmnet & oracular & gwr \\ 
  \hline
  \multirow{20}{*}{step} & \multirow{4}{*}{1} & 0.016 & 0.008 & 0.008 & 0.077 & 0.055 & 0.055 & \emph{0.003} & \textbf{0.001} \\ 
   &  & 0.042 & 0.124 & 0.125 & 0.270 & 0.566 & 0.585 & \emph{0.010} & \textbf{0.002} \\ 
   &  & 0.025 & 0.016 & 0.011 & 0.134 & 0.072 & 0.068 & \emph{0.003} & \textbf{0.001} \\ 
   &  & 0.076 & 0.157 & 0.151 & 0.296 & 0.437 & 0.444 & \emph{0.011} & \textbf{0.003} \\ 
  \cline{3-10}
   & \multirow{4}{*}{2} & 0.008 & 0.005 & 0.005 & 0.006 & 0.004 & 0.004 & \textbf{0.001} & \emph{0.001} \\ 
   &  & 0.025 & 0.045 & 0.046 & 0.022 & 0.046 & 0.047 & \emph{0.003} & \textbf{0.002} \\ 
   &  & 0.008 & 0.006 & 0.005 & 0.007 & 0.005 & 0.005 & \textbf{0.001} & \emph{0.001} \\ 
   &  & 0.020 & 0.034 & 0.040 & 0.018 & 0.037 & 0.041 & \emph{0.003} & \textbf{0.002} \\ 
  \cline{3-10}
   & \multirow{4}{*}{3} & 0.007 & 0.005 & 0.005 & 0.004 & 0.002 & 0.002 & \textbf{0.000} & \emph{0.001} \\ 
   &  & 0.021 & 0.050 & 0.050 & 0.021 & 0.063 & 0.063 & \textbf{0.001} & \emph{0.002} \\ 
   &  & 0.009 & 0.008 & 0.006 & 0.007 & 0.006 & 0.004 & \textbf{0.000} & \emph{0.001} \\ 
   &  & 0.030 & 0.047 & 0.054 & 0.032 & 0.054 & 0.061 & \textbf{0.002} & \emph{0.002} \\ 
  \cline{3-10}
   & \multirow{4}{*}{4} & 0.010 & 0.018 & 0.018 & 0.013 & 0.020 & 0.020 & \textbf{0.001} & \emph{0.001} \\ 
   &  & 0.029 & 0.030 & 0.030 & 0.032 & 0.034 & 0.033 & \emph{0.002} & \textbf{0.002} \\ 
   &  & 0.012 & 0.015 & 0.018 & 0.014 & 0.019 & 0.019 & \textbf{0.000} & \emph{0.001} \\ 
   &  & 0.028 & 0.025 & 0.029 & 0.032 & 0.033 & 0.031 & \textbf{0.002} & \emph{0.002} \\ 
  \cline{3-10}
   & \multirow{4}{*}{5} & 0.005 & \textbf{0.000} & \textbf{0.000} & 0.031 & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & 0.001 \\ 
   &  & 0.021 & 0.012 & 0.012 & 0.103 & 0.062 & 0.060 & \textbf{0.000} & \emph{0.002} \\ 
   &  & 0.007 & \textbf{0.000} & \textbf{0.000} & 0.046 & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & 0.001 \\ 
   &  & 0.020 & 0.008 & 0.012 & 0.127 & 0.030 & 0.039 & \textbf{0.000} & \emph{0.003} \\ 
  \hline
  \multirow{20}{*}{gradient} & \multirow{4}{*}{1} & 0.011 & 0.011 & 0.011 & 0.007 & 0.006 & 0.006 & \emph{0.003} & \textbf{0.001} \\ 
   &  & 0.016 & 0.018 & 0.018 & 0.045 & 0.082 & 0.087 & \emph{0.010} & \textbf{0.002} \\ 
   &  & 0.011 & 0.008 & 0.008 & 0.013 & 0.004 & 0.004 & \emph{0.002} & \textbf{0.001} \\ 
   &  & 0.026 & 0.043 & 0.046 & 0.080 & 0.167 & 0.135 & \emph{0.009} & \textbf{0.002} \\ 
  \cline{3-10}
   & \multirow{4}{*}{2} & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & \emph{0.001} & \textbf{0.000} \\ 
   &  & 0.003 & 0.003 & 0.003 & 0.003 & 0.003 & 0.003 & \emph{0.002} & \textbf{0.001} \\ 
   &  & 0.002 & 0.001 & 0.001 & 0.001 & 0.001 & \emph{0.001} & \textbf{0.001} & 0.001 \\ 
   &  & 0.008 & 0.015 & 0.020 & 0.008 & 0.018 & 0.022 & \emph{0.002} & \textbf{0.002} \\ 
  \cline{3-10}
   & \multirow{4}{*}{3} & 0.001 & 0.001 & 0.001 & 0.001 & 0.000 & 0.000 & \textbf{0.000} & \emph{0.000} \\ 
   &  & 0.003 & 0.009 & 0.009 & 0.002 & 0.011 & 0.011 & \textbf{0.001} & \emph{0.001} \\ 
   &  & 0.001 & 0.001 & 0.000 & 0.001 & 0.000 & \emph{0.000} & \textbf{0.000} & 0.001 \\ 
   &  & 0.012 & 0.022 & 0.020 & 0.011 & 0.023 & 0.020 & \textbf{0.001} & \emph{0.002} \\ 
  \cline{3-10}
   & \multirow{4}{*}{4} & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & \emph{0.001} & \textbf{0.000} \\ 
   &  & 0.006 & 0.010 & 0.010 & 0.003 & 0.007 & 0.007 & \emph{0.002} & \textbf{0.001} \\ 
   &  & 0.002 & 0.001 & 0.001 & 0.002 & 0.001 & \emph{0.001} & \textbf{0.001} & 0.001 \\ 
   &  & 0.013 & 0.017 & 0.020 & 0.010 & 0.017 & 0.018 & \emph{0.002} & \textbf{0.002} \\ 
  \cline{3-10}
   & \multirow{4}{*}{5} & 0.013 & 0.025 & 0.025 & 0.005 & 0.003 & 0.003 & \textbf{0.000} & \emph{0.001} \\ 
   &  & 0.029 & 0.029 & 0.029 & 0.018 & 0.012 & 0.012 & \textbf{0.000} & \emph{0.002} \\ 
   &  & 0.016 & 0.022 & 0.022 & 0.006 & 0.004 & 0.004 & \textbf{0.000} & \emph{0.001} \\ 
   &  & 0.037 & 0.026 & 0.029 & 0.068 & 0.012 & 0.014 & \textbf{0.000} & \emph{0.003} \\ 
  \hline
  \multirow{20}{*}{parabola} & \multirow{4}{*}{1} & 0.014 & 0.017 & 0.017 & 0.023 & 0.008 & 0.008 & \emph{0.002} & \textbf{0.000} \\ 
   &  & 0.034 & 0.029 & 0.030 & 0.122 & 0.035 & 0.038 & \emph{0.010} & \textbf{0.001} \\ 
   &  & 0.015 & 0.014 & 0.017 & 0.029 & 0.008 & 0.008 & \emph{0.003} & \textbf{0.001} \\ 
   &  & 0.047 & 0.023 & 0.026 & 0.163 & 0.036 & 0.039 & \emph{0.013} & \textbf{0.003} \\ 
  \cline{3-10}
   & \multirow{4}{*}{2} & 0.002 & 0.001 & 0.001 & 0.003 & 0.001 & 0.001 & \emph{0.000} & \textbf{0.000} \\ 
   &  & 0.010 & 0.013 & 0.013 & 0.012 & 0.019 & 0.018 & \emph{0.002} & \textbf{0.001} \\ 
   &  & 0.002 & 0.001 & 0.001 & 0.003 & 0.001 & 0.001 & \textbf{0.000} & \emph{0.000} \\ 
   &  & 0.015 & 0.010 & 0.016 & 0.018 & 0.011 & 0.021 & \emph{0.002} & \textbf{0.002} \\ 
  \cline{3-10}
   & \multirow{4}{*}{3} & 0.003 & 0.001 & 0.001 & 0.003 & 0.001 & 0.001 & \emph{0.000} & \textbf{0.000} \\ 
   &  & 0.010 & 0.011 & 0.011 & 0.014 & 0.015 & 0.014 & \emph{0.001} & \textbf{0.001} \\ 
   &  & 0.003 & 0.001 & 0.001 & 0.004 & 0.001 & 0.001 & \textbf{0.000} & \emph{0.000} \\ 
   &  & 0.015 & 0.008 & 0.010 & 0.018 & 0.009 & 0.019 & \textbf{0.001} & \emph{0.002} \\ 
  \cline{3-10}
   & \multirow{4}{*}{4} & 0.003 & 0.001 & 0.001 & 0.003 & 0.001 & 0.001 & \emph{0.001} & \textbf{0.000} \\ 
   &  & 0.010 & 0.012 & 0.012 & 0.011 & 0.013 & 0.013 & \emph{0.002} & \textbf{0.001} \\ 
   &  & 0.003 & 0.001 & 0.001 & 0.003 & 0.001 & 0.001 & \emph{0.001} & \textbf{0.000} \\ 
   &  & 0.018 & 0.011 & 0.022 & 0.024 & 0.011 & 0.026 & \textbf{0.002} & \emph{0.002} \\ 
  \cline{3-10}
   & \multirow{4}{*}{5} & 0.013 & 0.016 & 0.016 & 0.018 & 0.008 & 0.009 & \emph{0.003} & \textbf{0.001} \\ 
   &  & 0.032 & 0.024 & 0.025 & 0.090 & 0.021 & 0.019 & \emph{0.010} & \textbf{0.001} \\ 
   &  & 0.018 & 0.013 & 0.017 & 0.037 & 0.008 & 0.009 & \emph{0.003} & \textbf{0.001} \\ 
   &  & 0.032 & 0.020 & 0.025 & 0.068 & 0.023 & 0.018 & \emph{0.009} & \textbf{0.003} \\ 
  \end{tabular}}
\caption{Variance of $\hat{\beta_1}$ (\textbf{minimum}, \emph{next best}).\label{table:X1-var}}
\end{center}
\end{table}



% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Wed May  1 20:56:30 2013
		\begin{table}
		\thispagestyle{empty}
		\begin{center}
		\resizebox{!}{12cm}{
		\begin{tabular}{ccccccccccc}
  function & location & lars & enet & glmnet & u.lars & u.enet & u.glmnet & oracular & gwr \\ 
  \hline
  \multirow{20}{*}{step} & \multirow{4}{*}{1} & 0.130 & 0.145 & 0.145 & \textbf{0.097} & 0.118 & \emph{0.118} & 0.194 & 0.285 \\ 
   &  & 0.732 & 0.712 & 0.664 & 0.493 & \emph{0.444} & \textbf{0.415} & 0.979 & 1.061 \\ 
   &  & 0.232 & 0.296 & 0.302 & \textbf{0.122} & 0.227 & \emph{0.225} & 0.430 & 0.436 \\ 
   &  & 0.752 & 0.718 & 0.710 & 0.565 & \textbf{0.496} & \emph{0.515} & 1.048 & 1.042 \\ 
  \cline{3-10}
   & \multirow{4}{*}{2} & \emph{0.222} & 0.257 & 0.257 & \textbf{0.213} & 0.248 & 0.248 & 0.319 & 0.377 \\ 
   &  & 1.082 & 1.048 & 1.055 & 1.070 & \textbf{1.044} & \emph{1.047} & 1.221 & 1.236 \\ 
   &  & \emph{0.304} & 0.359 & 0.347 & \textbf{0.291} & 0.333 & 0.333 & 0.437 & 0.505 \\ 
   &  & 0.984 & 0.951 & 0.939 & 0.954 & \emph{0.914} & \textbf{0.911} & 1.087 & 1.187 \\ 
  \cline{3-10}
   & \multirow{4}{*}{3} & \emph{0.254} & 0.264 & 0.264 & \textbf{0.248} & 0.260 & 0.260 & 0.276 & 0.269 \\ 
   &  & 0.968 & 1.006 & 0.992 & 0.950 & \emph{0.924} & \textbf{0.908} & 1.048 & 1.057 \\ 
   &  & \emph{0.254} & 0.284 & 0.283 & \textbf{0.244} & 0.276 & 0.275 & 0.295 & 0.291 \\ 
   &  & \emph{0.624} & 0.701 & 0.674 & \textbf{0.611} & 0.654 & 0.630 & 0.699 & 0.693 \\ 
  \cline{3-10}
   & \multirow{4}{*}{4} & \textbf{0.244} & 0.291 & 0.291 & \emph{0.255} & 0.286 & 0.286 & 0.366 & 0.455 \\ 
   &  & 0.998 & 1.029 & 1.047 & \textbf{0.978} & \emph{0.994} & 1.018 & 1.113 & 1.147 \\ 
   &  & \textbf{0.272} & 0.286 & 0.285 & 0.285 & 0.286 & \emph{0.281} & 0.407 & 0.496 \\ 
   &  & \emph{0.749} & 0.754 & 0.751 & \textbf{0.749} & 0.757 & 0.751 & 0.872 & 0.930 \\ 
  \cline{3-10}
   & \multirow{4}{*}{5} & \emph{0.223} & 0.328 & 0.328 & \textbf{0.162} & 0.311 & 0.310 & 0.337 & 0.518 \\ 
   &  & 0.651 & 0.666 & 0.670 & \textbf{0.505} & \emph{0.577} & 0.599 & 0.789 & 0.838 \\ 
   &  & \emph{0.204} & 0.296 & 0.297 & \textbf{0.148} & 0.287 & 0.289 & 0.296 & 0.482 \\ 
   &  & 0.930 & 0.952 & 0.926 & \emph{0.817} & 0.842 & \textbf{0.791} & 1.166 & 1.182 \\ 
  \hline
  \multirow{20}{*}{gradient} & \multirow{4}{*}{1} & 0.328 & 0.300 & 0.300 & 0.242 & \textbf{0.236} & \emph{0.236} & 0.251 & 0.375 \\ 
   &  & 0.952 & 0.847 & 0.853 & 0.686 & \textbf{0.664} & \emph{0.664} & 0.765 & 0.939 \\ 
   &  & 0.357 & 0.359 & 0.354 & \textbf{0.261} & \emph{0.266} & 0.270 & 0.269 & 0.372 \\ 
   &  & 0.752 & 0.786 & 0.733 & \textbf{0.597} & 0.750 & \emph{0.671} & 0.873 & 0.842 \\ 
  \cline{3-10}
   & \multirow{4}{*}{2} & 0.228 & 0.225 & 0.225 & 0.223 & \textbf{0.223} & \emph{0.223} & 0.224 & 0.224 \\ 
   &  & 0.873 & 0.852 & 0.852 & 0.862 & \textbf{0.837} & \emph{0.837} & 0.862 & 0.865 \\ 
   &  & 0.266 & 0.264 & 0.264 & 0.257 & \textbf{0.257} & \emph{0.257} & 0.257 & 0.270 \\ 
   &  & 0.841 & 0.789 & 0.798 & 0.812 & \textbf{0.778} & \emph{0.781} & 0.817 & 0.809 \\ 
  \cline{3-10}
   & \multirow{4}{*}{3} & 0.248 & 0.247 & 0.247 & \emph{0.246} & 0.248 & 0.248 & 0.249 & \textbf{0.242} \\ 
   &  & \textbf{1.140} & 1.155 & 1.154 & 1.141 & 1.148 & 1.148 & 1.147 & \emph{1.140} \\ 
   &  & 0.317 & 0.321 & 0.321 & \emph{0.313} & 0.318 & 0.318 & 0.318 & \textbf{0.311} \\ 
   &  & 1.249 & 1.249 & \emph{1.216} & 1.248 & 1.226 & \textbf{1.189} & 1.297 & 1.274 \\ 
  \cline{3-10}
   & \multirow{4}{*}{4} & 0.283 & 0.291 & 0.291 & \textbf{0.278} & 0.281 & \emph{0.281} & 0.285 & 0.299 \\ 
   &  & 0.870 & 0.871 & 0.871 & 0.882 & 0.866 & \textbf{0.865} & 0.890 & \emph{0.866} \\ 
   &  & 0.213 & 0.209 & 0.209 & \textbf{0.207} & 0.208 & 0.208 & \emph{0.208} & 0.214 \\ 
   &  & 0.794 & 0.818 & 0.804 & 0.793 & \emph{0.793} & \textbf{0.779} & 0.815 & 0.820 \\ 
  \cline{3-10}
   & \multirow{4}{*}{5} & 0.332 & 0.320 & 0.320 & \textbf{0.205} & 0.209 & 0.209 & \emph{0.209} & 0.373 \\ 
   &  & 1.363 & 1.370 & 1.353 & \emph{1.222} & 1.234 & \textbf{1.195} & 1.313 & 1.466 \\ 
   &  & 0.279 & 0.293 & 0.298 & \textbf{0.218} & 0.228 & 0.228 & \emph{0.226} & 0.333 \\ 
   &  & 0.978 & 1.006 & 0.992 & \textbf{0.846} & 0.899 & \emph{0.889} & 1.041 & 1.103 \\ 
  \hline
  \multirow{20}{*}{parabola} & \multirow{4}{*}{1} & 0.151 & 0.154 & 0.154 & \textbf{0.126} & 0.144 & \emph{0.144} & 0.212 & 0.260 \\ 
   &  & 1.192 & 1.252 & 1.251 & \textbf{1.078} & \emph{1.098} & 1.102 & 1.414 & 1.516 \\ 
   &  & 0.286 & 0.285 & 0.288 & \textbf{0.229} & 0.271 & \emph{0.270} & 0.367 & 0.448 \\ 
   &  & 0.848 & 0.905 & 0.902 & \textbf{0.753} & 0.816 & \emph{0.808} & 0.968 & 1.017 \\ 
  \cline{3-10}
   & \multirow{4}{*}{2} & \emph{0.199} & 0.201 & 0.201 & \textbf{0.195} & 0.201 & 0.202 & 0.203 & 0.201 \\ 
   &  & 1.197 & 1.176 & 1.170 & 1.195 & \emph{1.161} & \textbf{1.155} & 1.229 & 1.223 \\ 
   &  & \textbf{0.241} & 0.252 & 0.249 & \emph{0.241} & 0.246 & 0.245 & 0.263 & 0.263 \\ 
   &  & 1.167 & 1.129 & \emph{1.101} & 1.160 & 1.118 & \textbf{1.090} & 1.211 & 1.207 \\ 
  \cline{3-10}
   & \multirow{4}{*}{3} & \emph{0.213} & 0.224 & 0.223 & \textbf{0.210} & 0.225 & 0.224 & 0.239 & 0.238 \\ 
   &  & \emph{1.042} & 1.047 & 1.048 & \textbf{1.038} & 1.046 & 1.044 & 1.072 & 1.081 \\ 
   &  & \emph{0.228} & 0.239 & 0.238 & \textbf{0.226} & 0.237 & 0.237 & 0.262 & 0.258 \\ 
   &  & 0.925 & 0.943 & 0.938 & \emph{0.911} & \textbf{0.910} & 0.920 & 1.026 & 1.018 \\ 
  \cline{3-10}
   & \multirow{4}{*}{4} & 0.248 & \emph{0.243} & \textbf{0.243} & 0.248 & 0.245 & 0.246 & 0.246 & 0.245 \\ 
   &  & \emph{1.354} & 1.379 & 1.379 & \textbf{1.344} & 1.359 & 1.361 & 1.421 & 1.393 \\ 
   &  & \emph{0.291} & 0.301 & 0.296 & \textbf{0.291} & 0.295 & 0.294 & 0.317 & 0.309 \\ 
   &  & 1.019 & 1.020 & 1.001 & 0.995 & \emph{0.992} & \textbf{0.960} & 1.086 & 1.058 \\ 
  \cline{3-10}
   & \multirow{4}{*}{5} & 0.239 & 0.236 & 0.235 & \textbf{0.201} & \emph{0.235} & 0.235 & 0.284 & 0.342 \\ 
   &  & \emph{0.953} & 1.007 & 1.005 & \textbf{0.902} & 0.962 & 0.965 & 1.009 & 1.059 \\ 
   &  & \emph{0.193} & 0.229 & 0.223 & \textbf{0.163} & 0.219 & 0.216 & 0.253 & 0.295 \\ 
   &  & \emph{0.850} & 0.905 & 0.917 & \textbf{0.746} & 0.891 & 0.873 & 0.987 & 1.044 \\ 
  \end{tabular}}
\caption{Mean squared error of $\hat{Y}$ (\textbf{minimum}, \emph{next best}).\label{table:Y-mse}}
\end{center}
\end{table}

	%\subsection{Figures}
	%The plots of bias demonstrate that GWL tended to ``fill the valleys" and ``trim the peaks" of the coefficient surface for $\beta_1$, which is not unexpected for a smoother like GWR. 
	
	%Figures \ref{fig:coveragemap1} - \ref{fig:coveragemap18} show the frequency with which the true value of the parameter $\beta_1$ was covered by the 95\% confidence intervals at each location under each simulation setting. The left column shows the coverage frequency of the 95\% CI of the GWL using the unshrunk-bootstrap method of CI construction. The middle column is the coverage frequency of the 95\% CI the O-GWR using the bootstrap to generate the CI. The right column is the relative efficiency of the GWL to O-GWR. In the first two columns, the color white is used to indicate areas where the nominal coverage frequency of 95\% is achieved, while blue codes areas that exceeded 95\% coverage and orange codes areas that fell short of 95\% coverage. In the third column, the color white indicates areas where the relative efficiency is unity, while orange indicates areas where the relative efficiency was less than unity and blue indicates areas where the relative efficiency exceeded unity.\\	

	


			
\section{Data Analysis\label{section:data-analysis}}
	\subsection{Census Poverty Data}
	An example data analysis is presented to demonstrate application of penalized GWR. In this example we use penalized GWR to do local variable selection and coefficient estimation for a  varying-coefficients model of how poverty is related to a list of demographic and social variables. The data is from the U.S. Census Bureau's decennial census  from 1970. This analysis looks specifically at the upper midwestern states of Minnesota, Iowa, Wisconsin, Illinois, Indiana, and Michigan. This is areal data, aggregated at the county level.
	
	Three kinds of variables  were considered as potential predictors of county-level poverty rate.
	\begin{itemize}
		\item Variables that describe the county's employment structure ($\verb!pag!$, the proportion of residents employed in agriculture, $\verb!pex!$, the proportion of residents employed in mining, $\verb!man!$, the proportion of residents employed in manufacturing, $\verb!pfire!$, the proportion of residents employed in finance, insurance, and real estate, $\verb!pserve!$, the proportion of residents employed in services, and $\verb!potprof!$, the proportion of residents employed in other professions)\\
		\item Variables that describe the county's racial makeup ($\verb!pwh!$, the proportion of residents who are white, $\verb!pblk!$, the proportion of residents who are black, and $\verb!phisp!$, the proportion of residents who are hispanic)\\
		\item $\verb!pmetro!$: an indicator of whether the county is in a metropolitan area.
	\end{itemize}
	The outcome of interest (poverty rate) is a proportion, taking values in $[0,1]$. To demonstrate the geographically-weighted Lasso in a linear regression context, we model the logit-transformed poverty rate. The predictor variables were not transformed - raw proportions were used.
	
	\subsection{Modeling}	
	The adaptive elastic net was used for variable selection, and then coefficients for the selected variables were estimated by weighted least squares without shrinkage. The standard \verb!gwr! algorithm was used to fit a model to the same data for the sake of comparison.
	
	\subsection{Figures}
	The coefficient estimates are plotted on maps of the upper midwest in Figure \ref{fig:unshrunk-enet-coefs-1970} (based on the adaptive elastic net) and Figure \ref{fig:gwr-coefs-1970} (for standard GWR).
	\begin{figure}
		\begin{center}
			\includegraphics[height=8in]{../../figures/poverty/1970.linear.coefficients.unshrunk.pdf}
			\caption{Coefficient surfaces for the logit of poverty rate, based on the 1970 census and estimated by the unshrunk adaptive elastic net. \label{fig:unshrunk-enet-coefs-1970}}
		\end{center}
	\end{figure}	
	
	\begin{figure}
		\begin{center}
			\includegraphics[height=8in]{../../figures/poverty/1970.gwr.pdf}
			\caption{Coefficient surfaces for the logit of poverty rate based on the 1970 census and estimated by base GWR. \label{fig:gwr-coefs-1970}}
		\end{center}
	\end{figure}	
			
	\subsection{Discussion}
	It is immediately apparent that the estimated coefficient surfaces are non-constant for most variables. The same large-scale patterns appear in both figures, but with differences. First of all, the adaptive elastic net has selected a larger bandwidth than base GWR, so there is less variability in the coefficient estimates from the adaptive elastic net. This may be one reason that the adaptive elastic net coefficient estimates are less extreme than those for base GWR. In a model with a logit-transformed proportion as the output, the coefficients can be interpreted as log odds ratios, so, e.g., the estimate of -100 as the coefficient of \verb!phisp! (albeit at the edge of the domain) seems unrealistic.\\
	
	Assessing variable selection for this data is difficult, since the adaptive elastic net almost never removed any variables from the model. Indeed, some coefficients seem nearly constant across the domain. An exception is the coefficient surface for \verb!pex! (mining employment). That surface indicates an interaction whereby the proportion of people working in mining in southern parts of the domain is associated with an increase in the poverty rate, while in northern parts of the domain it is associated with a decrease in the poverty rate.\\

%\end{spacing}

\section{References}
\bibliographystyle{chicago}
\bibliography{../../references/gwr}

\end{document}  