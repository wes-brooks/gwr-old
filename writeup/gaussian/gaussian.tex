\documentclass[authoryear, review, 11pt]{elsarticle}

\setlength{\textwidth}{6.5in}
%\setlength{\textheight}{9in}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{bm}

%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{verbatim}

\usepackage{relsize}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{fullpage}
\usepackage{booktabs}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\bw}{\mbox{bw}}
\DeclareMathOperator*{\df}{\mbox{df}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\E}{\mathop{\mathbb E}}


\title{Local variable selection for varying-coefficients models in the context of geographically-weighted regression}
\author{Wesley Brooks}
\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}





\section{Introduction}
	%Varying-coefficients regression
	Varying-coefficients regression (VCR) \citep{Hastie:1993a} is a technique used in spatial statistics to model processes with non-constant coefficients. Methods for VCR are typically divided between spline-based \citep{Wood:2006} and kernel-based \citep{Hastie:1993b, Loader:1999} methods. 
	
	%Spatial VCR
	In the setting of spatial data, VCR can be used to 
	
	%Kernel smoothing
	Geographically weighted regression (GWR) \citep{Fotheringham:2002} is a method of fitting varying-coefficients regression models for spatial data that uses kernel-weighted regression with weights based on the distance between observation locations. The presentation of GWR in \cite{Fotheringham:2002} follows the development of local likelihood in \cite{Loader:1999}.
	
	%Local regression
	GWR can be thought of as a kernel smoother for regression coefficients, and hence GWR coefficient estimates are likely to exhibit bias near the boundary of the region being modeled \citep{Hastie:1993b}. Modeling the coefficient surface as locally linear rather than locally constant (by including coefficient-by-location interactions) can reduce this boundary-effect bias \citep{Hastie:1993b}. Adding these interactions to the GWR model is analogous to a transition from kernel smoothing to local regression, and was introduced in \cite{Wang:2008b}.
	
	%Global Variable selection
	There is interest among practitioners of GWR not only in estimating the spatially-varying coefficient surface, but in doing variable selection to estimate which covariates are important predictors of the output variable, and in what regions they are important (citations?).
	
	%Local variable selection
	Some recent research has focused on variable selection in varying-coefficients models. In the context of varying-coefficients regression models, global variable selection (in which one compares the hypothesis that the coefficient on a given variable is zero everywhere against the hypothesis that the coefficient is nonzero somewhere) is distinguished from local variable selection (in which one compares the hypothesis that the coefficient on a given variable is zero at a given location against the hypothesis that the coefficient at that location is nonzero). Global variable selection for models where the varying coefficients are estimated using splines is addressed in \cite{Fan:1999} for response variables that belong to an exponential-family distribution (as in the generalized linear model), and in \cite{Wang:2008a} for models with repeated measurements. \cite{Antoniadis:2012a} estimates the coefficient functions with P-splines, and then uses the nonnegative garrote of \cite{Breiman:1995} to do local variable selection by selecting P-spline bases.
	
	Here we discuss a method of local variable selection in GWR models using the adaptive lasso of \cite{Zou:2006}. The idea first appears in the literature as the geographically-weighted LASSO (GWL) of \cite{Wheeler:2009}, which uses a jackknife criterion for selection of the lasso tuning parameters. Because the jackknife criterion can only be computed at locations where the response variable is observed, the GWL cannot be used for imputation of missing data nor for interpolation between observation locations. We avoid this limitation of the GWL by using a penalized-likelihood criterion to select the lasso tuning parameters (specifically the AIC, but in principle one could use the BIC, \emph{et cetera}). The AIC allows us to easily adapt our method to the setting of a generalized linear model. The local AIC presented here is based on an \emph{ad hoc} calculation of the degrees of freedom used to estimate the spatially-varying coefficient surfaces.
	
\section{Geographically-weighted regression \label{section:GWR}}

	\subsection{Model}
	Consider $n$ data observations, made at sampling locations $\bm{s}_1, \dots, \bm{s}_n$ in a spatial domain $D \subset \mathbb{R}^2$. For $i = 1, \dots, n$, let $y(\bm{s}_i)$ and $\bm{x}(\bm{s}_i)$ denote the univariate response variable, and a $(p+1)$-variate vector of covariates measured at location $\bm{s}_i$, respectively. At each location $\bm{s}_i$, assume that the outcome is related to the covariates by a linear model where the coefficients $\bm{\beta}_i(\bm{s}_i)$ may be spatially-varying and $\varepsilon(\bm{s}_i)$ is random noise at location $\bm{s}_i$.

	\begin{eqnarray}
		y(\bm{s}_i) = \bm{x}'(\bm{s}_i) \bm{\beta}(\bm{s}_i) + \varepsilon(\bm{s}_i)
	\label{eq:lm(s)}
	\end{eqnarray}
	
	Further assume that the error term $\varepsilon(\bm{s}_i)$ is normally distributed with zero mean and a possibly spatially-varying variance $\sigma^2(\bm{s}_i)$
	\begin{eqnarray}
		\varepsilon(\bm{s}_i) \sim \mathcal{N} \left( 0,\sigma^2(\bm{s}_i) \right)
	\label{eq:err}
	\end{eqnarray}
	
	In order to simplify the notation, let subscripts denote the values of data or parameters at the locations where data is observed. Thus, $\bm{x}(\bm{s}_i) \equiv \bm{x}_i \equiv \left( 1, x_{i1}, \dots, x_{ip} \right)'$, $\bm{\beta}(\bm{s}_i) \equiv \bm{\beta}_i \equiv \left(\beta_{i0}, \beta_{i1}, \dots, \beta_{ip} \right)'$, $y(\bm{s}_i) \equiv y_i$, and $\sigma^2(\bm{s}_i) \equiv \sigma^2_i$. Let $\bm{X} = \left( \bm{x}_1, \dots, \bm{x}_n \right)'$ and $\bm{Y} = \left( y_1, \dots, y_n \right)'$. Equations (\ref{eq:lm(s)}) and (\ref{eq:err}) can now be rewritten as
	\begin{eqnarray}
		y_i = \bm{x}'_i \bm{\beta}_i + \epsilon_i \text{ and } \epsilon_i \sim \mathcal{N} \left( 0,\sigma_i^2 \right)
	\end{eqnarray}
	
	Assume that, given the design matrix $\bm{X}$, observations of the response variable at different locations are statistically independent of each other. Then the total log-likelihood of the observed data is the sum of the log-likelihood of each individual observation.
	 \begin{eqnarray}
	 	\ell\left( \bm{\beta} \right) = - 1/2 \sum_{i=1}^n \left\{  \log \left( 2 \pi \sigma^2_i\right) +  \sigma^{-2}_i  \left(y_i - \bm{x}'_i\bm{\beta}_i \right)^2  \right\}
	\end{eqnarray}
	
	With $n$ observations and $n \times (p+1)$ free parameters, the model is not identifiable so it is not possible to directly maximize the total likelihood. One way to effectively reduce the number of parameters is to assume that the spatially-varying coefficients $\bm{\beta}(\bm{s})$ are \emph{smoothly} varying, and use a kernel smoother to make pointwise estimates of the coefficients by maximizing the local likelihood. In the setting of spatial data and with the kernel smoother based on the physical distance between observation locations, this method is called geographically-weighted regression (GWR).
		
	\subsection{Estimation}
		
	Geographically-weighted regression estimates the value of the coefficient surface $\bm{\beta}(\bm{s})$ at each location $\bm{s}_i$. First calculate the euclidean distance $\delta_{ii'} \equiv \delta\left(\bm{s}_i, \bm{s}_{i'}\right) \equiv \|\bm{s}_i  -\bm{s}_{i'}\|_2$ between locations $\bm{s}_i$ and $\bm{s}_{i'}$ for all $i, i'$. A bisquare kernel is used to generate spatial weights based on the euclidean distances and a bandwidth $\phi$:\\
	
	\begin{eqnarray}
		w_{ii'} = \begin{cases} \left[1-\left(\phi^{-1}\delta_{ii'}\right)^2\right]^2 &\mbox{ if } \delta_{ii'} < \phi \\ 0 &\mbox{ if } \delta_{ii'} \geq \phi \end{cases}
	\end{eqnarray}
	
	For the purpose of estimation, define the local likelihood at each location \citep{Fotheringham:2002}:
	\begin{eqnarray}
		\cal{L}_i \left(\bm{\beta}_i \right) &=& \prod_{i'=1}^n \left\{ \left(2 \pi \sigma^2_i  \right)^{-1/2}  \exp\left[-\frac{1}{2} \sigma^{-2}_i  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right] \right\} ^ {w_{ii'}}
	\end{eqnarray}
			
	Thus, the local log-likelihood function is:
	\begin{eqnarray}\label{eq:local-log-likelihood}
		\ell_i\left(\bm{\beta}_i\right) &\propto& -1/2 \sum_{i'=1}^n w_{ii'} \left\{ \log{\sigma^2_i}  + \sigma^{-2}_i  \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 \right\}
	\end{eqnarray}
	
	From which it is apparent that the GWR coefficient estimates $\hat{\bm{\beta}}_{i,\text{GWR}}$, which maximize the local likelihood at location $\bm{s}_i$, can be calculated using weighted least squares. Letting the weight matrix $\bm{W}_i$ be:
	\begin{eqnarray}
		\bm{W}_i =  {\rm diag}\left\{w_{ii'}\right\}_{i' = 1}^n
	\end{eqnarray}
	
	We have:	
	\begin{eqnarray}
		\hat{\bm{\beta}}_{i, \text{GWR}} = \left( \bm{X}'\bm{W}_i\bm{X} \right)^{-1} \bm{X}'\bm{W}_i\bm{Y}
	\end{eqnarray}
	
	And $\hat{\sigma}_i$, which maximizes (\ref{eq:local-log-likelihood}), is:
	\begin{eqnarray}
		\hat{\sigma}_i = \left(\bm{1}_n'\bm{w}_i \right)^{-1} \bm{w}_i'\left(\bm{Y}-\bm{X}\left(\bm{X}'\bm{W}_i\bm{X}\right)^{-1}\bm{X}'\bm{Y}\right)
	\end{eqnarray}
	
	\begin{comment}
	Estimation of $\hat{\bm{\beta}}_i$ and $\hat{\sigma}_i$ is by maximum local likelihood, which is implemented by setting the derivatives of (\ref{eq:local-log-likelihood}) to zero:
	\begin{eqnarray}
		\left\{\frac{\partial \ell_i}{\partial \bm{\beta}_i} \right\}_j =   \sum_{i'=1}^n \left\{ x_{i'j} w_{ii'} \sigma^{-2}_i \left( y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right) \right\} \\
		\frac{\partial \ell_i}{\partial \sigma_i^2} \bigg|_{\hat{\beta}_i} &=& -\frac{1}{2} \sum_{i'=1}^n w_{ii'} \left\{ \left(\sigma_i^{2}\right)^{-1} - \left(\sigma_i^{2}\right)^{-2} \left( y_i - \bm{x}_i'\bm{\hat{\beta}}_i \right)^2 \right\} \\
		\hat{\sigma}_i^2 &=& \left(\sum_{i'=1}^n w_{ii'}\right)^{-1} \sum_{i'=1}^n w_{ii'} \left(y_i - \bm{x}_i'\hat{\bm{\beta}}_i\right)
	\end{eqnarray}
		
		\left\{\frac{\partial^2 \ell_i}{\partial \bm{\beta}_i \partial \bm{\beta}'_i} \right\}_{j,k} = -\sum_{i'=1}^n \left\{ x_{i'j} x_{i'k} w_{ii'} \sigma^{-2}_i \right\}
	\end{eqnarray}
	
	So the observed Fisher information in the locally weighted sample is
	\begin{eqnarray}
		\bm{\mathcal{J}}_i &=& \sigma^{-2}_i \left( \begin{array}{ccc} \sum_{i'=1}^n  w_{ii'} x^2_{i'1}   & \dots & \sum_{i'=1}^n w_{ii'} x_{i'1} x_{i'p}   \\ \vdots & \ddots & \vdots \\ \sum_{i'=1}^n  w_{ii'} x_{i'p} x_{i'1}    & \dots & \sum_{i'=1}^n  w_{ii'} x^2_{i'p}  \end{array} \right) \\
		&=& \sigma^{-2}_i \sum_{i'=1}^n w_{ii'}\left( \begin{array}{ccc}  x^2_{i'1} & \dots & x_{i'1} x_{i'p} \\ \vdots & \ddots & \vdots \\ x_{i'p} x_{i'1} & \dots &  x^2_{i'p} \end{array} \right) \\
		&=& \sigma^{-2}_i \sum_{i'=1}^n w_{ii'} \bm{x}_{i'} \bm{x}'_{i'}
	\end{eqnarray}	
	
	The form of the observed Fisher information suggests that the information in the data $\bm{x}_{i'}$ about the coefficients at location $s_i$ is proportional to the weight $w_{ii'}$.
	\end{comment}

	
	 
	
\section{Model selection \label{section:method}}
	\subsection{Variable selection}
	Traditional GWR relies on \emph{a priori} model selection to decide which variables should be included in the model. In the context of ordinary least squares regression, regularization methods such as the adaptive LASSO \citep{Zou:2006} have been shown to have appealing properties for automating variable selection, sometimes including the ``oracle" property of asymptotically selecting exactly the correct variables for inclusion in a regression model.\\
	
	The adaptive LASSO is applied to GWR by first multiplying the design matrix $\bm{X}$ by $\bm{W}_i^{1/2}$, the diagonal matrix of geographic weights centered at $s_i$. Since some of the weights $w_{ii'}$ may be zero, the matrix $\bm{W}_i^{1/2}\bm{X}$ is not of full rank. The matrices $\bm{Y}_i^*$, $\bm{X}_i^*$, and $\bm{W}_i^*$ are formed by dropping the rows of $\bm{X}$  and $\bm{W}_i$ that correspond to observations with zero weight in the regression model at location $\bm{s}_i$. Now, letting $\bm{U}_i^* = \bm{W}_i^{*1/2} \bm{X}_i^*$ and $\bm{V}_i^* = \bm{W}_i^{*1/2} \bm{Y}_i^*$, we seek the coefficients $\bm{\beta}_i$ of the regression model:
	
	\begin{eqnarray}
		\bm{V}_i^* = \bm{U}_i^* \bm{\beta}_i + \epsilon
	\end{eqnarray}
	
	To apply the adaptive LASSO for estimating these regression coefficients, each column of $\bm{U}_i^*$ is centered around zero and rescaled to have an $\mbox{L}_2$-norm of one. Let $\widetilde{\bm{U}}_i^*$ be the centered-and-scaled version of $\bm{U}_i^*$. Adaptive weights are calculated using the OLS regression coefficients $\bm{\gamma}_i^*$ via ordinary least squares (OLS):
	
	\begin{eqnarray}\label{eq:adaptive-weights-regression}
		\bm{\gamma}_i^* = \left( \widetilde{\bm{U}}_i^{*'} \widetilde{\bm{U}}_i^* \right)^{-1} \widetilde{\bm{U}}_i^{*'} \bm{V}_i^*
	\end{eqnarray}
	
	Now a final scaling step is done: for $j=1, \dots, p$, the $j$th column of $\tilde{\bm{U}}_i^*$ is multiplied by $\left(\gamma_i^*\right)_j$, the corresponding coefficient from (\ref{eq:adaptive-weights-regression}). Call this rescaled matrix $\widecheck{\bm{U}}_i^*$.\\
	
	Finally, the adaptive LASSO coefficient estimates at location $\bm{s}_i$ are found by using the \verb!lars! algorithm \citep{Efron:2004b} to model $\bm{V}_i^*$ as a function of $\widecheck{\bm{U}}_i^*$.

	\subsection{Tuning parameter selection}
	The final task is to select the LASSO tuning parameter. We propose a locally-weighted version of the Akaike information criterion (AIC \citep{Akaike:1974}) to select the tuning parameter. The local AIC allows coefficients to be estimated at any location where the local likelihood can be calculated. The local AIC is calculated by adding a penalty to the local likelihood, with the sum of the weights around $s_i$, $\sum_{i'=1}^n w_{ii'}$, playing the role of the sample size and the number of nonzero coefficients in $\bm{\beta}_i$ playing the role of the ``degrees of freedom" $\left( \df_i \right)$ \citep{Zou:2007}.\\
	
	The objective minimized by the geographically-weighted LASSO (GWL) is:	
	\begin{eqnarray}
		\sum_{i'=1}^n w_{ii'} \left(y_{i'} - \bm{x}'_{i'} \bm{\beta}_i \right)^2 + \sum_{j=1}^p \lambda_{ij} \beta_{ij}
	\end{eqnarray}
	
	Where $\lambda_{ij}, j =1, \dots, p$ are penalties from the adaptive LASSO \citep{Zou:2006}. Taking the derivatives with respect to $\beta$ and setting to zero, we see that
	\begin{eqnarray}
		\hat{\bm{\beta}}_{i, \text{GAL}} &=& \left( \bm{X}'\bm{W}_i\bm{X} \right)^{-1}  \bm{X}'\bm{W}_i\bm{Y}  - \frac{1}{2} \left(\bm{X}'\bm{W}_i\bm{X} \right)^{-1} \bm{\lambda}_i\\
		\hat{y}_i = \bm{x}_i' \hat{\bm{\beta}}_{i, \text{GAL}} &=&  \bm{x}_i' \left( \bm{X}'\bm{W}_i\bm{X} \right)^{-1}  \bm{X}'\bm{W}_i\bm{Y}  - \frac{1}{2} \bm{x}_i' \left(\bm{X}'\bm{W}_i\bm{X} \right)^{-1} \bm{\lambda}_i
	\end{eqnarray}
	
	Unlike in the case of ordinary geographically-weighted regression, the fitted values $\hat{\bm{Y}}$ are not a linear combination of the observations $\bm{Y}$. Because GAL is not a linear smoother the AIC and confidence intervals as calculated in \cite{Fotheringham:2002} are not accurate for the GAL \citep{Zou:2006}. The local AIC at location $\bm{s}_i$($\mbox{AIC}_{\text{loc}, i}$) is minimized to select the adaptive lasso tuning parameter.
	\begin{eqnarray}
		\mbox{AIC}_{\text{loc}, i} &=& -2 \sum_{i'=1}^n \ell_{ii'}  + 2 \mbox{df}_i\\
		&=& -2 \times \sum_{i'=1}^n \log \left\{ \left(2 \pi \hat{\sigma}_i^2\right)^{-1/2} \exp \left[-\frac{1}{2} \hat{\sigma}_i^{-2} \left(y_{i'} - \bm{x}_{i'}' \hat{\bm{\beta}}_{i'} \right)^2\right] \right\}^{w_{ii'}} + 2\mbox{df}_i\\
		&=& \sum_{i'=1}^n w_{ii'} \left\{ \log \left(2 \pi \right) + \log \hat{\sigma}_i^2 + \hat{\sigma}_i^{-2} \left(y_{i'} - \bm{x}_{i'}' \hat{\bm{\beta}}_{i'} \right)^2 \right\} + 2\mbox{df}_i \\
		&=& \hat{\sigma}_i^{-2} \sum_{i'=1}^n w_{ii'} \left( y_{i'} - \bm{x}'_{i'} \hat{\bm{\beta}}_i \right)^2 + 2 \mbox{df}_i + C_i
	\end{eqnarray}	
	Where the estimated local variance $\hat{\sigma}_i^2$ is the variance estimate from the unpenalized local model \citep{Zou:2007}, so $C_i$ does not depend on the choice of tuning parameter and can be ignored.
	
	\cite{Wheeler:2009} proposed selecting the tuning parameter for the LASSO at location $\bm{s}_i$ to minimize the jackknife prediction error $|y_i - \hat{y}_i^{(i)}|$. This choice restricts coefficient estimation to occur at the locations where data has been observed. By contrast, the local AIC can be calculated at any location within one bandwidth $\phi$ of any data point. 
	 
	\subsection{Bandwidth selection}
	The bandwidth is selected to minimize the total AIC ($\mbox{AIC}_{\mbox{tot}}$). Because of the kernel weights and the application of the Adaptive LASSO, the sample size and degrees of freedom are different at each location. The total AIC is found by taking the sum over all of the observed data:	
		
	\begin{eqnarray}
		\mbox{AIC}_{\mbox{tot}} &=& -2 \times \sum_{i=1}^n \ell_i + 2 \times \mbox{df}\\
		&=& \sum_{i=1}^n \left\{ \log \hat{\sigma}_i^2 + \hat{\sigma}_i^{-2} \left(y_i - \bm{x}_i' \hat{\bm{\beta}}_i \right)^2 \right\} + 2 \times \mbox{df}\label{eq:total-AIC1}
	\end{eqnarray}	
	
	What remains is to calculate $\mbox{df}$, the number of degrees of freedom used by the model. Classical GWR, as developed in \cite{Loader:1999} and \cite{Fotheringham:2002} calculates $\mbox{df}$ using the trace of the ``hat" matrix, but because the GAL is not a linear smoother, there is no ``hat" matrix associated with GWR. Instead, notice that $\mbox{df}$ can be pulled into the summation in (\ref{eq:total-AIC1}):
		
	%This is different from the formulas for the AIC as proposed in \cite{Fotheringham:2002} and \cite{Loader:1999}. The reason is that the basic GWR estimator is linear, so the degrees of freedom can be approximated using the trace of the ``hat" matrix. The GAL, though, is not a linear estimator so some...\ However, since the GAL does local variable selection, the model may use a different number of parameters at each location $s_i$. 
			
	\begin{eqnarray}
		\mbox{df} &=& \sum_{i=1}^n \left( n^{-1} \mbox{df} \right)
	\end{eqnarray}
	
	Now, because we are considering the sum of local weights to be the sample size for the local models, we estimate $\mbox{df}$ by $\sum_{i=1}^n \left\{ \left(\sum_{i'=1}^n w_{ii'} \right)^{-1} \mbox{df}_i \right\}$, and the total AIC is then:
	
	\begin{eqnarray}\label{eq:total-AIC2}
		\mbox{AIC}_{\mbox{tot}} &=& \sum_{i=1}^n \left\{ \log \hat{\sigma}_i^2 + \hat{\sigma}_i^{-2} \left(y_i - \bm{x}'_i \hat{\bm{\beta}}_i \right)^2 + 2 \times \left(\sum_{i'=1}^n w_{ii'} \right)^{-1} \mbox{df}_i \right\}
	\end{eqnarray}
			
	The bandwidth that minimizes (\ref{eq:total-AIC2}) is found by a line search.\\
	
	\subsection{Confidence interval construction}	
	Confidence intervals for the GAL's coefficient estimates can be calculated either by the bootstrap \citep{Efron:1986} or by exploiting an assumption of normally-distributed residuals. The, e.g., 95\% confidence interval for each regression coefficient is then the (2.5, 97.5) percentiles of the coefficient estimates from the bootstrap replicates.\\
	 
	 	\subsubsection{Bootstrap confidence interval}
		To compute coefficient confidence intervals via the bootstrap, the observations with non-zero geographic weights are resampled uniformly with replacement for each of $n_B$ bootstrap replicates. For each bootstrap replicate, the GAL is used to estimate regression coefficients. The local likelihood of the bootstrap replicates may be different from that of the original sample, so the adaptive lasso tuning parameter may differ for each bootstrap replicate. Since the GAL is applied independently to each bootstrap replicate, the variables selected by GAL may be different for each replicate.
	
		 Unshrunk coefficient estimates are found by using the GAL at each location for variable selection only and then estimating the coefficients for the selected variables by GWR. An unshrunk bootstrap confidence interval is found by estimating the unshrunk coefficients for each of the $n_B$ bootstrap replicates and then calculating the percentiles as above.\\
	 
		
	 	\subsubsection{Normal approximation-based confidence interval}
		A third way to estimate the coefficient confidence intervals is to use the GAL for variable selection only and then to use GWR to calculate a confidence interval based on the assumption of an independent, identically distributed, Gaussian error structure. In this case, the standard error of the regression coefficients is 
		\begin{eqnarray}
			\hat{\mbox{se}}_{\beta_i} &=& \left( \tilde{\bm{X}}_i'\bm{W}_i \tilde{\bm{X}}_i \right)^{-1}  \tilde{\bm{X}}_i'\bm{W}_i\bm{Y}
		\end{eqnarray}
	
		where $\tilde{\bm{X}}_i$ is the model matrix including only those variables that are selected by GAL at location $i$.



\section{Simulation}
	\subsection{Simulation setup}
	A simulation study was conducted to assess the finite-sample properties of the method described in Sections \ref{section:model}-\ref{section:method}. Data was simulated on $[0,1] \times [0,1]$, which was divided into a $30 \times 30$ grid. Each of $p=5$ covariates $Z_1, \dots, Z_p$ was simulated by a Gaussian random field (GRF) with mean zero and exponential spatial covariance $Cov \left(Z_{ji}, Z_{ji'} \right) = \sigma_z^2 \exp{\left( -\tau_z^{-1} \|s_i - s_{i'} \| \right)}$ where $\sigma_z^2=1$ is the variance and $\tau_z$ is a range parameter. Correlation was induced between the covariates by multiplying the $\bm{Z}$ matrix by $\bm{R}$, where $\bm{R}$ is the Cholesky decomposition of the covariance matrix $\Sigma = \bm{R}'\bm{R}$. The covariance matrix $\bm{\Sigma}$ is a $5 \times 5$ matrix that has ones on the diagonal and $\rho$ for all off-diagonal entries, where $\rho$ is the between-covariate correlation.
		
	The simulated response is $y_i = \bm{z}'_i \bm{\beta}_i + \epsilon_i$ for $i=1, \dots, 900$ where the vector of additive errors $\bm{\epsilon}$ is generated from a GRF with spatial covariance $Cov \left(\epsilon_{i}, \epsilon_{i'} \right) = \sigma_{\epsilon}^2 \exp{\left( -\tau_{\epsilon}^{-1} \|s_i - s_{i'} \| \right)}$ where $\sigma_{\epsilon}^2=1$.
	
	The simulated data include the output $y$ and five covariates $Z_1, \dots, Z_5$. The true data-generating model uses only $Z_1$, so $Z_2, \dots, Z_5$ are included to test the variable-selection properties of GAL. The coefficient surface of $\beta_1$ is described by the ``step" function:
	\begin{eqnarray}
		\beta_1(s) = \begin{cases} 0 &\mbox{ if } s_y<0.4 \\ 5(s_y-0.4) &\mbox{ if } 0.4 \leq s_y<0.6 \\ 1 &\mbox{ o.w.} \end{cases}
	\end{eqnarray}.\\
		
	In order to evaluate the performance of GAL under a range of conditions, the data was simulated under 18 different settings (Table \ref{table:simulation_settings}): high (0.1) and low (0.03) levels of $\tau_z$, the autoregression range parameter for the covariate GRFs $Z_1, \dots, Z_5$; three levels (0, 0.5, 0.8) of between-covariate correlation $\rho$; and three levels (0, 0.03, 0.1) of the autoregression range parameter $\tau_{\epsilon}$ for the error-term GRF $\bm{\epsilon}$. Each case was simulated 100 times.
	
	For measuring performance, we look at the pointwise selection frequency of $\beta_1, \dots, \beta_5$ and the coverage frequency of $\beta_1$ (for a nominal 95\% confidence interval).
	
	As a baseline, ordinary GWR was used to estimate the coefficients using the same data but under an ``oracle" setting (oracular GWR, or O-GWR), meaning that GWR was provided with the exactly correct set of predictors as used in the data-generating process. The ratio of the coverage frequency of the GAL to the coverage frequency of O-GWR is called the relative efficiency of the GAL.\\
	
	% latex table generated in R 2.15.1 by xtable 1.7-0 package
% Fri Jan 18 10:19:47 2013
\begin{table}[h!]
	\begin{center}
	\begin{tabular}{rrrr}
		\hline
		& $\tau_z$ & $\rho$ & $\tau_{\epsilon}$ \\ 
		\hline
		1 & 0.03 & 0.00 & 0.00 \\ 
		2 & 0.03 & 0.00 & 0.03 \\ 
		3 & 0.03 & 0.00 & 0.10 \\ 
		4 & 0.03 & 0.50 & 0.00 \\ 
		5 & 0.03 & 0.50 & 0.03 \\ 
		6 & 0.03 & 0.50 & 0.10 \\ 
		7 & 0.03 & 0.80 & 0.00 \\ 
		8 & 0.03 & 0.80 & 0.03 \\ 
		9 & 0.03 & 0.80 & 0.10 \\ 
		10 & 0.10 & 0.00 & 0.00 \\ 
		11 & 0.10 & 0.00 & 0.03 \\ 
		12 & 0.10 & 0.00 & 0.10 \\ 
		13 & 0.10 & 0.50 & 0.00 \\ 
		14 & 0.10 & 0.50 & 0.03 \\ 
		15 & 0.10 & 0.50 & 0.10 \\ 
		16 & 0.10 & 0.80 & 0.00 \\ 
		17 & 0.10 & 0.80 & 0.03 \\ 
		18 & 0.10 & 0.80 & 0.10
	\end{tabular}
	\end{center}
	\caption{Simulation parameters for each setting.\label{table:simulation_settings}}
\end{table}
	
	\subsection{Simulation results}
	Results of the simulation experiment were summarized to asses the consistency in selection and estimation, as well as the coverage properties of the confidence intervals. The confidence intervals based on the bootstrap (without shrinkage) were used for the GAL because they seemed to uniformly outperform the other options.\\
	
% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 14:01:39 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{ccc}
 GWL & GWL-U & Oracle \\ 
  \hline
0.023 & \emph{0.017} & \textbf{0.012} \\ 
  \emph{0.029} & \textbf{0.023} & 0.031 \\ 
  \emph{0.027} & \textbf{0.023} & 0.056 \\ 
  0.028 & \emph{0.023} & \textbf{0.012} \\ 
  0.038 & \emph{0.034} & \textbf{0.029} \\ 
  \emph{0.032} & \textbf{0.028} & 0.057 \\ 
  0.041 & \emph{0.039} & \textbf{0.012} \\ 
  \emph{0.077} & 0.081 & \textbf{0.030} \\ 
  0.062 & \emph{0.060} & \textbf{0.055} \\ 
  0.026 & \emph{0.023} & \textbf{0.016} \\ 
  \emph{0.040} & \textbf{0.039} & 0.061 \\ 
  \emph{0.057} & \textbf{0.050} & 0.125 \\ 
  0.029 & \emph{0.026} & \textbf{0.016} \\ 
  \textbf{0.055} & \emph{0.056} & 0.059 \\ 
  \emph{0.078} & \textbf{0.074} & 0.130 \\ 
  \emph{0.046} & 0.046 & \textbf{0.016} \\ 
  \emph{0.119} & 0.134 & \textbf{0.063} \\ 
  0.167 & \emph{0.161} & \textbf{0.125} \\ 
  \end{tabular}
\caption{Mean squared error of estimates for $\beta_1$ (\textbf{minimum}, \emph{next best}).\label{MSEX}}
\end{center}
\end{table}


% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 14:01:39 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{ccc}
 GWL & GWL-U & Oracle \\ 
  \hline
0.036 & \emph{0.023} & \textbf{0.020} \\ 
  \emph{0.043} & \textbf{0.030} & 0.052 \\ 
  \emph{0.037} & \textbf{0.028} & 0.093 \\ 
  0.043 & \emph{0.032} & \textbf{0.020} \\ 
  0.057 & \textbf{0.046} & \emph{0.049} \\ 
  \emph{0.043} & \textbf{0.034} & 0.095 \\ 
  0.064 & \emph{0.057} & \textbf{0.020} \\ 
  0.113 & \emph{0.110} & \textbf{0.050} \\ 
  \emph{0.083} & \textbf{0.075} & 0.092 \\ 
  0.038 & \emph{0.028} & \textbf{0.026} \\ 
  \emph{0.056} & \textbf{0.049} & 0.102 \\ 
  \emph{0.067} & \textbf{0.054} & 0.208 \\ 
  0.043 & \emph{0.035} & \textbf{0.026} \\ 
  \emph{0.078} & \textbf{0.071} & 0.099 \\ 
  \emph{0.092} & \textbf{0.082} & 0.217 \\ 
  0.071 & \emph{0.068} & \textbf{0.027} \\ 
  \emph{0.166} & 0.177 & \textbf{0.105} \\ 
  \emph{0.191} & \textbf{0.184} & 0.208 \\ 
  \end{tabular}
\caption{Mean squared error of estimates for $\beta_1$ at locations where $\beta_1 != 0$ (\textbf{minimum}, \emph{next best}).\label{MSEX-nonzero}}
\end{center}
\end{table}


% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 13:23:38 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{rr}
 $\tau_x$ & Frequency \\ 
  \hline
0.03 & 0.41 \\ 
  0.10 & 0.29 \\ 
  \end{tabular}
\caption{Frequency of perfect selection for different settings of $\tau_x$.}
\end{center}
\end{table}

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 13:26:38 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{rr}
 $\tau_{\sigma}$ & Frequency \\ 
  \hline
0.00 & 0.54 \\ 
  0.03 & 0.35 \\ 
  0.10 & 0.17 \\ 
  \end{tabular}
\caption{Frequency of perfect selection for different settings of $\tau_{\sigma}$.}
\end{center}
\end{table}

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 13:27:49 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{rr}
 $\rho$ & Frequency \\ 
  \hline
0.00 & 0.38 \\ 
  0.50 & 0.36 \\ 
  0.80 & 0.32 \\ 
  \end{tabular}
\caption{Frequency of perfect selection for different settings of $\rho$.}
\end{center}
\end{table}


% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 13:46:39 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrr}
$\tau_x$ & GWL & GWL-U & Oracle \\ 
  \hline
0.03 & 0.040 & 0.037 & \textbf{0.033} \\ 
  0.10 & 0.068 & \textbf{0.068} & 0.068 \\ 
  \end{tabular}
\caption{Mean squared error of the estimate of $\beta_1$ for different settings of $\tau_x$.}
\end{center}
\end{table}

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 13:48:03 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrr}
 $\tau_\sigma$ & GWL & GWL-U & Oracle \\ 
  \hline
0.00 & 0.032 & 0.029 & \textbf{0.014} \\ 
  0.03 & 0.060 & 0.061 & \textbf{0.046} \\ 
  0.10 & 0.070 & \textbf{0.066} & 0.091 \\ 
  \end{tabular}
\caption{Mean squared error of the estimate of $\beta_1$ for different settings of $\tau_\sigma$.}
\end{center}
\end{table}

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 13:48:03 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrr}
 $\rho$ & GWL & GWL-U & Oracle \\ 
  \hline
0.00 & 0.034 & \textbf{0.029} & 0.050 \\ 
  0.50 & 0.043 & \textbf{0.040} & 0.051 \\ 
  0.80 & 0.085 & 0.087 & \textbf{0.050} \\ 
  \end{tabular}
\caption{Mean squared error of the estimate of $\beta_1$ for different settings of $\rho$.}
\end{center}
\end{table}

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 13:51:50 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrr}
 $\tau_x$ & GWL & GWL-U & Oracle \\ 
  \hline
0.03 & \textbf{0.025} & 0.028 & 0.031 \\ 
  0.10 & \textbf{0.053} & 0.058 & 0.066 \\ 
  \end{tabular}
\caption{Variance of the estimate of $\beta_1$ for different settings of $\tau_x$.}
\end{center}
\end{table}

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 13:51:51 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrr}
 $\tau_\sigma$ & GWL & GWL-U & Oracle \\ 
  \hline
0.00 & 0.014 & 0.018 & \textbf{0.009} \\ 
  0.03 & \textbf{0.044} & 0.053 & 0.045 \\ 
  0.10 & 0.058 & \textbf{0.058} & 0.091 \\ 
  \end{tabular}
\caption{Variance of the estimate of $\beta_1$ for different settings of $\tau_\sigma$.}
\end{center}
\end{table}

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 13:51:51 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrr}
 $\rho$ & GWL & GWL-U & Oracle \\ 
  \hline
0.00 & \textbf{0.022} & 0.023 & 0.048 \\ 
  0.50 & \textbf{0.030} & 0.032 & 0.049 \\ 
  0.80 & 0.065 & 0.074 & \textbf{0.048} \\ 
  \end{tabular}
\caption{Variance of the estimate of $\beta_1$ for different settings of $\rho$.}
\end{center}
\end{table}


% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 13:59:40 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrr}
 $\tau_x$ & GWL & GWL-U & Oracle \\ 
  \hline
0.03 & 0.016 & 0.008 & \textbf{0.002} \\ 
  0.10 & 0.016 & 0.011 & \textbf{0.003} \\ 
  \end{tabular}
\caption{Squared bias of the estimate of $\beta_1$ for different settings of $\tau_x$.}
\end{center}
\end{table}

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 13:59:40 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrr}
 $\tau_\sigma$ & GWL & GWL-U & Oracle \\ 
  \hline
0.00 & 0.018 & 0.011 & \textbf{0.005} \\ 
  0.03 & 0.016 & 0.008 & \textbf{0.001} \\ 
  0.10 & 0.013 & 0.009 & \textbf{0.001} \\ 
  \end{tabular}
\caption{Squared bias of the estimate of $\beta_1$ for different settings of $\tau_\sigma$.}
\end{center}
\end{table}

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Fri Mar  1 13:59:40 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrr}
 $\rho$ & GWL & GWL-U & Oracle \\ 
  \hline
0.00 & 0.012 & 0.007 & \textbf{0.002} \\ 
  0.50 & 0.014 & 0.008 & \textbf{0.002} \\ 
  0.80 & 0.021 & 0.014 & \textbf{0.002} \\ 
  \end{tabular}
\caption{Squared bias of the estimate of $\beta_1$ for different settings of $\rho$.}
\end{center}
\end{table}



\section{References}
\bibliographystyle{chicago}
\bibliography{../../references/gwr}

\end{document}  